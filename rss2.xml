<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"
  xmlns:atom="http://www.w3.org/2005/Atom"
  xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>楪祈のBlog</title>
    <link>http://example.com/</link>
    
    <atom:link href="http://example.com/rss2.xml" rel="self" type="application/rss+xml"/>
    
    <description>星座になれたら。</description>
    <pubDate>Wed, 05 Mar 2025 06:55:09 GMT</pubDate>
    <generator>http://hexo.io/</generator>
    
    <item>
      <title>设计模式</title>
      <link>http://example.com/inori/364ea8cc.html</link>
      <guid>http://example.com/inori/364ea8cc.html</guid>
      <pubDate>Wed, 05 Mar 2025 06:35:13 GMT</pubDate>
      
      <description>设计模式</description>
      
      
      
      <content:encoded><![CDATA[<p>待更新……</p><p>随便插入一张图片…</p><p><img src="/inori/364ea8cc/img/aaa.png" alt="aaa"></p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/java/">java</category>
      
      
      <category domain="http://example.com/tags/java/">java</category>
      
      
      <comments>http://example.com/inori/364ea8cc.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>redis内存回收</title>
      <link>http://example.com/inori/228807ea.html</link>
      <guid>http://example.com/inori/228807ea.html</guid>
      <pubDate>Mon, 03 Mar 2025 02:39:21 GMT</pubDate>
      
      <description>redis入门</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Redis内存回收机制"><a href="#Redis内存回收机制" class="headerlink" title="Redis内存回收机制"></a>Redis内存回收机制</h1><p>Redis中的内存策略主要包含下列四点：</p><ol><li>内存清除策略（Eviction Policy）：当Redis内存空间不足时，会根据特定的算法删除一些key来释放内存。其中，常用的算法有LRU（最少最近使用）、LFU（最少使用频率）和随机算法</li><li>内存淘汰策略（Expiration）：在插入或更新key的时候，可以指定key的过期时间（expire）时间。过期后，Redis会自动将key删除，释放内存</li><li>内存回收策略（Memory Reclamation）：在使用Redis时，可能会因为未正确释放内存而导致内存泄漏。Redis针对这种情况实现了自动内存回收机制来防止内存泄漏的问题</li><li>内存优化策略（Memory Optimization）：Redis提供了各种内存优化策略，例如使用压缩（压缩整数值、压缩非常短的字符串）、使用哈希对象来优化内存使用等，以最大限度地减少内存使用。Redis也使用专门的数据结构来实现某些特定的数据类型，例如基数计数器和位数组，这些也是为了优化内存使用而设计的</li></ol><h2 id="过期key处理"><a href="#过期key处理" class="headerlink" title="过期key处理"></a>过期key处理</h2><p>Redis之所以性能强，最主要的原因就是基于内存存储。然而单节点的Redis其内存大小不宜过大，会影响持久化或主从同步性能。当内存使用达到上限时，就无法存储更多数据了。为了解决这个问题，Redis提供了一些策略实现内存回收：Key过期策略.可以通过<code>expire key 5</code>命令（设置TTL为5秒）给Redis的key设置TTL（存活时间），当key的TTL到期以后，再次访问name返回的是nil，说明这个key已经不存在了，对应的内存也得到释放。</p><h3 id="Redis如何Key是否过期"><a href="#Redis如何Key是否过期" class="headerlink" title="Redis如何Key是否过期"></a>Redis如何Key是否过期</h3><p>Redis本身是一个典型的key-value内存存储数据库，因此所有的key、value都保存在Dict结构中</p><p>Redis判断一个key是否过期：<strong>在其database结构体中，有两个Dict：一个用来记录key-value；另一个用来记录key-TTL</strong></p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">redisDb</span>&#123;</span></span><br><span class="line">    dict *dict;<span class="comment">/*存放所有key及value，也被称为keyspace*/</span></span><br><span class="line">    dict *expires;<span class="comment">/*存放每一个key及其对应的TTL存活时间，只包含设置了TTL的key*/</span></span><br><span class="line">    dict *blocking_keys;<span class="comment">/* Keys with clients waiting for data (BLPOP)*/</span></span><br><span class="line">    dict *ready_keys;<span class="comment">/* Blocked keys that received a PUSH */</span></span><br><span class="line">    dict *watched_keys;<span class="comment">/* WATCHED keys for MULTI/EXEC CAS */</span></span><br><span class="line">    <span class="type">int</span> id;<span class="comment">/* Database ID, 0~15 */</span></span><br><span class="line">    <span class="type">long</span> <span class="type">long</span> avg_ttl;<span class="comment">/*记录平均TTL时长*/</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> expires_cursor;<span class="comment">/*expire检查时在dict中抽样的索引|位置*/</span></span><br><span class="line">    <span class="built_in">list</span> *defrag_later;<span class="comment">/*等待碎片整理的key列表*/</span></span><br><span class="line">&#125; redisDb;</span><br></pre></td></tr></table></figure><h3 id="TTL到期后的删除策略"><a href="#TTL到期后的删除策略" class="headerlink" title="TTL到期后的删除策略"></a>TTL到期后的删除策略</h3><p><strong>惰性删除</strong>：并不是在TTL到期后就立刻删除，而是在访问一个key的时候，检查该key的存活时间，如果已经过期才执行删除。也就是在增删改查的时候才会去检查这个key去判断这个key是否有过期</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">//查找一个key执行写操作</span></span><br><span class="line">robj *<span class="title function_">lookupKeyWriteWithFlags</span><span class="params">(redisDb *db, robj *key, <span class="type">int</span> flags)</span>&#123;</span><br><span class="line">    <span class="comment">// 检查key是否过期</span></span><br><span class="line">    expirelfNeeded(db, key);</span><br><span class="line">    <span class="keyword">return</span> lookupKey(db,key,flags);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 查找一个key执行读操作</span></span><br><span class="line">robj *<span class="title function_">lookupKeyReadWithFlags</span><span class="params">(redisDb *db, robj *key, <span class="type">int</span> flags)</span> &#123;</span><br><span class="line">    robj *val;</span><br><span class="line">    <span class="comment">// 检查key是否过期</span></span><br><span class="line">    <span class="keyword">if</span> (expirelfNeeded(db,key) == <span class="number">1</span>) (</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">NULL</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">expirelfNeeded</span><span class="params">(redisDb *db, robj *key)</span>&#123;</span><br><span class="line">    <span class="comment">//判断是否过期，如果未过期直接结束并返回0</span></span><br><span class="line">    <span class="keyword">if</span> (!keylsExpired(db,key)) <span class="keyword">return</span> O;</span><br><span class="line">    <span class="comment">// ...略</span></span><br><span class="line">    <span class="comment">// 删除过期key</span></span><br><span class="line">    deleteExpiredKeyAndPropagate(db,key);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>周期删除</strong>：通过一个定时任务，周期性的抽样部分过期的key，执行删除，执行周期有两种：</p><ol><li>在Redis服务初始化函数initServer()中设置定时任务serverCron()，按照server.hz的频率来执行过期key清理，模式为SLOW</li><li>在Redis的每个事件循环前会调用beforeSleep()函数，执行过期key清理，模式为FAST</li></ol><p>SLOW模式：</p><ul><li>执行频率受server.hz影响，默认为10，即每秒执行10次，每个执行周期100ms。</li><li>执行清理耗时不超过一次执行周期的25%.默认slow模式耗时不超过25ms</li><li>逐个遍历db，逐个遍历db中的bucket（可以理解为hash列表中的每个下标），抽取20个key判断是否过期</li><li>如果没达到时间上限（25ms）并且过期key比例大于10%(就是过期的key和数据库中总的key进行对比)，再进行一次抽样，否则结束</li></ul><p>FAST模式规则（过期key比例小于10%不执行）：</p><ul><li>执行频率受beforeSleep()调用频率影响，但两次FAST模式间隔不低于2ms</li><li>执行清理耗时不超过1ms</li><li>逐个遍历db，逐个遍历db中的bucket，抽取20个key判断是否过期，如果没达到时间上限（1ms）并且过期key比例大于10%，再进行一次抽样，否则结束</li></ul><h2 id="内存淘汰策略"><a href="#内存淘汰策略" class="headerlink" title="内存淘汰策略"></a>内存淘汰策略</h2><p>当Redis内存使用达到设置的上限时，主动挑选部分key删除以释放更多内存的流程。Redis会在处理客户端命令的方法processCommand()中尝试做内存淘汰</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">int</span> <span class="title function_">processCommand</span><span class="params">(client *c)</span> &#123;</span><br><span class="line">    <span class="comment">// 如果服务器设置了server.maxmemory属性，并且没有执行lua脚本</span></span><br><span class="line">    <span class="keyword">if</span> (server.maxmemory &amp;&amp; !server.lua_timedout) &#123;</span><br><span class="line">        <span class="comment">//尝试进行内存淘汰performEvictions</span></span><br><span class="line">        <span class="type">int</span> out_of_memory = (performEvictions()) == EVICT_FAIL);</span><br><span class="line">        <span class="comment">// ..</span></span><br><span class="line">        <span class="keyword">if</span> (out_of_memory &amp;&amp; reject_cmd_on_oom) &#123;</span><br><span class="line">            rejectCommand(c, shared.oomerr);</span><br><span class="line">            <span class="keyword">return</span> C_OK;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>Redis支持8种不同策略来选择要删除的key：</p><ol><li>noeviction： 不淘汰任何key，但是内存满时不允许写入新数据，默认就是这种策略</li><li>volatile-ttl： 对设置了TTL的key，比较key的剩余TTL值，TTL越小越先被淘汰</li><li>allkeys-random：对全体key ，随机进行淘汰。也就是直接从db-&gt;dict中随机挑选</li><li>volatile-random：对设置了TTL的key ，随机进行淘汰。也就是从db-&gt;expires中随机挑选</li><li>allkeys-lru： 对全体key，基于LRU算法进行淘汰</li><li>volatile-lru： 对设置了TTL的key，基于LRU算法进行淘汰</li><li>allkeys-lfu： 对全体key，基于LFU算法进行淘汰</li><li>volatile-lfu： 对设置了TTL的key，基于LFI算法进行淘汰。</li><li>LRU（Least Recently Used），最少最近使用。用当前时间减去最后一次访问时间，这个值越大则淘汰优先级越高</li><li>LFU（Least Frequently Used），最少频率使用。会统计每个key的访问频率，值越小淘汰优先级越高</li></ol><p>之前记录过的RedisObject：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">redisObject</span>&#123;</span></span><br><span class="line">    <span class="type">unsigned</span> type: <span class="number">4</span>;<span class="comment">//对象类型</span></span><br><span class="line">    <span class="type">unsigned</span> encoding: <span class="number">4</span>; <span class="comment">// 编码方式</span></span><br><span class="line">    <span class="type">unsigned</span> lru: LRU_BITS;<span class="comment">//LRU:以秒为单位记录最近一次访问时间，长度24bit</span></span><br><span class="line">    <span class="comment">//LFU：高16位以分钟为单位记录最近一次访问时间，低8位记录逻辑访问次数</span></span><br><span class="line">    <span class="type">int</span> refcount;<span class="comment">//引用计数，计数为0则可以回收</span></span><br><span class="line">    <span class="type">void</span> *ptr;<span class="comment">//数据指针，指向真实数据</span></span><br><span class="line">&#125; robj;</span><br></pre></td></tr></table></figure><p>LFU的访问次数之所以叫做逻辑访问次数，是因为并不是每次key被访问都计数，而是通过运算：</p><ol><li>生成0~1之间的随机数R</li><li>计算旧次数*lfu_log_factor+1&#96;，记录为P</li><li>如果R&lt;P ，则计数器+1，且最大不超过255</li><li>访问次数会随时间衰减，距离上一次访问时间每隔<code>lfu_decay_time</code>分钟，计数器-1</li></ol><blockquote><p>当Redis使用的内存超出maxmemory设置时，会根据指定的淘汰策略在键空间中选择要删除的键值对。在删除键值对时，Redis会先检查该键值对是否在使用中（例如，有没有客户端正在访问该键值对），然后再根据具体的淘汰策略选择一个待删除的键值对，并将其从缓存中清除。</p></blockquote><p><img src="D:\Blog\source\img\elimination.png" alt="elimination"></p><p>如果把redis全部的key都拿出来进行比较再淘汰，消耗的时间就会大大的增加。所以这里就引入了一个叫<code>eviction_pool</code>（驱逐池）它会抽取的一些样本，将样本放到池子里，再去比较看看谁应该被淘汰，这里<code>maxmemory_samples</code>默认值是5，但是策略不同淘汰的方式不同，这样实现就会比较麻烦所以这里进行了统一，就是按照key其中的某一个值的升序排列，值越大的优先淘汰</p><p>例如LFU最少频率使用，使用的越少就应该越早被淘汰，但是是升序排列的，那么就用255-LFU计算，LFU越少255-LFU就越大越应该被淘汰</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/redis/">redis</category>
      
      
      <category domain="http://example.com/tags/redis/">redis</category>
      
      
      <comments>http://example.com/inori/228807ea.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>redis网络模型</title>
      <link>http://example.com/inori/e34c024b.html</link>
      <guid>http://example.com/inori/e34c024b.html</guid>
      <pubDate>Sat, 01 Mar 2025 00:30:39 GMT</pubDate>
      
      <description>redis入门</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="Linux网络模型"><a href="#Linux网络模型" class="headerlink" title="Linux网络模型"></a>Linux网络模型</h1><p>在《UNIX网络编程》一书中，总结归纳了5种IO模型：</p><ul><li>阻塞IO（Blocking IO）</li><li>非阻塞IO（Nonblocking IO）</li><li>IO多路复用（IO Multiplexing）</li><li>信号驱动IO（Signal Driven IO）</li><li>异步IO（Asynchronous IO）</li></ul><h2 id="阻塞IO"><a href="#阻塞IO" class="headerlink" title="阻塞IO"></a>阻塞IO</h2><p>应用程序想要去读取数据，他是无法直接去读取磁盘数据的，他需要先到内核里边去等待内核操作硬件拿到数据，这个过程就是1，是需要等待的，等到内核从磁盘上把数据加载出来之后，再把这个数据写给用户的缓存区，这个过程是2，如果是阻塞IO，那么整个过程中，用户从发起读请求开始，一直到读取到数据，都是一个阻塞状态。</p><p><img src="D:\Blog\source\img\Blocking_IO.png" alt="Blocking_IO"></p><p>用户去读取数据时，会去先发起recvform一个命令，去尝试从内核上加载数据，如果内核没有数据，那么用户就会等待，此时内核会去从硬件上读取数据，内核读取数据之后，会把数据拷贝到用户态，并且返回ok，整个过程，都是阻塞等待的，这就是阻塞IO</p><h2 id="非阻塞IO"><a href="#非阻塞IO" class="headerlink" title="非阻塞IO"></a>非阻塞IO</h2><p>非阻塞IO的recvfrom操作会立即返回结果而不是阻塞用户进程</p><p>阶段一：</p><ul><li>用户进程尝试读取数据（比如网卡数据）</li><li>此时数据尚未到达，内核需要等待数据</li><li>返回异常给用户进程</li><li>用户进程拿到error后，再次尝试读取</li><li>循环往复，直到数据就绪</li></ul><p>阶段二：</p><ul><li>将内核数据拷贝到用户缓冲区</li><li>拷贝过程中，用户进程依然阻塞等待</li><li>拷贝完成，用户进程解除阻塞，处理数据</li></ul><p>可以看到，非阻塞IO模型中，用户进程在第一个阶段是非阻塞，第二个阶段是阻塞状态。虽然是非阻塞，但性能并没有得到提高。而且忙等机制会导致CPU空转，CPU使用率暴增</p><p><img src="D:\Blog\source\img\Nonblocking_IO.png" alt="Nonblocking_IO"></p><h2 id="IO多路复用"><a href="#IO多路复用" class="headerlink" title="IO多路复用"></a>IO多路复用</h2><p>论是阻塞IO还是非阻塞IO，用户应用在一阶段都需要调用recvfrom来获取数据，差别在于无数据时的处理方案：</p><ul><li>如果调用recvfrom时，恰好没有数据，阻塞IO会使CPU阻塞，非阻塞IO使CPU空转，都不能充分发挥CPU的作用</li><li>如果调用recvfrom时，恰好有数据，则用户进程可以直接进入第二阶段，读取并处理数据</li></ul><p>而在单线程情况下，只能依次处理IO事件，如果正在处理的IO事件恰好未就绪（数据不可读或不可写），线程就会被阻塞，所有IO事件都必须等待，性能表现很差</p><p>想要提高IO事件效率，可以使用多线程，或者即时通知（哪个数据就绪，用户进程就读取这个数据）</p><p>那么用户进程如何知道内核中数据是否就绪呢？</p><p>这个问题的解决依赖于提出的文件描述符。</p><p>文件描述符（File Descriptor）：简称FD，是一个从0开始的无符号整数，用来关联Linux中的一个文件。在Linux中，一切皆文件，例如常规文件、视频、硬件设备等，当然也包括网络套接字（Socket）</p><p>通过FD，我们的网络模型可以利用一个线程监听多个FD，并在某个FD可读、可写时得到通知，从而避免无效的等待，充分利用CPU资源</p><p>用IO复用模式，可以确保去读数据的时候，数据是一定存在的，他的效率比原来的阻塞IO和非阻塞IO性能都要高</p><p>IO多路复用即线程来同时监听多个FD，并在某个FD可读、可写时得到通知，从而避免无效的等待，充分利用CPU资源。不过监听FD的方式、通知的方式又有多种实现，常见的有：</p><ol><li>select</li><li>poll</li><li>epoll</li></ol><h3 id="select"><a href="#select" class="headerlink" title="select"></a>select</h3><p>select是Linux最早是由的I&#x2F;O多路复用技术。</p><p>具体可以描述为：我们把需要处理的数据封装成FD，然后在用户态时创建一个fd的集合（这个集合的大小是要监听的那个FD的最大值+1），这个集合的长度大小是有限制的，同时在这个集合中，标明出来我们要控制哪些数据</p><p>比如要监听的数据，是1,2,5三个数据，此时会执行select函数，然后将整个fd发给内核态，内核态会去遍历用户态传递过来的数据，如果发现这里边都数据都没有就绪，就休眠，直到有数据准备好时，就会被唤醒，唤醒之后，再次遍历一遍，看看谁准备好了，然后再将处理掉没有准备好的数据，最后再将这个FD集合写回到用户态中去，此时用户态就知道了，奥，有人准备好了，但是对于用户态而言，并不知道谁处理好了，所以用户态也需要去进行遍历，然后找到对应准备好数据的节点，再去发起读请求</p><p>seletc模式存在的问题：</p><ul><li>需要将整个fd_set用户空间拷贝到内核空间，select结束还要再次拷贝回用户空间</li><li>select无法得知具体是哪个fd就绪，需要遍历整个fd_set</li><li>fd_set监听的fd数量不能超过1024</li></ul><h3 id="poll"><a href="#poll" class="headerlink" title="poll"></a>poll</h3><p>poll模式对select模式做了简单改进，但性能提升不明显，IO流程：</p><ol><li>创建pollfd数组，向其中添加关注的fd信息，<strong>数组大小自定义</strong></li><li>调用poll函数，将pollfd数组拷贝到内核空间，转链表存储，无上限</li><li>内核遍历fd，判断是否就绪</li><li>数据就绪或超时后，拷贝pollfd数组到用户空间，返回就绪fd数量n</li><li>用户进程判断n是否大于0,大于0则遍历pollfd数组，找到就绪的fd</li></ol><p>与select对比：</p><ul><li><p>select模式中的fd_set大小固定为1024，而pollfd在内核中采用链表，理论上无上限</p></li><li><p>监听FD越多，每次遍历消耗时间也越久，性能反而会下降</p></li></ul><h3 id="epoll"><a href="#epoll" class="headerlink" title="epoll"></a>epoll</h3><p>epoll模式是对select和poll的改进，它提供了三个函数：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">eventpoll</span> &#123;</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">rb_root</span> <span class="title">rbr</span>;</span><span class="comment">// 一颗红黑树，记录要监听的FD</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">list_head</span> <span class="title">rdlist</span>;</span><span class="comment">// 一个链表，记录就绪的FD</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">//1.创建一个epoll实例,内部是event poll，返回对应的句柄epfd</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">epoll_create</span><span class="params">(<span class="type">int</span> size)</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 2.将一个FD添加到epoll的红黑树中，并设置ep_pollL_callback</span></span><br><span class="line"><span class="comment">// callback触发时，就把对应的FD加入到rdlist这个就绪列表中</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">epoll_ctl</span><span class="params">(</span></span><br><span class="line"><span class="params">    <span class="type">int</span> epfd; <span class="comment">// epoll实例的句柄</span></span></span><br><span class="line"><span class="params">    <span class="type">int</span> op;<span class="comment">//要执行的操作，包括：ADD、MOD、DEL</span></span></span><br><span class="line"><span class="params">    <span class="type">int</span> fd; <span class="comment">//要监听的FD</span></span></span><br><span class="line"><span class="params">    <span class="keyword">struct</span> epoll_event *event;<span class="comment">//要监听的事件类型:读、写、异常等</span></span></span><br><span class="line"><span class="params">&#125;;</span></span><br><span class="line"><span class="params"></span></span><br><span class="line"><span class="params"><span class="comment">// 3.检查rdlist列表是否为空，不为空则返回就绪的FD的数量</span></span></span><br><span class="line"><span class="params"><span class="type">int</span> epoll_wait(</span></span><br><span class="line"><span class="params">    <span class="type">int</span> epfd;<span class="comment">// epoll实例的句柄</span></span></span><br><span class="line"><span class="params">    <span class="keyword">struct</span> epoll_event *events;<span class="comment">//空event数组，用于接收就绪的FD</span></span></span><br><span class="line"><span class="params">    <span class="type">int</span> maxevents;<span class="comment">//events数组的最大长度</span></span></span><br><span class="line"><span class="params">    <span class="type">int</span> timeout;<span class="comment">// 超时时间，-1用不超时，0不阻塞；大于0为阻塞时间</span></span></span><br><span class="line"><span class="params">&#125;;</span></span><br></pre></td></tr></table></figure><h4 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h4><ol><li>使用epoll_create创建epoll实例</li><li>紧接着调用epoll_ctl操作，将要监听的数据添加到红黑树上去，并且给每个fd设置一个监听函数，这个函数会在fd数据就绪时触发，把fd数据添加到list_head中去，epoll_ctl中的epfd表明要将监听的fd添加到eventepoll中</li><li>使用epoll_wait等待fd就绪，在用户态创建一个空的events数组，当就绪之后，我们的回调函数会把数据添加到list_head中去，当调用这个函数的时候，会去检查list_head，如果在此过程中，检查到了list_head中有数据会将数据添加到链表中，此时将数据放入到events数组中，并且返回对应的操作的数量</li><li>用户态的此时收到响应后，从events中拿到对应准备好的数据的节点，再去调用方法去拿数据</li></ol><h4 id="epoll中的事件通知模式"><a href="#epoll中的事件通知模式" class="headerlink" title="epoll中的事件通知模式"></a>epoll中的事件通知模式</h4><p>当FD有数据可读时，我们调用epoll_wait可以得到通知。但是事件通知的模式有两种：</p><ul><li>LevelTriggered：简称LT，也叫做水平触发。只要某个FD中有数据可读，每次调用epoll_wait都会得到通知。</li><li>EdgeTriggered：简称ET，也叫做边沿触发。只有在某个FD有状态变化时，调用epoll_wait才会被通知。</li></ul><p>假设一个客户端socket对应的FD已经注册到了epoll实例中，客户端socket发送了2kb的数据，服务端调用epoll_wait，得到通知说FD就绪，服务端从FD读取了1kb数据</p><ul><li>如果我们采用LT模式，因为FD中仍有1kb数据，调用epoll_wait依然会返回结果，并且得到通知</li><li>如果我们采用ET模式，因为已经消费了FD可读事件，后续FD状态没有变化，因此epoll_wait不会返回，数据无法读取，客户端响应超时</li></ul><table><thead><tr><th><strong>特性</strong></th><th><strong>LT 模式</strong></th><th><strong>ET 模式</strong></th></tr></thead><tbody><tr><td><strong>触发条件</strong></td><td>只要条件满足，持续通知。</td><td>只有状态变化时通知一次。</td></tr><tr><td><strong>通知频率</strong></td><td>高</td><td>低</td></tr><tr><td><strong>性能</strong></td><td>较低，适合低并发场景。</td><td>较高，适合高并发场景。</td></tr><tr><td><strong>编程复杂度</strong></td><td>简单，无需担心事件丢失。</td><td>复杂，需要确保一次性处理完数据。</td></tr><tr><td><strong>适用场景</strong></td><td>简单的 I&#x2F;O 操作或初学者。</td><td>高并发、高性能的网络服务器。</td></tr></tbody></table><h2 id="信号驱动IO"><a href="#信号驱动IO" class="headerlink" title="信号驱动IO"></a>信号驱动IO</h2><p>信号驱动IO是与内核建立SIGIO的信号关联并设置回调，当内核有FD就绪时，会发出SIGIO信号通知用户，期间用户应用可以执行其它业务，无需阻塞等待。</p><p>流程：</p><ol><li>用户进程调用sigaction，注册信号处理函数</li><li>内核返回成功，开始监听FD</li><li>用户进程不阻塞等待，可以执行其它业务</li><li>当内核数据就绪后，回调用户进程的SIGIO处理函数</li><li>收到SIGIO回调信号后，用户进程调用recvfrom，读取</li><li>内核将数据拷贝到用户空间</li><li>用户进程处理数据</li></ol><p>当有大量IO操作时，信号较多，SIGIO处理函数不能及时处理可能导致信号队列溢出，而且内核空间与用户空间的频繁信号交互性能也较低。</p><h2 id="异步IO"><a href="#异步IO" class="headerlink" title="异步IO"></a>异步IO</h2><p>这种方式，不仅仅是用户态在试图读取数据后，不阻塞，而且当内核的数据准备完成后，也不会阻塞</p><p>他会由内核将所有数据处理完成后，由内核将数据写入到用户态中，然后才算完成，所以性能极高，不会有任何阻塞，全部都由内核完成，可以看到，异步IO模型中，用户进程在两个阶段都是非阻塞状态</p><p><img src="D:\Blog\source\img\asyncIO.png" alt="asyncIO"></p><p>异步IO避免了同步I&#x2F;O中的等待时间，提高了CPU和I&#x2F;O设备的利用率，但是这种方式对内核的负载很大，在高并发场景下可能会因为内存占用过多出现崩溃现象。如果要使用必须要做并发访问的限流</p><h1 id="Redis网络模型"><a href="#Redis网络模型" class="headerlink" title="Redis网络模型"></a>Redis网络模型</h1><p><strong>Redis到底是单线程还是多线程？</strong></p><ul><li>如果仅仅聊Redis的核心业务部分（命令处理），答案是单线程</li><li>如果是聊整个Redis，那么答案是多线程</li></ul><p>在Redis版本迭代过程中，在两个重要的时间节点上引入了多线程的支持：</p><ul><li>Redis v4.0：引入多线程异步处理一些耗时较久的任务，例如异步删除命令unlink</li><li>Redis v6.0 ：在核心网络模型中引入多线程，进一步提高对多核cpu的利用率</li></ul><p>因此，对于Redis的核心网络模型，在Redis6.0之前确实都是单线程。是利用epoll（Linux系统）这样的IO多路复用技术在事件循环中不断处理客户端情况</p><p><strong>为什么Redis要选择单线程？</strong></p><ul><li>抛开持久化不谈，Redis是纯内存操作，执行速度非常快，它的性能瓶颈是网络延迟而不是执行速度，因此多线程并不会带来巨大的性能提升</li><li>多线程会导致过多的上下文切换，带来不必要的开销</li><li>引入多线程会面临线程安全问题，必然要引入线程锁这样的安全手段，实现复杂度增高，而且性能会降低</li></ul><h2 id="Redis单线程网络模型执行流程"><a href="#Redis单线程网络模型执行流程" class="headerlink" title="Redis单线程网络模型执行流程"></a>Redis单线程网络模型执行流程</h2><ol><li>监听端口：Redis服务器开始监听指定的端口，等待客户端连接。</li><li>接收客户端连接：当有客户端请求连接到Redis服务器时，服务器会接受连接请求，并创建一个客户端套接字，用于与客户端通信。</li><li>接收命令：一旦客户端与服务器建立连接，客户端可以发送命令请求到服务器。Redis服务器通过套接字接收到客户端发送的命令。</li><li>命令解析：服务器会对接收到的命令进行解析，以确定客户端请求的具体操作。</li><li>执行命令：根据解析的结果，服务器会执行相应的命令操作。由于Redis使用单线程模型，每个命令都会按顺序依次执行，不会并发执行。</li><li>数据读写：在执行命令期间，如果需要读取或修改数据，服务器会从内存中读取数据或将修改后的数据写回内存。</li><li>命令回复：执行完命令后，服务器会将执行结果封装为响应，并通过套接字发送回客户端。</li><li>关闭连接：命令执行完成后，服务器会关闭与客户端的连接，等待下一个连接请求</li></ol><p><img src="D:\Blog\source\img\redis_netModel.png" alt="redis_netModel"></p><h3 id="多线程改进"><a href="#多线程改进" class="headerlink" title="多线程改进"></a>多线程改进</h3><p>由于影响网络模型速率的是IO操作，所以可以在命令请求处理器的请求数据写入部分使用多线程和将数据写入buf或reply部分使用多线程来提高速度</p><h1 id="Redis通信协议"><a href="#Redis通信协议" class="headerlink" title="Redis通信协议"></a>Redis通信协议</h1><p>Redis是一个CS架构的软件，通信一般分两步（不包括pipeline和PubSub）：</p><ol><li>客户端（client）向服务端（server）发送一条命令</li><li>服务端解析并执行命令，返回响应结果给客户端</li></ol><p>因此客户端发送命令的格式、服务端响应结果的格式必须有一个规范，这个规范就是通信协议。</p><p>而在Redis中采用的是RESP（Redis Serialization Protocol）协议：</p><ol><li>Redis 1.2版本引入了RESP协议</li><li>Redis 2.0版本中成为与Redis服务端通信的标准，称为RESP2</li><li>Redis 6.0版本中，从RESP2升级到了RESP3协议，增加了更多数据类型并且支持6.0的新特性–客户端缓存</li></ol><p>但目前，默认使用的依然是RESP2协议。</p><p>在RESP中，通过首字节的字符来区分不同数据类型，常用的数据类型包括5种：</p><ol><li>单行字符串：首字节是 <code>&#39;+&#39;</code> ，后面跟上单行字符串，以CRLF（ <code>&quot;\r\n&quot;</code> ）结尾。例如返回”OK”： <code>&quot;+OK\r\n&quot;</code></li><li>错误（Errors）：首字节是 <code>&#39;-&#39; </code>，与单行字符串格式一样，只是字符串是异常信息，例如：<code>&quot;-Error message\r\n&quot;</code></li><li>数值：首字节是 <code>&#39;:&#39; </code>，后面跟上数字格式的字符串，以CRLF结尾。例如：<code>&quot;:10\r\n&quot;</code></li><li>多行字符串：首字节是 <code>&#39;$&#39;</code> ，表示二进制安全的字符串，最大支持512MB：<ol><li>如果大小为0，则代表空字符串：<code>&quot;$0\r\n\r\n&quot;</code></li><li>如果大小为-1，则代表不存在：<code>&quot;$-1\r\n&quot;</code></li></ol></li><li>数组：首字节是 <code>&#39;*&#39;</code>， 后面跟上数组元素个数，再跟上元素，元素数据类型不限，如：</li></ol><blockquote><p>*3\r\n</p><p>:10\r\n</p><p>$5\r\nhello\r\n</p><p>*2\r\n$3\r\nage\r\n:10\r\n</p></blockquote>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/redis/">redis</category>
      
      
      <category domain="http://example.com/tags/redis/">redis</category>
      
      
      <comments>http://example.com/inori/e34c024b.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>redis数据结构</title>
      <link>http://example.com/inori/abcb1f8f.html</link>
      <guid>http://example.com/inori/abcb1f8f.html</guid>
      <pubDate>Wed, 26 Feb 2025 08:34:39 GMT</pubDate>
      
      <description>redis入门</description>
      
      
      
      <content:encoded><![CDATA[<h2 id="简单动态字符串SDS"><a href="#简单动态字符串SDS" class="headerlink" title="简单动态字符串SDS"></a>简单动态字符串SDS</h2><p>Redis中保存的key是字符串，value往往是字符串或者字符串的集合。可以说字符串是Redis中最常见的数据结构。</p><p>不过Redis中没有直接使用C语言中的字符串，而是构建了一种新的字符串结构，称为简单动态字符串（Simple Dynamic String）。</p><p>比如我们执行命令<code>set name Jack</code>，那么Redis将在底层创建两个SDS，其中一个是包含<code>name</code>的SDS，另一个是包含<code>Jack</code>的SDS。</p><p>Redis是C语言实现的，其中SDS是一个结构体，源码如下：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> __<span class="title">attribute__</span>((<span class="title">packed</span>)) <span class="title">sdshdr8</span> &#123;</span></span><br><span class="line">    <span class="type">uint8_t</span> len;<span class="comment">// buf已保存的字符串字节数，不包含结束标示\0</span></span><br><span class="line">    <span class="type">uint8_t</span> alloc;<span class="comment">// buf申请的总字节数，不包含结束标示\0</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> flags;<span class="comment">// 不同的SDS的头类型，用来控制SDS头大小</span></span><br><span class="line">    <span class="type">char</span> buf[];</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>flag对应的种类有五五种</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> SDS_TYPE_5 0</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SDS_TYPE_8 1</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SDS_TYPE_16 2</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SDS_TYPE_32 3</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> SDS_TYPE_64 4</span></span><br></pre></td></tr></table></figure><p>SDS之所以叫做动态字符串，是因为它具备动态扩容的能力，例如有一个内容为<code>abc</code>的SDS，其中<code>len=2,alloc=2,flag=1,h,i,\0</code>，我们要给他追加一段字符串<code>defgh</code>，首先会申请新的内存空间：</p><ul><li>如果新字符串小于1M，则新空间为扩展后字符串长度的两倍+1</li><li>如果新字符串大于1M，则新空间为扩展后字符串长度+1M+1</li></ul><p><strong>动态字符串优点</strong>：</p><ol><li>获取字符串长度的时间复杂度为0(1)[因为长度已经存在于结构体中]</li><li>支持动态扩容</li><li>减少内存分配次数</li><li>二进制安全（可以存储特殊字符，无需考虑结束符的问题）</li></ol><h2 id="IntSet"><a href="#IntSet" class="headerlink" title="IntSet"></a>IntSet</h2><p>Redis为了优化内存和性能，会根据Set中元素的特性自动选择底层实现：</p><ol><li>所有元素都是整数</li><li>元素数量不超过配置的阈值</li></ol><ul><li>Redis配置项<code>set-max-intset-entries</code>定义了IntSet的最大元素数量，默认值为512</li><li>如果元素数量超过该阈值，Redis 会将IntSet转换为HashTable</li></ul><p>IntSet基于整数数组来实现，具备长度可变，有序（便于查找）等特性。</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">intset</span> &#123;</span></span><br><span class="line">    <span class="type">uint32_t</span> encoding; <span class="comment">// 编码方式（int16_t、int32_t、int64_t）</span></span><br><span class="line">    <span class="type">uint32_t</span> length;   <span class="comment">// 集合中元素的数量</span></span><br><span class="line">    <span class="type">int8_t</span> contents[];  <span class="comment">// 存储整数的数组</span></span><br><span class="line">&#125; intset;</span><br></pre></td></tr></table></figure><p>其中的encoding包含三种模式，表示存储的整数大小不同：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> INTSET_ENC_INT16 (sizeof(int16_t))<span class="comment">/*2字节整数,范围类似java的short */</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> INTSET_ENC_INT32 (sizeof(int32_t))<span class="comment">/* 4字节整数,范围类似java的int */</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> INTSET_ENC_INT64 (sizeof(int64_t))<span class="comment">/* 8字节整数,范围类似java的long */</span></span></span><br></pre></td></tr></table></figure><p>为了方便查找，Redis会将intset中所有的整数按照升序依次保存在contents数组中，内部结构类似于<code>encoding:INTSET_ENC_INT16,length:3,5,10,15</code>。</p><blockquote><p>寻址公式：startPtr【开始的起始地址为0】+（sizeof(int16) 【数据类型的字节大小】* index【它对应的下标】）就可以快速找到对应的数据</p></blockquote><h3 id="IntSet升级"><a href="#IntSet升级" class="headerlink" title="IntSet升级"></a>IntSet升级</h3><p>现在，假设有一个intset,元素为{5,10,20}，采用的编码为INTSET_ENC_INT16，则每个整数占2字节。如果现在我想添加50000，这个数字超出了int16_t的范围，intset会自动升级编码方式到合适的大小。</p><p>以当前案例来说流程如下：</p><ol><li>升级编码为INTSET_ENC_INT32, 每个整数占4字节，并按照新的编码方式及元素个数扩容数组</li><li>倒序依次将数组中的元素拷贝到扩容后的正确位置（倒叙放是为了防止字节扩大时将后面的数据给覆盖掉）</li><li>将待添加的元素放入数组末尾</li><li>将inset的encoding属性改为INTSET_ENC_INT32，将length属性改为4</li></ol><p>如果插入的元素位于IntSet中间或者开头，底层会使用二分查找确定插入位置，然后再执行移动和插入操作。</p><h3 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h3><ul><li>IntSet可以看做是特殊的整数数组</li></ul><ul><li>IntSet中的元素唯一、有序</li><li>具备类型升级机制，可以节省内存空间</li><li>底层采用二分查找方式来查询</li></ul><h2 id="Dict"><a href="#Dict" class="headerlink" title="Dict"></a>Dict</h2><p>Redis是一个键值型的数据库，我们可以根据键实现快速的增删改查，而键与值的映射关系正是通过Dict来实现的</p><p>Dict由三部分组成：哈希表（DictHashTable）、哈希节点（DictEntry）、字典（Dict）</p><h3 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h3><p>每个哈希表由数个entry组成，每个桶是一个链表，用于解决哈希冲突</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">dictht</span> &#123;</span></span><br><span class="line">    dictEntry **table;      <span class="comment">// entry数组</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> size;     <span class="comment">// 哈希表的大小</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> sizemask; <span class="comment">// 用于计算索引的掩码（size - 1）</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> used;     <span class="comment">// 已使用的entry数量</span></span><br><span class="line">&#125; dictht;</span><br></pre></td></tr></table></figure><h3 id="哈希节点"><a href="#哈希节点" class="headerlink" title="哈希节点"></a>哈希节点</h3><p>每个键值对存储在一个 <code>dictEntry</code> 结构中</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">dictEntry</span> &#123;</span></span><br><span class="line">    <span class="type">void</span> *key;              <span class="comment">// 键</span></span><br><span class="line">    <span class="class"><span class="keyword">union</span> &#123;</span></span><br><span class="line">        <span class="type">void</span> *val;          <span class="comment">// 值</span></span><br><span class="line">        <span class="type">uint64_t</span> u64;</span><br><span class="line">        <span class="type">int64_t</span> s64;</span><br><span class="line">        <span class="type">double</span> d;</span><br><span class="line">    &#125; v;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">dictEntry</span> *<span class="title">next</span>;</span> <span class="comment">// 指向下一个节点（解决哈希冲突）</span></span><br><span class="line">&#125; dictEntry;</span><br></pre></td></tr></table></figure><h3 id="字典结构"><a href="#字典结构" class="headerlink" title="字典结构"></a>字典结构</h3><p>Dict 包含两个哈希表（<code>ht[0]</code>和<code>ht[1]</code>），以及Rehash相关的状态信息</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">dict</span> &#123;</span></span><br><span class="line">    dictType *type;         <span class="comment">// dict类型，内置不同的hash函数</span></span><br><span class="line">    <span class="type">void</span> *privdata;         <span class="comment">// 私有数据</span></span><br><span class="line">    dictht ht;          <span class="comment">// 两个哈希表，一个是当前数据，另一个一般是空，rehash时使用</span></span><br><span class="line">    <span class="type">long</span> rehashidx;         <span class="comment">// Rehash 的进度（-1 表示未进行 Rehash）</span></span><br><span class="line">    <span class="type">int16_t</span> pauserehash;<span class="comment">// rehash是否暂停，1表示暂停，0则继续</span></span><br><span class="line">&#125; dict;</span><br></pre></td></tr></table></figure><p>当我们向Dict添加键值对时，Redis首先根据key计算出hash值（h），然后利用<code>h&amp;sizemask</code>来计算元素应该存储到数组中的哪个索引位置。我们存储k1&#x3D;v1，假设k1的哈希值h&#x3D;1，则1&amp;3&#x3D;1，因此k1&#x3D;v1要存储到数组角标1位置，键值对内部结构使用的是链表，新插入的节点使用的是头插法。</p><h3 id="Dict的扩容和收缩"><a href="#Dict的扩容和收缩" class="headerlink" title="Dict的扩容和收缩"></a>Dict的扩容和收缩</h3><p>Dict中的HashTable就是数组结合单向链表的实现，当集合中元素较多时，必然导致哈希冲突增多，链表过长，则查询效率会大大降低。</p><p>Dict在每次新增键值对时都会检查负载因子（<code>LoadFactor = used/size</code>），满足以下两种情况时会触发哈希表扩容：</p><ul><li>哈希表的LoadFactor &gt;&#x3D; 1，并且服务器没有执行BGSAVE或者BGREWRITEAOF等后台进程（因为这种操作对性能要求高，如果我再进行rehash的操作就可能导致阻塞）</li><li>哈希表的LoadFactor &gt; 5</li></ul><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* 检查是否需要扩容 */</span></span><br><span class="line"><span class="type">static</span> <span class="type">int</span> _dictExpandIfNeeded(dict *d) &#123;</span><br><span class="line">    <span class="comment">// 如果正在进行 Rehash，则不需要扩容</span></span><br><span class="line">    <span class="keyword">if</span> (dictIsRehashing(d)) <span class="keyword">return</span> DICT_OK;</span><br><span class="line">    <span class="comment">// 如果哈希表为空，则初始化为默认大小（4）</span></span><br><span class="line">    <span class="keyword">if</span> (d-&gt;ht.size == <span class="number">0</span>) <span class="keyword">return</span> dictExpand(d, DICT_HT_INITIAL_SIZE);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果负载因子超过1并且没有进行bgrewrite等子进程操作，则触发扩容</span></span><br><span class="line">    <span class="keyword">if</span> (d-&gt;ht.used &gt;= d-&gt;ht.size &amp;&amp;</span><br><span class="line">        (dict_can_resize || d-&gt;ht.used/d-&gt;ht.size &gt; dict_force_resize_ratio))</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">return</span> dictExpand(d, d-&gt;ht[<span class="number">0</span>].used + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> DICT_OK;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当哈希表的负载因子低于<code>0.1</code>，并且<code>size &gt; 4</code>时，Redis会触发收缩操作</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">/* 检查是否需要收缩 */</span></span><br><span class="line"><span class="type">int</span> <span class="title function_">dictResize</span><span class="params">(dict *d)</span> &#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> minimal;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果正在进行 Rehash 或 dict_can_resize 为 0，则不能收缩</span></span><br><span class="line">    <span class="keyword">if</span> (!dict_can_resize || dictIsRehashing(d)) <span class="keyword">return</span> DICT_ERR;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取used，也就是entry个数</span></span><br><span class="line">    minimal = d-&gt;ht[<span class="number">0</span>].used;</span><br><span class="line">    <span class="comment">// 如果used小于4，则重置为4</span></span><br><span class="line">    <span class="keyword">if</span> (minimal &lt; DICT_HT_INITIAL_SIZE)</span><br><span class="line">        minimal = DICT_HT_INITIAL_SIZE;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 调用 dictExpand 进行收缩，值为第一个大于等于minimal的2^n</span></span><br><span class="line">    <span class="keyword">return</span> dictExpand(d, minimal);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Dict的rehash"><a href="#Dict的rehash" class="headerlink" title="Dict的rehash"></a>Dict的rehash</h3><p>不管是扩容还是收缩，必定会创建新的哈希表，导致哈希表的size和sizemask变化，而key的查询与sizemask有关。因此必须对哈希表中的每一个key重新计算索引，插入新的哈希表，这个过程称为rehash。过程是这样的：</p><ul><li>计算新hash表的realeSize，值取决于当前要做的是扩容还是收缩：<ul><li>如果是扩容，则新size为第一个大于等于dict.ht[0].used + 1的2^n</li><li>如果是收缩，则新size为第一个大于等于dict.ht[0].used的2^n （不得小于4）</li></ul></li><li>按照新的realeSize申请内存空间，创建dictht，并赋值给dict.ht[1]</li><li>设置dict.rehashidx &#x3D; 0，标示开始rehash</li><li>每次执行新增、查询、修改、删除操作时，都检查一下dict.rehashidx是否大于-1，如果是则将<code>dict.ht[0].table[rehashidx]</code>的entry链表rehash到dict.ht[1]，并且将<code>rehashidx ++</code>。直至dict.ht[0]的所有数据都rehash到dict.ht[1]</li><li>将dict.ht[1]赋值给dict.ht[0]，给dict.ht[1]初始化为空哈希表，释放原来的dict.ht[0]的内存</li><li>将rehashidx赋值为-1，代表rehash结束</li><li>在rehash过程中，新增操作，则直接写入ht[1]，查询、修改和删除则会在dict.ht[0]和dict.ht[1]依次查找并执行。这样可以确保ht[0]的数据只减不增，随着rehash最终为空</li></ul><h2 id="ZipList"><a href="#ZipList" class="headerlink" title="ZipList"></a>ZipList</h2><p>ZipList是一种特殊的“双端链表”（其实不是链表），由一系列特殊编码的连续内存块组成。可以在任意一端进行压入&#x2F;弹出操作, 并且该操作的时间复杂度为O(1)</p><h3 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h3><p>Ziplist由三部分组成：</p><ol><li>头部</li></ol><ul><li><code>zlbytes</code>：Ziplist的总字节数（4字节）</li><li><code>zltail</code>：最后一个节点的偏移量（4字节），便于定位到最后一个entry节点</li><li><code>zllen</code>：entry节点的数量（2字节）</li></ul><ol start="2"><li>entry节点(长度不固定)</li></ol><ul><li>每个entry节点包含以下字段：<ul><li><code>previous_entry_len</code>：前一个节点的长度（1或5字节）<ul><li>如果前一节点的长度小于254字节，则采用1个字节来保存</li><li>如果前一节点的长度大于254字节，则采用5个字节来保存这个长度值，第一个字节为<code>0xfe</code>，后四个字节才是真实长度数据</li></ul></li><li><code>encoding</code>：当前节点的编码方式（1、2或5字节）</li><li><code>content</code>：实际存储的数据（字符串或整数）</li></ul></li></ul><ol start="3"><li>尾部</li></ol><ul><li><code>zlend</code>：Ziplist的结束标志（1字节，固定值<code>0xff</code>）</li></ul><p>ZipList中所有存储长度的数值均采用小端字节序，即低位字节在前，高位字节在后。例如：数值0x1234，采用小端字节序后实际存储值为：0x3412</p><h3 id="Encodeing编码"><a href="#Encodeing编码" class="headerlink" title="Encodeing编码"></a>Encodeing编码</h3><p>ZipListEntry中的encoding编码分为字符串和整数两种：<br>字符串：如果encoding是以“00”、“01”或者“10”开头，则证明content是字符串</p><table><thead><tr><th align="center">编码</th><th align="center">编码长度</th><th>字符串大小</th></tr></thead><tbody><tr><td align="center">|00pppppp|</td><td align="center">1 bytes</td><td>&lt;&#x3D; 63 bytes</td></tr><tr><td align="center">|01pppppp|qqqqqqqq|</td><td align="center">2 bytes</td><td>&lt;&#x3D; 16383 bytes</td></tr><tr><td align="center">|10000000|qqqqqqqq|rrrrrrrr|ssssssss|tttttttt|</td><td align="center">5 bytes</td><td>&lt;&#x3D; 4294967295 bytes</td></tr></tbody></table><p>整数：如果encoding是以“11”开始，则证明content是整数，且encoding固定只占用1个字节</p><table><thead><tr><th align="center">编码</th><th align="center">编码长度</th><th align="center">整数类型</th></tr></thead><tbody><tr><td align="center">11000000</td><td align="center">1</td><td align="center">int16_t（2 bytes）</td></tr><tr><td align="center">11010000</td><td align="center">1</td><td align="center">int32_t（4 bytes）</td></tr><tr><td align="center">11100000</td><td align="center">1</td><td align="center">int64_t（8 bytes）</td></tr><tr><td align="center">11110000</td><td align="center">1</td><td align="center">24位有符整数(3 bytes)</td></tr><tr><td align="center">11111110</td><td align="center">1</td><td align="center">8位有符整数(1 bytes)</td></tr><tr><td align="center">1111xxxx</td><td align="center">1</td><td align="center">直接在xxxx位置保存数值，范围从0001~1101，减1后结果为实际值</td></tr></tbody></table><h3 id="连锁更新问题"><a href="#连锁更新问题" class="headerlink" title="连锁更新问题"></a>连锁更新问题</h3><p>前面提到过，ZipList的每个Entry都包含previous_entry_length来记录上一个节点的大小，长度是1个或5个字节：</p><ul><li>如果前一节点的长度小于254字节，则采用1个字节来保存这个长度值</li><li>如果前一节点的长度大于等于254字节，则采用5个字节来保存这个长度值，第一个字节为0xfe，后四个字节才是真实长度数据</li></ul><p>当插入或删除节点时，可能导致后续节点的<code>prevlen</code>字段发生变化。如果<code>prevlen</code>的长度从1字节变为5字节，则需要扩展当前节点的空间，这可能导致后续节点的<code>prevlen</code>字段也需要更新，从而引发连锁更新</p><p>不过发生的可能性较低，redis并没有解决这个问题</p><h2 id="QuickList"><a href="#QuickList" class="headerlink" title="QuickList"></a>QuickList</h2><p>Redis在3.2版本引入了新的数据结构QuickList，它是一个双端链表，链表中的每个节点都是一个ZipList。可以用多个ZipList来分片存储数据</p><p><img src="D:\Blog\source\img\QuickList.png" alt="QuickList"></p><p>为了避免QuickList中的每个ZipList中entry过多，Redis提供了一个配置项：<code>list-max-ziplist-size</code></p><ul><li>如果值为正，则代表ZipList的允许的entry个数的最大值</li><li>如果值为负，则代表ZipList的最大内存大小，分5种情况：<ul><li>-1：每个ZipList的内存占用不能超过4kb</li><li>-2：每个ZipList的内存占用不能超过8kb（默认值）</li><li>-3：每个ZipList的内存占用不能超过16kb</li><li>-4：每个ZipList的内存占用不能超过32kb</li><li>-5：每个ZipList的内存占用不能超过64kb</li></ul></li></ul><p>QuickList源码：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">quicklist</span> &#123;</span></span><br><span class="line">    <span class="comment">// 头节点指针</span></span><br><span class="line">    quicklistNode *head;</span><br><span class="line">    <span class="comment">// 尾节点指针</span></span><br><span class="line">    quicklistNode *tail;</span><br><span class="line">    <span class="comment">// 所有ziplist的entry的数量</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> count;</span><br><span class="line">    <span class="comment">// ziplists总数量</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> len;</span><br><span class="line">    <span class="comment">// ziplist的entry上限，默认值 -2</span></span><br><span class="line">    <span class="type">int</span> fill: QL_FILL_BITS;</span><br><span class="line">    <span class="comment">// 首尾不压缩的节点数量</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> compress:QL_COMP_BITS;</span><br><span class="line">    <span class="comment">// 内存重分配时的书签数量及数组，一般用不到</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> bookmark count: QL_BM_BITS;</span><br><span class="line">    quicklistBookmark bookmarks[];</span><br><span class="line">&#125; quicklist;</span><br></pre></td></tr></table></figure><p>QuickListNode源码：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">quicklistNode</span> &#123;</span></span><br><span class="line">    <span class="comment">// 前一个节点指针</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">quicklistNode</span> *<span class="title">prev</span>;</span></span><br><span class="line">    <span class="comment">// 下一个节点指针</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">quicklistNode</span> *<span class="title">next</span>;</span></span><br><span class="line">    <span class="comment">// 当前节点的ZipList指针</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">char</span> *zl;</span><br><span class="line">    <span class="comment">// 当前节点的Ziplist的字节大小</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> sz;</span><br><span class="line">    <span class="comment">// 当前节点的ZipList的entry个数</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> count: <span class="number">16</span>;</span><br><span class="line">    <span class="comment">// 编码方式:1.ZipList; 2.lzf压缩模式</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> encoding: <span class="number">2</span>;</span><br><span class="line">    <span class="comment">// 数据容器类型(预留):1.其它; 2.ZipList</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> container: <span class="number">2</span>;</span><br><span class="line">    <span class="comment">// 是否被解压缩。1说明被解压了，将来要重新压缩</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> recompress: <span class="number">1</span>;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> attempted_compress: <span class="number">1</span>; <span class="comment">//测试用</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> extra: <span class="number">10</span>;<span class="comment">/*预留字段*/</span></span><br><span class="line">&#125; quicklistNode;</span><br></pre></td></tr></table></figure><p>ZipList的压缩可以选择头尾几个链表不压缩，一般crud都在头尾，只压缩中间的ZipList</p><p><img src="D:\Blog\source\img\quicklist_ziplist.png" alt="quicklist_ziplist"></p><h2 id="SkipList"><a href="#SkipList" class="headerlink" title="SkipList"></a>SkipList</h2><p>SkipList是链表，但是与传统链表有些差异，具有以下特点：</p><ul><li>节点按照score值排序，score值一样则按照ele字典排序</li><li>节点可能包含多个指针，指针跨度不同</li><li>查找、插入和删除操作的平均时间复杂度为O(logn)</li><li>SkipList的索引层级是动态调整的，插入新元素时会随机生成其层级</li><li>跳跃表是一个双向链表，每一个节点都包含score和ele值</li></ul><p>SkipList的源码：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">zskiplist</span> &#123;</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">zskiplistNode</span> *<span class="title">header</span>, *<span class="title">tail</span>;</span><span class="comment">// 头尾节点指针</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> length;<span class="comment">// 节点数量</span></span><br><span class="line">    <span class="type">int</span> level;<span class="comment">// 最大的索引层级，默认是1</span></span><br><span class="line">&#125; zskiplist;</span><br></pre></td></tr></table></figure><p>SkipListNode的源码：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">zskiplistNode</span> &#123;</span></span><br><span class="line">    sds ele;                      <span class="comment">// 元素值</span></span><br><span class="line">    <span class="type">double</span> score;                   <span class="comment">// 分值，排序、查找用</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">zskiplistNode</span> *<span class="title">backward</span>;</span> <span class="comment">// 前一个节点指针</span></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">zskiplistLevel</span> &#123;</span></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">zskiplistNode</span> *<span class="title">forward</span>;</span> <span class="comment">// 下一个节点指针</span></span><br><span class="line">        <span class="type">unsigned</span> <span class="type">long</span> span;             <span class="comment">// 索引跨度</span></span><br><span class="line">    &#125; level[];                        <span class="comment">// 多级索引数组</span></span><br><span class="line">&#125; zskiplistNode;</span><br></pre></td></tr></table></figure><p><img src="D:\Blog\source\img\skiplist.png" alt="skiplist"></p><h2 id="RedisObject"><a href="#RedisObject" class="headerlink" title="RedisObject"></a>RedisObject</h2><p>Redis中的任意数据类型的键和值都会被封装为一个RedisObject，也叫做Redis对象，它是Redis数据存储的基础，通过封装数据的类型、编码方式和实际值，提供了统一的接口来操作各种数据类型。源码如下：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">redisObject</span> &#123;</span></span><br><span class="line">    <span class="type">unsigned</span> type: <span class="number">4</span>;<span class="comment">// 5种数据结构类型，占4bit</span></span><br><span class="line">    <span class="type">unsigned</span> encoding: <span class="number">4</span>;<span class="comment">// 底层编码方式，共11种，占4bit</span></span><br><span class="line">    <span class="type">unsigned</span> lru： LRU_BITS;<span class="comment">// lru表示该对象最后一次被访问的时间，占24bit,便于判断空闲时间太久的key</span></span><br><span class="line">    <span class="type">int</span> refcount;<span class="comment">// 对象引用计数器，计数器为0则说明对象无人引用，可以被回收</span></span><br><span class="line">    <span class="type">void</span> *ptr;<span class="comment">// 指针，指向存放实际数据的空间</span></span><br><span class="line">&#125; robj;</span><br></pre></td></tr></table></figure><h3 id="Redis的编码方式"><a href="#Redis的编码方式" class="headerlink" title="Redis的编码方式"></a>Redis的编码方式</h3><p>Redis中会根据存储的数据类型不同，选择不同的编码方式，共包含11种不同类型：</p><table><thead><tr><th align="center">编号</th><th align="center">编码方式</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">0</td><td align="center">OBJ_ENCODING_RAW</td><td align="center">raw编码动态字符串</td></tr><tr><td align="center">1</td><td align="center">OBJ_ENCODING_INT</td><td align="center">long类型的整数的字符串</td></tr><tr><td align="center">2</td><td align="center">OBJ_ENCODING_HT</td><td align="center">hash表（字典dict）</td></tr><tr><td align="center">3</td><td align="center">OBJ_ENCODING_ZIPMAP</td><td align="center">已废弃</td></tr><tr><td align="center">4</td><td align="center">OBJ_ENCODING_LINKEDLIST</td><td align="center">双端链表</td></tr><tr><td align="center">5</td><td align="center">OBJ_ENCODING_ZIPLIST</td><td align="center">压缩列表</td></tr><tr><td align="center">6</td><td align="center">OBJ_ENCODING_INTSET</td><td align="center">整数集合</td></tr><tr><td align="center">7</td><td align="center">OBJ_ENCODING_SKIPLIST</td><td align="center">跳表</td></tr><tr><td align="center">8</td><td align="center">OBJ_ENCODING_EMBSTR</td><td align="center">embstr的动态字符串</td></tr><tr><td align="center">9</td><td align="center">OBJ_ENCODING_QUICKLIST</td><td align="center">快速列表</td></tr><tr><td align="center">10</td><td align="center">OBJ_ENCODING_STREAM</td><td align="center">Stream流</td></tr></tbody></table><h3 id="五种数据结构对应的编码"><a href="#五种数据结构对应的编码" class="headerlink" title="五种数据结构对应的编码"></a>五种数据结构对应的编码</h3><p>Redis中会根据存储的数据类型不同，选择不同的编码方式。每种数据类型的使用的编码方式如下：</p><table><thead><tr><th align="center">数据类型</th><th align="center">编码方式</th></tr></thead><tbody><tr><td align="center">OBJ_STRING</td><td align="center">int、embstr、raw</td></tr><tr><td align="center">OBJ_LIST</td><td align="center">LinkedList和ZipList(3.2以前)、QuickList（3.2以后）</td></tr><tr><td align="center">OBJ_SET</td><td align="center">intset、HT</td></tr><tr><td align="center">OBJ_ZSET</td><td align="center">ZipList、HT、SkipList</td></tr><tr><td align="center">OBJ_HASH</td><td align="center">ZipList、HT</td></tr></tbody></table><h2 id="五种数据结构"><a href="#五种数据结构" class="headerlink" title="五种数据结构"></a>五种数据结构</h2><h3 id="String"><a href="#String" class="headerlink" title="String"></a>String</h3><p>String是Redis中最常见的数据存储类型</p><ul><li>其基本编码方式是RAW，基于简单动态字符串（SDS）实现，存储上限为512MB</li><li>如果存储的SDS长度小于44字节，则会采用EMBSTR编码，此时object head与SDS是一段连续空间。申请内存时只需要调用一次内存分配函数，效率更高</li><li>如果存储的字符串是整数值，并且大小在LONG_MAX范围内，则会采用INT编码，直接将数据保存在RedisObject的ptr指针位置(刚好8字节)，不需要SDS</li></ul><h4 id="底层实现"><a href="#底层实现" class="headerlink" title="底层实现"></a>底层实现</h4><p><code>OBJ_ENCODING_RAW</code>的实现就是<code>RedisObject</code>中的ptr指向一个SDS</p><p><img src="D:\Blog\source\img\raw.png" alt="raw"></p><p><code>EMBSTR</code>的形式为RedisObject后直接跟SDS，而不是两个分开的地址空间</p><p><img src="D:\Blog\source\img\embstr.png" alt="embstr"></p><p><code>INT</code>形式删除了SDS，直接把数据保存在ptr位置</p><p><img src="D:\Blog\source\img\INT.png" alt="INT"></p><h3 id="List"><a href="#List" class="headerlink" title="List"></a>List</h3><p>Redis的List类型可以从首、尾操作列表中的元素，哪一个数据结构能满足上述特征？</p><ul><li>LinkedList：普通链表，可以从双端访问，内存占用较高，内存碎片较多</li><li>ZipList：压缩列表，可以从双端访问，内存占用低，存储上限低</li><li>QuickList：<code>LinkedList + ZipList</code>，可以从双端访问，内存占用较低，包含多个ZipList，存储上限高</li></ul><p>在3.2版本之前，Redis采用ZipList和LinkedList来实现List，当元素数量<code>&lt;512</code>并且元素大小<code>&lt;64字节</code>时使用ZipList编码，超过则采用LinkedList编码</p><p>在3.2版本之后，Redis统一采用QuickList来实现List</p><p>创建List：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">pushGenericCommand</span><span class="params">(client *c, <span class="type">int</span> where, <span class="type">int</span> xx)</span>&#123;</span><br><span class="line">    <span class="type">int</span> j;</span><br><span class="line">    <span class="comment">// 尝试找到KEY对应的list</span></span><br><span class="line">    robj *lobj= lookupKeyWrite(c-&gt;db, c-&gt;argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="comment">// 检查类型是否正确</span></span><br><span class="line">    <span class="keyword">if</span> (checkType(c, lobj, OBJ_LIST)) <span class="keyword">return</span>;</span><br><span class="line">    <span class="comment">// 检查是否为空</span></span><br><span class="line">    <span class="keyword">if</span> (!lobj) &#123;</span><br><span class="line">        <span class="keyword">if</span> (xx) &#123;</span><br><span class="line">        addReply(c, shared.czero);</span><br><span class="line">            <span class="keyword">return</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 为空，则创建新的QuickList</span></span><br><span class="line">        lobj= createQuicklistObject();</span><br><span class="line">        quicklistSetOptions(lobj-&gt;ptr, server.list_max_ziplist_size, server.list_compress_depth);</span><br><span class="line">        dbAdd(c-&gt;db,c-&gt;argv[<span class="number">1</span>],lobj);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>创建Quicklist：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">robj *<span class="title function_">createQuicklistObject</span><span class="params">(<span class="type">void</span>)</span>&#123;</span><br><span class="line">    <span class="comment">// 申请内存并初始化QuickList</span></span><br><span class="line">    quicklist *l = quicklistCreate();</span><br><span class="line">    <span class="comment">// 创建RedisObject，type为OBJ_LIST</span></span><br><span class="line">    <span class="comment">// ptr指向QuickList</span></span><br><span class="line">    robj *o = createObject(OBJ_LIST,l);</span><br><span class="line">    <span class="comment">// 设置编码为 QuickList</span></span><br><span class="line">    <span class="number">0</span>-&gt;encoding = OBJ_ENCODING_QUICKLIST;</span><br><span class="line">    <span class="keyword">return</span> o;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure><p><img src="D:\Blog\source\img\redis_List.png" alt="redis_List"></p><h3 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h3><p>Set是Redis中的单列集合，满足下列特点：</p><ul><li>不保证有序性</li><li>保证元素唯一</li><li>支持求交集、并集、差集</li></ul><p>可以看出，Set对查询元素的效率要求非常高，所以底层使用了IntSet和Dict实现。</p><p>为了查询效率和唯一性，set采用HT编码（Dict）。Dict中的key用来存储元素，value统一为null。</p><p>当存储的所有数据都是整数，并且元素数量不超过<code>set-max-intset-entries(默认512)</code>时，Set会采用IntSet编码，以节省内存</p><p>创建Set的源码：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">robj *<span class="title function_">setTypeCreate</span><span class="params">(sds value)</span>&#123;</span><br><span class="line">    <span class="comment">// 判断value是否是数值类型long long</span></span><br><span class="line">    <span class="keyword">if</span> (isSdsRepresentableAsLongLong(value, <span class="literal">NULL</span>)==C_OK)</span><br><span class="line">    <span class="comment">// 如果是数值类型，则采用IntSet编码</span></span><br><span class="line">    <span class="keyword">return</span> createIntsetObject();</span><br><span class="line">    <span class="comment">//否则采用默认编码，也就是HT</span></span><br><span class="line">    <span class="keyword">return</span> createSetObject();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果是数值类型：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">robj *<span class="title function_">createIntsetobject</span><span class="params">(<span class="type">void</span>)</span>&#123;</span><br><span class="line">    <span class="comment">// 初始化INTSET并申请内存空间</span></span><br><span class="line">    intset *is= intsetNew();</span><br><span class="line">    <span class="comment">// 创建RedisObject</span></span><br><span class="line">    robj *o= createObject(OBJ_SET, is);</span><br><span class="line">    <span class="comment">// 指定编码为INTSET</span></span><br><span class="line">    <span class="number">0</span>-&gt;encoding = OBJ_ENCODING_INTSET;</span><br><span class="line">    <span class="keyword">return</span> o;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>默认编码HT：</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">robj *<span class="title function_">createSetObject</span><span class="params">(<span class="type">void</span>)</span> &#123;</span><br><span class="line">    <span class="comment">// 初始化Dict类型，并申请内存</span></span><br><span class="line">    dict *d = dictCreate(&amp;setDictType, <span class="literal">NULL</span>);</span><br><span class="line">    <span class="comment">// 创建RedisObject</span></span><br><span class="line">    robj *o= createObject(OBJ_SET, d);</span><br><span class="line">    <span class="comment">// 设置encoding为HT</span></span><br><span class="line">    <span class="number">0</span>-&gt;encoding = OBJ_ENCODING_HT;</span><br><span class="line">    <span class="keyword">return</span> o;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果原本set中存储的元素使5,10,20，然后执行命令<code>sadd s1 m1</code>，因为新插入的元素是字符串，redis需要转换编码，会新建一个Dict，把原本IntSet中的元素和m1都存入Dict中，然后转换RedisObject中的ptr，指向Dict，最后更改encoding为<code>OBJ_ENCODING_HT</code></p><p><img src="D:\Blog\source\img\redis_set.png" alt="redis_set"></p><h3 id="ZSet"><a href="#ZSet" class="headerlink" title="ZSet"></a>ZSet</h3><p>ZSet也就是SortedSet，其中每一个元素都需要指定一个score值和member值：</p><ul><li>可以根据score值排序后</li><li>member必须唯一</li><li>可以根据member查询分数</li></ul><p>因此，zset底层数据结构必须满足键值存储、键必须唯一、可排序这几个需求。</p><ul><li>SkipList：可以排序，并且可以同时存储score和ele值（member）</li><li>HT（Dict）：可以键值存储，并且可以根据key找value</li></ul><p>zset会同时使用两个结构，需要查找，维护键唯一时使用HT，需要排序时使用SkipList</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// zset结构</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">zset</span> &#123;</span></span><br><span class="line">    <span class="comment">// Dict指针</span></span><br><span class="line">    dict *dict;</span><br><span class="line">    <span class="comment">// SkipList指针</span></span><br><span class="line">    zskiplist *zsl;</span><br><span class="line">&#125; zset;</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">robj *<span class="title function_">createZsetObject</span><span class="params">(<span class="type">void</span>)</span> &#123;</span><br><span class="line">    zset *zs = zmalloc(<span class="keyword">sizeof</span>(*zs));</span><br><span class="line">    robj *o;</span><br><span class="line"><span class="comment">// 创建Dict</span></span><br><span class="line">    zs-&gt;dict = dictCreate(&amp;zsetDictType, <span class="literal">NULL</span>);</span><br><span class="line">    <span class="comment">// 创建SkipList</span></span><br><span class="line">    zs-&gt;zsl = zslCreate();</span><br><span class="line">    o = createObject(OBJ_ZSET, zs);</span><br><span class="line">    o-&gt;encoding = OBJ_ENCODING_SKIPLIST;</span><br><span class="line"><span class="keyword">return</span> o;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="D:\Blog\source\img\zset.png" alt="zset"></p><p>当元素数量不多时，HT和SkipList的优势不明显，而且更耗内存。因此zset还会采用ZipList结构来节省内存，不过需要同时满足两个条件:</p><ul><li>元素数量小于zset_max_ziplist_entries，默认值128</li><li>每个元素都小于zset_max_ziplist_value字节，默认值64</li></ul><p>ziplist本身没有排序功能，而且没有键值对的概念，因此需要有zset通过编码实现：</p><ul><li>ZipList是连续内存，因此score和element是紧挨在一起的两个entry，element在前，score在后(当我们查询的时候就可以直接遍历即可，当我们要找m1的score，只需要找到m1在找下一个即可)</li><li>score越小越接近队首，score越大越接近队尾，按照score值升序排列</li></ul><h3 id="Hash"><a href="#Hash" class="headerlink" title="Hash"></a>Hash</h3><p>Hash结构与Redis中的Zset非常类似：</p><ul><li>都是键值存储</li><li>都需求根据键获取值</li><li>键必须唯一</li></ul><p>区别：</p><ul><li>zset的键是member，值是score，hash的键和值都是任意值</li><li>zset要根据score排序；hash则无需排序</li></ul><p>Hash的底层实现也与zset类似，只不过不需要用于排序的SkipList。</p><p>Hash结构默认采用ZipList编码，用以节省内存。ZipList中相邻的两个entry分别保存key和value，随着数据的增加，底层的ziplist就可能会转成dict，具体配置如下：</p><ul><li><code>hash-max-ziplist-entries &gt; 512</code></li><li><code>hash-max-ziplist-value &gt; 64</code></li></ul><p>当满足上面两个条件其中之一的时候，Redis就使用dict字典来实现hash。Redis的hash之所以这样设计，是因为当ziplist变得很大的时候，它有如下几个缺点：</p><ul><li>每次插入或修改引发的realloc操作会有更大的概率造成内存拷贝，从而降低性能</li><li>一旦发生内存拷贝，内存拷贝的成本也相应增加，因为要拷贝更大的一块数据</li><li>当ziplist数据项过多的时候，在它上面查找指定的数据项就会性能变得很低，因为ziplist上的查找需要进行遍历</li></ul><p><img src="D:\Blog\source\img\redis_hash.png" alt="redis_hash"></p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/redis/">redis</category>
      
      
      <category domain="http://example.com/tags/redis/">redis</category>
      
      
      <comments>http://example.com/inori/abcb1f8f.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>redis分布式缓存</title>
      <link>http://example.com/inori/1e2438b1.html</link>
      <guid>http://example.com/inori/1e2438b1.html</guid>
      <pubDate>Wed, 19 Feb 2025 07:30:39 GMT</pubDate>
      
      <description>redis入门</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="分布式缓存"><a href="#分布式缓存" class="headerlink" title="分布式缓存"></a>分布式缓存</h1><p>之前学的都是单点Redis，而这种单点Redis存在一些问题：</p><ul><li>数据丢失</li><li>并发能力差</li><li>存储量小</li><li>故障恢复能力弱</li></ul><p>我们可以使用Redis分布式缓存解决以上问题。</p><h2 id="Redis持久化解决数据丢失问题"><a href="#Redis持久化解决数据丢失问题" class="headerlink" title="Redis持久化解决数据丢失问题"></a>Redis持久化解决数据丢失问题</h2><h3 id="RDB持久化"><a href="#RDB持久化" class="headerlink" title="RDB持久化"></a>RDB持久化</h3><p>RDB全称 <code>Redis Database Backup file</code>（Redis数据备份文件），也可以称为叫<strong>Redis数据快照</strong>。简单来说就是把内存中的所有数据都记录到磁盘中。当Redis实例故障重启后，从磁盘读取快照文件，恢复数据。快照文件称为<strong>RDB文件</strong>，默认是保存在<strong>当前运行目录</strong>。</p><p>RDB持久化的执行条件有<strong>save命令</strong>、<strong>bfsave命令</strong>、<strong>Redis手动停机</strong>、<strong>触发系统内置RDB条件时</strong>。</p><h4 id="save命令"><a href="#save命令" class="headerlink" title="save命令"></a>save命令</h4><p>使用<code>redis-cli</code>链接到redis中后使用<code>save</code>命令可以立刻执行一次RDB。save命令会导致<strong>主进程执行RDB</strong>，这个过程中<strong>其它所有命令都会被阻塞</strong>。只有在数据迁移时可能用到。</p><h4 id="bgsave命令"><a href="#bgsave命令" class="headerlink" title="bgsave命令"></a>bgsave命令</h4><p>与<code>save</code>命令的执行步骤相同，但<code>bgsave</code>命令执行是异步的，执行后会<strong>开启独立进程完成RDB</strong>，主进程可以持续处理用户请求，不受影响。</p><h4 id="停机"><a href="#停机" class="headerlink" title="停机"></a>停机</h4><p>主动停机时会自动执行一次RDB</p><h4 id="Redis内置RDB条件"><a href="#Redis内置RDB条件" class="headerlink" title="Redis内置RDB条件"></a>Redis内置RDB条件</h4><p>Redis内部有触发RDB的机制，可以在<code>redis.conf</code>文件中找到（保持默认即可），格式如下：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">900秒内，如果至少有1个key被修改，则执行bgsave， 如果是save <span class="string">&quot;&quot;</span> 则表示禁用RDB</span></span><br><span class="line">save 900 1  </span><br><span class="line">save 300 10  </span><br><span class="line">save 60 10000 </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">是否压缩,建议不开启</span></span><br><span class="line">rdbcompression yes</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">RDB文件名称</span></span><br><span class="line">dbfilename dump.rdb  </span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">文件保存的路径目录</span></span><br><span class="line">dir ./ </span><br></pre></td></tr></table></figure><h4 id="RDB原理"><a href="#RDB原理" class="headerlink" title="RDB原理"></a>RDB原理</h4><p>当 bgsave 执行时，主进程会 fork（<u>fork()是unix和linux这种操作系统的一个api</u>）一个子进程，子进程共享主进程的内存数据，完成fork后子进程读取内存数据并写入 RDB 文件。</p><p>当子进程读取内存数据写入 RDB 文件时，主进程可以继续进行工作，依靠的是 <code>copy-on-write</code> 技术。</p><ul><li>当主进程执行读操作时，直接访问共享内存</li><li>当主进程执行写操作时，则会在内存中拷贝一份数据，对拷贝的数据执行写操作，这样不会影响到子进程读取的内存数据</li></ul><h4 id="如果不使用copy-on-write会怎么样？"><a href="#如果不使用copy-on-write会怎么样？" class="headerlink" title="如果不使用copy-on-write会怎么样？"></a>如果不使用copy-on-write会怎么样？</h4><p>不使用copy-on-write，就意味着子进程在进行写RDB文件时，主进程可以修改子进程要读取的内存数据，那么就无法保证某一时刻数据的一致性。</p><h4 id="RDB的缺点"><a href="#RDB的缺点" class="headerlink" title="RDB的缺点"></a>RDB的缺点</h4><ul><li>RDB执行间隔时间长，两次RDB之间写入数据有丢失风险</li><li>fork子进程、压缩、写出RDB文件都比较耗时</li></ul><h3 id="AOF持久化"><a href="#AOF持久化" class="headerlink" title="AOF持久化"></a>AOF持久化</h3><p>AOF全称为<code>Append Only File</code>（追加文件）。Redis处理的<strong>每一个写命令都会记录在AOF文件</strong>，可以看做是命令日志文件。（<strong>由主进程先写入到缓冲区，之后由后台线程将缓冲区中的数据写入到AOF文件</strong>）</p><h4 id="AOF配置"><a href="#AOF配置" class="headerlink" title="AOF配置"></a>AOF配置</h4><p>AOF在<code>redis.conf</code>中默认是关闭的，需要修改<code>redis.conf</code>配置文件开启AOF:</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">是否开启AOF功能，默认是no</span></span><br><span class="line">appendonly yes</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">AOF文件的名称</span></span><br><span class="line">appendfilename &quot;appendonly.aof&quot;</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">表示每执行一次写命令，立即记录到AOF文件</span></span><br><span class="line">appendfsync always </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">写命令执行完先放入AOF缓冲区，然后表示每隔1秒将缓冲区数据写到AOF文件，是默认方案</span></span><br><span class="line">appendfsync everysec </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">写命令执行完先放入AOF缓冲区，由操作系统决定何时将缓冲区内容写回磁盘</span></span><br><span class="line">appendfsync no</span><br></pre></td></tr></table></figure><p><code>appendfsync</code>三种策略对比：</p><table><thead><tr><th align="center">配置项</th><th align="center">刷盘实机</th><th align="center">优点</th><th align="center">缺点</th></tr></thead><tbody><tr><td align="center">always</td><td align="center">同步刷盘</td><td align="center">可靠性高，几乎不丢失数据</td><td align="center">性能影响大</td></tr><tr><td align="center">everysec</td><td align="center">每秒刷盘</td><td align="center">性能适中</td><td align="center">可能会丢失1秒内的数据</td></tr><tr><td align="center">no</td><td align="center">操作系统控制</td><td align="center">性能最好</td><td align="center">可靠性较差，可能丢失大量数据</td></tr></tbody></table><h4 id="AOF文件重写"><a href="#AOF文件重写" class="headerlink" title="AOF文件重写"></a>AOF文件重写</h4><p>因为是记录命令，AOF文件会比RDB文件大的多。而且AOF会记录对<strong>同一个key的多次写操作</strong>，但只有最后一次写操作才有意义。通过执行<code>bgrewriteaof</code>命令，可以<strong>让AOF文件执行重写功能</strong>，用最少的命令达到相同效果。</p><p>如果一直没执行此命令，Redis也会在触发阈值时自动重写AOF文件（<strong>异步执行</strong>）。阈值也可以在<code>redis.conf</code>中配置：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">AOF文件比上次文件 增长超过多少百分比则触发重写</span></span><br><span class="line">auto-aof-rewrite-percentage 100</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">AOF文件体积最小多大以上才触发重写</span> </span><br><span class="line">auto-aof-rewrite-min-size 64mb </span><br></pre></td></tr></table></figure><h3 id="RDB与AOF对比"><a href="#RDB与AOF对比" class="headerlink" title="RDB与AOF对比"></a>RDB与AOF对比</h3><p>在实际开发中往往会结合两者来使用</p><table><thead><tr><th></th><th>RDB</th><th>AOF</th></tr></thead><tbody><tr><td>持久化方式</td><td>定时对整个内存做快照</td><td>每一次执行的命令</td></tr><tr><td>数据完整性</td><td>不完整，两次备份之间会丢失</td><td>相对完整，取决于刷盘策略</td></tr><tr><td>文件大小</td><td>会有压缩，文件体积小</td><td>记录命令，文件体积很大</td></tr><tr><td>宕机恢复速度</td><td>快</td><td>慢</td></tr><tr><td>数据恢复优先级</td><td>低，数据完整性不如AOF</td><td>高，因为数据完整性更高</td></tr><tr><td>系统资源占用</td><td>高，大量CUP和内存消耗</td><td>低，主要是磁盘IO占用，但AOF重写时会占用大量CPU和内存资源</td></tr><tr><td>使用场景</td><td>可容忍数分钟的数据丢失，追求更快的启动速度</td><td>对数据安全性要求较高</td></tr></tbody></table><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><h4 id="RDB"><a href="#RDB" class="headerlink" title="RDB"></a>RDB</h4><ul><li>RDB是一种快照持久化方法，它会在指定的时间间隔内生成数据的完整快照</li><li>适合于灾难恢复，可以很方便的被迁移到另一个数据中心</li><li>RDB在保存快照时速度快，恢复时也非常迅速，适合用作备份</li><li>最后一次快照之后的数据可能会丢失，因为这部分数据还没有被写入快照</li></ul><h4 id="AOF"><a href="#AOF" class="headerlink" title="AOF"></a>AOF</h4><ul><li>AOF记录每一条写命令</li><li>AOF提供了更好的数据安全性，可以配置为每秒同步一次，或者每写入一条命令就同步一次</li><li>AOF文件通常会比RDB文件更大，且恢复速度可能会更慢，但可以通过AOF文件重写进行压缩</li><li>AOF在系统崩溃时能最大化数据恢复，最多只丢失几秒钟的数据</li></ul><p>如果需要<strong>快速恢复</strong>且可以<strong>接受少量数据丢失</strong>，RDB可能是更好的选择。如果<strong>注重数据完整性</strong>且<strong>可以接受较慢的恢复速度</strong>，则应该使用AOF。在很多场景下，结合使用RDB和AOF能提供更为可靠的数据保护机制。</p><h2 id="主从集群解决并发问题"><a href="#主从集群解决并发问题" class="headerlink" title="主从集群解决并发问题"></a>主从集群解决并发问题</h2><p>单节点Redis的并发能力是有上限的，要进一步提高Redis的并发能力，就需要搭建主从集群，实现读写分离。</p><p>主从集群可以是一台Redis服务器作为主节点(master)，数台服务器作为从节点(slave)，主节点只负责写数据，从节点只负责读数据。</p><h3 id="数据同步原理"><a href="#数据同步原理" class="headerlink" title="数据同步原理"></a>数据同步原理</h3><p>主从之间的第一次同步<strong>全量同步</strong>，既要加载RDB文件，又要读取并执行repl_baklog中的命令。</p><p><img src="D:\Blog\source\img\master&slave.png" alt="master&amp;slave"></p><p><strong>如何判断是不是第一次同步？</strong></p><ul><li><strong>replication id</strong>：简称replid，数据集标记，id一致则说明是同一数据集。每一个master都有唯一的replid，slave则会继承master节点的replid</li><li><strong>offset</strong>：偏移量，随着记录在<code>repl_baklog</code>中的数据增多而逐渐增大。slave完成同步时也会记录当前同步的offset。如果slave的offset小于master的offset，说明slave数据落后于master，需要更新。</li></ul><p>因此slave做数据同步，必须向master声明自己的replication id和offset，master才可以判断到底需要同步哪些数据</p><p>一般来讲，如果id能对上就不用做全量同步，但是<code>repl_baklog</code>大小有上限，写满后会覆盖最早的数据。如果slave断开时间过久，导致尚未备份的数据被覆盖，则无法同步基于log做增量同步，只能再次做全量同步。</p><h3 id="优化主从集群性能"><a href="#优化主从集群性能" class="headerlink" title="优化主从集群性能"></a>优化主从集群性能</h3><ul><li>在master中配置<code>repl-diskless-sync yes</code>启用无磁盘复制，避免全量同步时的磁盘IO。</li><li>Redis单节点上的内存占用不要太大，减少RDB导致的过多磁盘IO</li><li>适当提高<code>repl_baklog</code>的大小，发现slave宕机时尽快实现故障恢复，尽可能避免全量同步</li><li>限制一个master上的slave节点数量，如果实在是太多slave，则可以采用主-从-从链式结构，减少master压力</li></ul><h2 id="Redis哨兵解决故障恢复问题"><a href="#Redis哨兵解决故障恢复问题" class="headerlink" title="Redis哨兵解决故障恢复问题"></a>Redis哨兵解决故障恢复问题</h2><p><code>slave</code>节点宕机恢复后可以找<code>master</code>节点同步数据，那<code>master</code>节点宕机怎么办?</p><p>这就需要指定一个<code>slave</code>节点为新的<code>master</code>，执行写操作。这个操作不需要人工手动执行，因为Redis提供了哨兵(Sentinel)机制来实现主从集群的自动故障恢复。<u>哨兵本身也是一个集群</u>。</p><h3 id="哨兵的结构和作用"><a href="#哨兵的结构和作用" class="headerlink" title="哨兵的结构和作用"></a>哨兵的结构和作用</h3><ul><li><strong>监控</strong>：Sentinel会不断检查master和slave是否按预期工作</li><li><strong>自动故障恢复</strong>：如果master故障，Sentinel会将一个slave提升为master。当故障实例恢复后也以新的master为主</li><li><strong>通知</strong>：Sentinel充当Redis客户端的服务发现来源，当集群发生故障转移时，会将最新消息推送给Redis的客户端</li></ul><h3 id="服务状态监控"><a href="#服务状态监控" class="headerlink" title="服务状态监控"></a>服务状态监控</h3><p><code>Sentinel</code>基于心跳机制检测服务状态，每隔1秒向集群的每个实例发送ping命令：</p><ul><li>主观下线：如果<code>Sentinel</code>节点发现某实例未在规定时间相应，则认为该实例主观下线</li><li>客观下线：若超过指定数量（<code>quorum</code>）的<code>Sentinel</code>都认为该实例主观下线，则该实例客观下线。<code>quorum</code>的取值最好超过<code>Sentinel</code>数量的一半</li></ul><h3 id="选举新的master"><a href="#选举新的master" class="headerlink" title="选举新的master"></a>选举新的master</h3><p>一旦发现<code>master</code>故障，<code>Sentinel</code>需要在<code>salve</code>中选择一个作为新的<code>master</code>，选择依据是这样的:</p><ul><li>首先会判断<code>slave</code>节点与<code>master</code>节点断开时间长短，如果超过指定值(down-after-miliseconds*10)则会排除该<code>slave</code>节点</li><li>然后判断<code>slave</code>节点的<code>slave-priority</code>值，越小优先级越高，如果是<u>0</u>则永不参与选举</li><li>如果<code>slave-prority</code>一样，则判断<code>slave</code>节点的<code>offset</code>值，越大说明数据越新，优先级越高</li><li>最后是判断<code>slave</code>节点的运行id大小，越小优先级越高</li></ul><h4 id="实现故障转移"><a href="#实现故障转移" class="headerlink" title="实现故障转移"></a>实现故障转移</h4><p>当选中了其中一个<code>slave</code>为新的<code>master</code>后：</p><ul><li><code>Sentinel</code>给备选的<code>slave</code>节点发送<code>slaveof no one</code>命令，让该节点成为<code>master</code></li><li><code>Sentinel</code>给所有其它<code>slave</code>发送<code>slaveof IP地址 端口</code>命令，让这些<code>slave</code>成为新<code>master</code>的从节点，开始从新的<code>master</code>上同步数据</li><li>最后，<code>Sentinel</code>将故障节点标记为<code>slave</code>，当故障节点恢复后会自动成为新的<code>master</code>的<code>slave</code>节点</li></ul><h3 id="Java中使用RedisTemplate配置哨兵集群"><a href="#Java中使用RedisTemplate配置哨兵集群" class="headerlink" title="Java中使用RedisTemplate配置哨兵集群"></a>Java中使用RedisTemplate配置哨兵集群</h3><p>引入依赖</p><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.springframework.boot<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spring-boot-starter-data-redis<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>配置Redis</p><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">logging:</span></span><br><span class="line">  <span class="attr">level:</span></span><br><span class="line">    <span class="attr">io.lettuce.core:</span> <span class="string">debug</span></span><br><span class="line">  <span class="attr">pattern:</span></span><br><span class="line">    <span class="attr">dateformat:</span> <span class="string">MM-dd</span> <span class="string">HH:mm:ss:SSS</span></span><br><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">redis:</span></span><br><span class="line">    <span class="attr">sentinel:</span></span><br><span class="line">      <span class="attr">master:</span> <span class="string">mymaster</span></span><br><span class="line">      <span class="attr">nodes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">IP地址</span> <span class="string">端口1</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">IP地址</span> <span class="string">端口2</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">IP地址</span> <span class="string">端口3</span></span><br></pre></td></tr></table></figure><p>配置主从读写分离</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@SpringBootApplication</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">RedisDemoApplication</span> &#123;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> &#123;</span><br><span class="line">        SpringApplication.run(RedisDemoApplication.class, args);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 配置主从读写分离</span></span><br><span class="line">    <span class="meta">@Bean</span></span><br><span class="line">    <span class="keyword">public</span> LettuceClientConfigurationBuilderCustomizer <span class="title function_">configurationBuilderCustomizer</span><span class="params">()</span>&#123;</span><br><span class="line">        <span class="keyword">return</span> clientConfigurationBuilder -&gt; clientConfigurationBuilder.readFrom(ReadFrom.MASTER_PREFERRED);</span><br><span class="line">    &#125;</span><br><span class="line"> </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>ReadForm</code>函数是配置Redis的读取策略，有以下选择：</p><ul><li>MASTER：从主节点读取</li><li>MASTER_PREFERRED：优先从master节点读取，master不可使用才读取slave</li><li>REPLICA：从slave节点读取</li><li>REPLICA_PREFERRED：优先从slave节点读取，所有的slave都不可用才读取master</li></ul><h2 id="Redis分片集群解决海量数据存储问题"><a href="#Redis分片集群解决海量数据存储问题" class="headerlink" title="Redis分片集群解决海量数据存储问题"></a>Redis分片集群解决海量数据存储问题</h2><p>分片集群特征：</p><ul><li>集群中有多个<code>master</code>，每个<code>master</code>保存不同数据</li><li>每个<code>master</code>都可以有多个<code>slave</code>节点</li><li><code>master</code>之间通过ping监测彼此健康状态</li><li>客户端请求可以访问集群任意节点，最终都会被转发到正确节点</li></ul><h3 id="散列插槽"><a href="#散列插槽" class="headerlink" title="散列插槽"></a>散列插槽</h3><p>Redis会把每一个master节点映射到<code>0~16384</code>个插槽上，通过查看集群信息时即可看到每个节点插槽的区间</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-cli -p 7001 cluster nodes</span><br></pre></td></tr></table></figure><h3 id="集群伸缩"><a href="#集群伸缩" class="headerlink" title="集群伸缩"></a>集群伸缩</h3><p>redis-cli –cluster提供很多操作集群的命令，可以通过下面的命令查看：</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">redis-cli --cluster help</span><br></pre></td></tr></table></figure><h3 id="故障迁移"><a href="#故障迁移" class="headerlink" title="故障迁移"></a>故障迁移</h3><p>如果集群中某个master宕机了，则该master下优先级最高的slave节点会变成master，选举出的新master会接管原master的槽位（slot）和数据，继续对外提供服务。其他节点会更新自己的路由表，将请求转发到新的master。如果原master重新上线，它会成为新master的slave节点，开始同步数据。</p><p><img src="D:\Blog\source\img\failover.png" alt="failover"></p><h3 id="Redis访问分片集群"><a href="#Redis访问分片集群" class="headerlink" title="Redis访问分片集群"></a>Redis访问分片集群</h3><p>RedisTemplate底层同样基于lettuce实现了分片集群的支持，而使用的步骤与哨兵模式基本一致:</p><ol><li>引入redis的starter依赖</li><li>配置分片集群地址</li><li>配置读写分离</li><li>与哨兵模式相比，其中只有分片集群的配置方式略有差异，如下:</li></ol><figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">spring:</span></span><br><span class="line">  <span class="attr">redis:</span></span><br><span class="line">    <span class="attr">cluster:</span></span><br><span class="line">      <span class="attr">nodes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">IP地址</span> <span class="string">端口1</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">IP地址</span> <span class="string">端口2</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">IP地址</span> <span class="string">端口3</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">IP地址</span> <span class="string">端口4</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">IP地址</span> <span class="string">端口5</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">IP地址</span> <span class="string">端口6</span></span><br></pre></td></tr></table></figure>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/redis/">redis</category>
      
      
      <category domain="http://example.com/tags/redis/">redis</category>
      
      
      <comments>http://example.com/inori/1e2438b1.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>二分查找算法入门</title>
      <link>http://example.com/inori/6b525bdc.html</link>
      <guid>http://example.com/inori/6b525bdc.html</guid>
      <pubDate>Mon, 17 Feb 2025 06:59:22 GMT</pubDate>
      
      <description>算法入门</description>
      
      
      
      <content:encoded><![CDATA[<p>二分查找可以有几种写法：</p><ul><li>闭区间</li><li>开区间</li><li>半开半闭区间（左开右闭&#x2F;左闭右开）</li></ul><p>所谓开闭区间，指的是二分查找中的<code>left</code>和<code>right</code>的值，如果取值在数组内部（大部分情况下），就可以成为闭区间，反之为开区间。</p><p>展示这三种写法，我们从一道<a href="https://leetcode.cn/problems/binary-search/description/">题目</a>开始：</p><hr><p>给定一个 <code>n</code> 个元素有序的（升序）整型数组 <code>nums</code> 和一个目标值 <code>target</code> ，写一个函数搜索 <code>nums</code> 中的 <code>target</code>，如果目标值存在返回下标，否则返回 <code>-1</code>。</p><hr><p>这道题就是纯粹的二分查找板子题，数组有序，数组中无重复元素。</p><p>这里使用<code>Java</code>进行展示，首先是<code>闭区间</code>写法。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">search</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> target)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> nums.length;</span><br><span class="line">        <span class="type">int</span> <span class="variable">l</span> <span class="operator">=</span> <span class="number">0</span>, r = n - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(l &lt;= r)&#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">mid</span> <span class="operator">=</span> l + (r - l &gt;&gt; <span class="number">1</span>);<span class="comment">// 防止爆int</span></span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &lt; target) l = mid + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span> r = mid - <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> nums[l] == target ? l : -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于全程需要<code>left</code>和<code>right</code>处于闭区间内，所以更新后的取值一定是<code>mid+1</code>或<code>mid-1</code>。<code>left=right</code>时，<code>mid</code>也等于<code>right</code>，会陷入死循环，所以要在边界条件的判断中加入<code>=</code>，即<code>while(l &lt;= r)</code>，最后<code>right+1</code>也就是<code>left</code>为所求答案。</p><p>然后是<code>半开半闭</code>区间：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">search</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> target)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> nums.length;</span><br><span class="line">        <span class="type">int</span> <span class="variable">l</span> <span class="operator">=</span> <span class="number">0</span>, r = n;<span class="comment">// 左闭右开</span></span><br><span class="line">        <span class="keyword">while</span>(l &lt; r)&#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">mid</span> <span class="operator">=</span> l + (r - l &gt;&gt; <span class="number">1</span>);</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &lt; target) l = mid + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span> r = mid;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> nums[l] == target ? l : -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>半开半闭写法中，谁是开区间，谁更新后就要等于<code>mid</code>。结束时<code>left=right</code>，两者返回其一即可。</p><p>最后是<code>开区间</code>写法：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">search</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> target)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> nums.length;</span><br><span class="line">        <span class="type">int</span> <span class="variable">l</span> <span class="operator">=</span> -<span class="number">1</span>, r = n;</span><br><span class="line">        <span class="keyword">while</span>(l + <span class="number">1</span> &lt; r)&#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">mid</span> <span class="operator">=</span> l + (r - l &gt;&gt; <span class="number">1</span>);</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &lt; target) l = mid;</span><br><span class="line">            <span class="keyword">else</span> r = mid;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(r == n) <span class="keyword">return</span> -<span class="number">1</span>;<span class="comment">// 由于是开区间，r有可能等于nums.length，所以要特判</span></span><br><span class="line">        <span class="keyword">return</span> nums[r] == target ? r : -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>三种写法只有代码上的区别，最后答案都是一样的，选一个顺眼的写法就好。</p><p>我们来做几道扩展题，提升对二分查找的熟练度：</p><p><a href="https://leetcode.cn/problems/find-first-and-last-position-of-element-in-sorted-array/">在排序数组中查找元素的第一个和最后一个位置</a></p><hr><p>给你一个按照非递减顺序排列的整数数组 <code>nums</code>，和一个目标值 <code>target</code>。请你找出给定目标值在数组中的开始位置和结束位置。</p><p>如果数组中不存在目标值 <code>target</code>，返回 <code>[-1, -1]</code>。</p><p>你必须设计并实现时间复杂度为 <code>O(log n)</code> 的算法解决此问题。</p><hr><p>这道题要求我们在数组中找到一段等于<code>target</code>的子数组，返回其起点和终点的下标，如果没有返回<code>[-1, -1]</code>。</p><p>起始位置很好找，套用二分模板即可。</p><p>终点如何找呢？</p><p>这就要讨论不同的要求了</p><ul><li>如果要求我们寻找<code>&gt;= x</code>时，就直接使用二分模板</li><li>如果要求我们寻找<code>&gt; x</code>时，可以将题目转化为<code>&gt;= x+1</code></li><li>如果要求我们寻找<code>&lt; x</code>时，可以将题目转化为<code>(&gt;= x) - 1</code></li><li>如果要求我们寻找<code>&lt;= x</code>某个值时，可以将题目转化为<code>(&gt; x) - 1</code> -》 <code>(&gt;= x + 1) - 1</code></li></ul><p>要寻找终点，也就是<code>&gt; x</code>的情况。</p><p>题解代码</p><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">    private int[] nums;</span><br><span class="line"></span><br><span class="line">    private int lowerBound(int target) &#123;</span><br><span class="line">        int l = 0, r = nums.length - 1;</span><br><span class="line">        while(l &lt;= r)&#123;</span><br><span class="line">            int mid = l + ((r - l) &gt;&gt; 1);</span><br><span class="line">            if(nums[mid] &lt; target) l = mid + 1;</span><br><span class="line">            else r = mid - 1;</span><br><span class="line">        &#125;</span><br><span class="line">        return l;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public int[] searchRange(int[] nums, int target) &#123;</span><br><span class="line">        this.nums = nums;</span><br><span class="line">        int start = lowerBound(target);</span><br><span class="line">        if(start == nums.length || nums[start] != target) return new int[]&#123;-1, -1&#125;;</span><br><span class="line">        int end = lowerBound(target + 1) - 1;</span><br><span class="line">        return new int[]&#123;start, end&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>二分不止能用在有序数组中，也能用在部分有序的数组中。</p><p>比如<a href="https://leetcode.cn/problems/find-peak-element/solution/by-endlesscheng-9ass/">寻找峰值</a>：</p><hr><p>峰值元素是指其值严格大于左右相邻值的元素。</p><p>给你一个整数数组 <code>nums</code>，找到峰值元素并返回其索引。数组可能包含多个峰值，在这种情况下，返回 <strong>任何一个峰值</strong> 所在位置即可。</p><p>你可以假设 <code>nums[-1] = nums[n] = -∞</code> 。</p><p>你必须实现时间复杂度为 <code>O(log n)</code> 的算法来解决此问题。</p><hr>这道题给定的数组是由很多部分有序的数组组成的，峰值可能有多个，我们只需要找到其中一个即可。<p>这道题为什么能够使用二分查找呢？</p><p>因为我们要找到一个峰值，而峰值是一定比周围两个值高的（如果是边界则为一个值），这里可以使用二分查找来逐渐逼近这个点。</p><p>由于要找峰值，<code>target</code>的选择可以是<code>nums[mid+1]</code>或<code>nums[mid-1]</code>。</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">findPeakElement</span><span class="params">(<span class="type">int</span>[] nums)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">l</span> <span class="operator">=</span> <span class="number">0</span>, r = nums.length - <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">while</span>(l &lt;= r)&#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">mid</span> <span class="operator">=</span> l + ((r - l) &gt;&gt; <span class="number">1</span>);</span><br><span class="line">            <span class="keyword">if</span>(mid + <span class="number">1</span> == nums.length) <span class="keyword">return</span> mid;<span class="comment">// 特殊判断，防止越界</span></span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &lt; nums[mid + <span class="number">1</span>]) l = mid + <span class="number">1</span>;<span class="comment">// 如果mid的下一个比mid大，则峰值一定在mid后面，更新l</span></span><br><span class="line">            <span class="keyword">else</span> r = mid - <span class="number">1</span>; <span class="comment">// 反之更新r</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> l;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后是<a href="https://leetcode.cn/problems/search-in-rotated-sorted-array/">搜索旋转排序数组</a></p><hr><p>整数数组 <code>nums</code> 按升序排列，数组中的值 <strong>互不相同</strong> 。</p><p>在传递给函数之前，<code>nums</code> 在预先未知的某个下标 <code>k</code>（<code>0 &lt;= k &lt; nums.length</code>）上进行了 <strong>旋转</strong>，使数组变为 <code>[nums[k], nums[k+1], ..., nums[n-1], nums[0], nums[1], ..., nums[k-1]]</code>（下标 <strong>从 0 开始</strong> 计数）。例如， <code>[0,1,2,4,5,6,7]</code> 在下标 <code>3</code> 处经旋转后可能变为 <code>[4,5,6,7,0,1,2]</code> 。</p><p>给你 <strong>旋转后</strong> 的数组 <code>nums</code> 和一个整数 <code>target</code> ，如果 <code>nums</code> 中存在这个目标值 <code>target</code> ，则返回它的下标，否则返回 <code>-1</code> 。</p><p>你必须设计一个时间复杂度为 <code>O(log n)</code> 的算法解决此问题。</p><hr><p>这道题给定是一个有序无重复数组经过数次旋转后得到的数组，要求我们在其中找到<code>target</code>并返回其下标，如果没有返回<code>-1</code>。</p><p>我们可以用两次二分来解决这道题，首先类似于上一道题的查找峰值，我们先使用二分找到数组中的最小值，也就是找到两段递增数组的分界点，然后比较<code>target</code>与数组中最后一位数<code>last</code>的大小，如果<code>target</code>比<code>last</code>小，则说明其在第二段递增数组中，否则在第一段。然后最相应的数组再做一次二分即可。</p><p>题解代码：</p><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">search</span><span class="params">(<span class="type">int</span>[] nums, <span class="type">int</span> target)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">n</span> <span class="operator">=</span> nums.length;</span><br><span class="line">        <span class="type">int</span> <span class="variable">l</span> <span class="operator">=</span> <span class="number">0</span>, r = n - <span class="number">2</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">last</span> <span class="operator">=</span> nums[n - <span class="number">1</span>];</span><br><span class="line">        <span class="keyword">while</span>(l &lt;= r) &#123;</span><br><span class="line">            <span class="type">int</span> <span class="variable">mid</span> <span class="operator">=</span> l + (r - l &gt;&gt; <span class="number">1</span>);</span><br><span class="line">            <span class="keyword">if</span>(nums[mid] &gt; last) l = mid + <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">else</span> r = mid - <span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span>(target &lt;= last)&#123;</span><br><span class="line">            r = n - <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">while</span>(l &lt;= r) &#123;</span><br><span class="line">                <span class="type">int</span> <span class="variable">mid</span> <span class="operator">=</span> l + (r - l &gt;&gt; <span class="number">1</span>);</span><br><span class="line">                <span class="keyword">if</span>(nums[mid] &lt; target) l = mid + <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">else</span> r = mid - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(l == nums.length) <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">            <span class="keyword">return</span> nums[l] == target ? l : -<span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            r = l - <span class="number">1</span>;</span><br><span class="line">            l = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span>(l &lt;= r) &#123;</span><br><span class="line">                <span class="type">int</span> <span class="variable">mid</span> <span class="operator">=</span> l + (r - l &gt;&gt; <span class="number">1</span>);</span><br><span class="line">                <span class="keyword">if</span>(nums[mid] &lt; target) l = mid + <span class="number">1</span>;</span><br><span class="line">                <span class="keyword">else</span> r = mid - <span class="number">1</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">return</span> nums[l] == target ? l : -<span class="number">1</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在寻找低谷时，我们将比较对象定为<code>last</code>，这是因为我们要寻找最小值，如果<code>mid</code>比<code>last</code>要大，则说明数组一定被分割过并且<code>mid</code>在最小值左侧，移动<code>left</code>，反之移动<code>right</code>。并且由于我们的比较对象是<code>last</code>，所以二分时不需要考虑它，<code>r</code>直接定为<code>n-2</code>。</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/Algorithms/">Algorithms</category>
      
      
      <category domain="http://example.com/tags/Algorithms/">Algorithms</category>
      
      
      <comments>http://example.com/inori/6b525bdc.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>强化学习基础知识</title>
      <link>http://example.com/inori/64d6f53b.html</link>
      <guid>http://example.com/inori/64d6f53b.html</guid>
      <pubDate>Thu, 06 Jun 2024 08:06:57 GMT</pubDate>
      
      <description>ReinforceLearning基础知识</description>
      
      
      
      <content:encoded><![CDATA[<p>记录一些强化学习基础名词解释，理解的不深</p><h1 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h1><p>强化学习（Reinforcement Learning，RL）是一种机器学习方法，旨在通过与环境的交互来学习如何采取行动以最大化累积奖励。与监督学习不同，强化学习没有预先标注的输入输出对，而是通过试错和从反馈中学习</p><p><code>Agent</code>：做出动作的角色，即智能体</p><p><code>state(状态)</code>：智能体当前所处的环境，包含环境当前的全部或部分信息，在玩游戏时，state就是当前的游戏画面，如果状态，动作，奖励等变量被观测到了，用s表示；没观测到就表示随机变量，用S表示</p><p><code>action(动作)</code>：智能体在每个时间步可以采取的操作</p><p><code>policy(决策)</code>：agent在当前做出的动作</p><p><code>policy function(策略函数)</code>：在某种state下，agent采取某种action的概率$\pi(s.a)$，函数值范围为[0,1]</p><p><code>Reward(奖励)</code>：是指采取动作后，环境给agent的奖励，它可能为正、负、0</p><p><code>state transition(状态转移)</code>：agent在某个状态采取某个动作后，可能发生的状态变化</p><p>强化学习是agent与环境互动的过程。我们观测到状态$s_t$，然后根据策略$\pi(a|s)$选择一个动作$a_t$并执行它，环境会给出一个新的状态$s_{t+1}$和奖励r</p><p><code>Return(回报)</code>：也叫做未来累计奖励，即$U_t&#x3D;R_t+R_{t+1}+R_{t+2}+…+R_n$</p><p>由于现在的奖励可能比未来的奖励更重要，所以定义<code>Discounted Return(折扣回报)</code>：$U_t&#x3D;R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+…+\gamma^n R_n$，$\gamma$意为折扣因子，是我们设定的超参数</p><p><code>Action-Value Function(动作值函数)</code>：用Q(s,a)表示，用于评估在给定状态s下采取某个动作a所能获得的累积奖励的期望值</p><p><code>State-Value Function(状态值函数)</code>：用V(s)表示，用于评估智能体在给定状态s下，从该状态开始到未来所能获得的累积奖励的期望值</p><p><code>Q值</code>：表示在状态s下采取动作a的return</p><p><code>model-based</code>：基于模型的方法涉及构建环境的模型，即尝试估计或学习环境的动态（状态转移概率和奖励函数）。这些方法可以在模型上进行规划，从而选择最佳行动。</p><p><code>model-free</code>：不直接构建环境的模型，而是通过与环境的交互来学习最优策略或价值函数，直接从经验中学习如何行动</p><p><code>policy-based</code>：利用策略梯度优化策略函数，来选择最优行动，直接学习映射状态到行动的策略函数。可以更自然地应用于连续动作空间问题。</p><p><code>value-based</code>：基于价值的方法对价值函数进行建模和优化。通过估计每个状态或状态-动作对的价值来选择最优行动。主要特点包括：</p><ul><li>优化价值函数：学习状态价值函数或动作价值函数。</li><li>间接确定策略：通过选择具有最大价值的动作来确定策略。</li><li>例子：Q-learning，SARSA，DQN</li></ul><h1 id="Value-Based-Reinforcement-Learning"><a href="#Value-Based-Reinforcement-Learning" class="headerlink" title="Value-Based Reinforcement Learning"></a>Value-Based Reinforcement Learning</h1><p>定义Optimal action-value function$Q^*(s_t,a_t)&#x3D;maxQ_\pi(s_t,a_t)$，$Q^*(s_t,a_t)$就是最优情况下的$U_t$</p><p>在价值学习中我们要训练的函数就是$Q(s_t,a_t)$，训练的目标是$Q^*(s_t,a_t)$，采取的最佳动作就是$a^*&#x3D;argmaxQ^*(s_t,a_t)$</p><h2 id="Temporal-Difference-Learning算法"><a href="#Temporal-Difference-Learning算法" class="headerlink" title="Temporal Difference Learning算法"></a>Temporal Difference Learning算法</h2><p>时序差分算法的思想为用局部基于真实观测的数据来更新全局预测的数据，因为它的可信度大于完全基于预测的数据</p><p>TD target：相比于当前估计来说更准确的估计，使用$V(s)←V(s)+α[r+γV(s^′)−V(s)]$更新</p><p>TD error：之前的估计与当前估计的损失值</p><h2 id="Deep-Q-Network"><a href="#Deep-Q-Network" class="headerlink" title="Deep Q Network"></a>Deep Q Network</h2><p>DQN是使用一个神经网络$Q(s,a;w)$来估计action-value function$Q(s,a)$，我们基于TD算法来训练这个神经网络，从而得到Optimal action-value function$Q^*(s_t,a_t)$的估计</p><p>由于$U_t&#x3D;R_t+\gamma R_{t+1}+\gamma^2 R_{t+2}+…+\gamma^n R_n&#x3D;R_t+\gamma U_{t+1}$</p><p>基于这种思想,做一下蒙特卡洛近似，得出DQN的更新公式为$Q(s_t,a_t;w)&#x3D;r_t+\gamma Q(s_{t+1},a_{t+1};w)$</p><p>Loss也与TD算法相似:$L_t&#x3D;\frac{1}{2}[Q(s_t,a_t;w)-y_t]^2 $</p><p>流程总结：</p><ul><li>从当前状态s中选择一个动作a</li><li>执行动作a，获得奖励r并转移到新状态s′</li><li>将经历（s,a,r,s′）存储到回放缓冲区</li><li>从回放缓冲区中随机抽取一个小批量样本</li><li>对每个样本，计算目标Q值(TD target)并更新Q网络参数</li><li>每隔一定步数，更新目标网络参数</li></ul><h1 id="Policy-Based-Reinforcement-Learning"><a href="#Policy-Based-Reinforcement-Learning" class="headerlink" title="Policy-Based Reinforcement Learning"></a>Policy-Based Reinforcement Learning</h1><h2 id="Policy-Function"><a href="#Policy-Function" class="headerlink" title="Policy Function"></a>Policy Function</h2><p>策略函数$π(a|s)$是一个概率密度函数，其输入是某个状态s，输出是在该状态下可能产生的动作a的概率值。假设在状态$s_t$下，Agent可能做出的动作$a_t$可能有n个，那么$π(a_t|s_t)$即输出一个n维向量，其元素对应每个可能做出动作at的概率值。有了这n个概率值，Agent就会在这个向量中做一次随机抽样并做出所得到的动作$a_i$</p><h2 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h2><p>定义状态值函数近似函数$V(s;\theta)&#x3D;\Sigma_a\pi(a|s;\theta)Q_\pi(s,a)$，其中$\theta$是神经网络中的参数。V可以评价状态s和策略网络$π(a|s_t;θ)$的好坏。若给定状态s，策略网络越好，那么V的值越大。因此，我们采用V关于$θ$的随机梯度上升的方法更新参数θ，即策略梯度就是函数V对$\theta$的导数</p><p>大致流程：</p><ul><li>观测状态s</li><li>根据策略函数$\pi$随机采样一个a</li><li>根据$Q(s_tr,a_t)$近似$q_t$</li><li>对策略网络求导</li><li>近似策略梯度$g(a_t,\theta_t)&#x3D;q_t*d_{\theta,t}$</li><li>更新策略网络参数</li></ul><p>那么我们如何估计动作价值函数Q？</p><p>这里有两种方法：</p><p><code>REINFORCE算法</code>：必须获取到一个完整的采样trajectory才能进行一次模型参数的更新，也就是蒙特卡罗方法（MC Method）来估计Q</p><p><code>Actor-Critic（AC）</code>：使用另一个神经网络来近似Qπ</p><h2 id="Actor-Critic-Method"><a href="#Actor-Critic-Method" class="headerlink" title="Actor-Critic Method"></a>Actor-Critic Method</h2><p>Actor-Critic方法，是策略学习和价值学习结合的一种方法</p><p>Actor是策略网络，用来控制Agent运动，可以看做“演员”</p><p>Critic是价值网络，用来给演员打分，可以看做“评论家”</p><h3 id="Actor"><a href="#Actor" class="headerlink" title="Actor"></a>Actor</h3><p>Actor网络的输入为状态s</p><p>价值网络中包含：卷积层，将画面变成特征向量；全连接层，将特征向量转变为相对于动作空间（Action Space）中元素个数对应的向量；再采用Softmax激活函数将其转换为一策略的概率密度</p><h3 id="Critic"><a href="#Critic" class="headerlink" title="Critic"></a>Critic</h3><p>Critic网络的输入为状态s和动作a</p><p>对于两个不同的输入，处理方法如下：</p><p>针对于状态s，从输入中提取特征，获得一个状态特征向量</p><p>针对于动作a，采用全连接层提取特征，获得一个动作特征向量</p><p>然后将两个特征向量进行concat，得到一个更高维的特征向量，再通过一个全连接层输出一个实数$q(s,a;w)$，即Critic对于Agent处于状态s下做出动作a的评价</p><h3 id="训练神经网络"><a href="#训练神经网络" class="headerlink" title="训练神经网络"></a>训练神经网络</h3><ul><li>观测状态s</li><li>根据策略函数$\pi$随机采样一个a</li><li>执行动作a，获取下一个状态$s_{t+1}$和奖励r</li><li>使用TD算法更新Critic网络的参数</li><li>使用策略梯度更新Actor的参数</li></ul><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul><li>观测旧状态$s_t$，用策略网络$\pi$计算概率分布，随机采样动作$a_t$</li><li>执行$a_t$，得到新状态$s_{t+1}$和奖励$r_t$</li><li>根据新状态，随机采样动作$a_{t+1}$(假想的动作，并不会真的做)</li><li>计算价值网络$q_t&#x3D;q(s_t,a_t;w_t)$和$q_{t+1}&#x3D;q(s_{t+1},a_{t+1};w_t)$</li><li>计算TD error：$\delta_t&#x3D;q_t-(r_t+\gamma q_{t+1})$</li><li>对Critic参数求导，更新其参数$w_{t+1}&#x3D;w_t-\alpha \delta_t d_{w,t}$</li><li>对Actor参数求导，更新其参数$\theta_{t+1}&#x3D;\theta_t-\beta q_t d_{\theta,t}$</li></ul><h1 id="Monte-Carlo方法"><a href="#Monte-Carlo方法" class="headerlink" title="Monte Carlo方法"></a>Monte Carlo方法</h1><p>蒙特卡洛是一类通过随机采样和统计分析来估计问题解的方法，在强化学习中用于评估策略和估计价值函数</p><h2 id="基本概念-1"><a href="#基本概念-1" class="headerlink" title="基本概念"></a>基本概念</h2><p><strong>随机采样</strong>：通过从可能的结果中随机采样，来估计某个数量的期望值或概率分布。</p><p><strong>经验回报</strong>：在每个episode中，通过累积获得的实际回报来估计状态或状态-动作对的价值。</p><h2 id="主要步骤"><a href="#主要步骤" class="headerlink" title="主要步骤"></a>主要步骤</h2><p><strong>生成轨迹</strong>：根据当前策略，从起始状态开始，生成多条轨迹，每条轨迹包含一系列状态、动作和奖励，直到达到终止状态</p><p><strong>计算回报</strong>：对每条轨迹，计算每个时间步Return</p><p><strong>更新价值估计</strong>：根据每个状态或状态-动作对的回报，更新价值函数或Q值函数</p><h1 id="Sarsa算法"><a href="#Sarsa算法" class="headerlink" title="Sarsa算法"></a>Sarsa算法</h1><p>SARSA（State-Action-Reward-State-Action）是一种基于TD方法的强化学习算法，用于学习状态-动作价值函数。SARSA是一个on-policy算法，直接使用当前策略选择的动作来更新Q值</p><h2 id="表格形式的Sarsa"><a href="#表格形式的Sarsa" class="headerlink" title="表格形式的Sarsa"></a>表格形式的Sarsa</h2><p>如果agent的状态和动作是有限的，那么可以画一个表格，一行对应一个状态$s_i$，一列对应一个动作$a_j$，那么表中的每个元素则对应着在该状态和该动作下的动作价值$Q_π(s_i,a_j)$。我们要做的就是用Sarsa算法去更新表格，每次更新一个元素。</p><p>当我们观测到一个四元组$（s_t,a_t,r_t,s_{t+1}）$，这样的一个四元组被称为transition。然后采用策略函数$π$去采样一个$a_{t+1}$，接着计算TD target $y_t&#x3D;r_t+\gamma Q(s_{t+1},a_{t+1})$，Q值可以直接通过查表获得。同时我们也能计算出TD error $δ_t$，然后利用$δ_t$更新$Q_π(st,at)$，其中α是学习率</p><h2 id="神经网络形式的Sarsa"><a href="#神经网络形式的Sarsa" class="headerlink" title="神经网络形式的Sarsa"></a>神经网络形式的Sarsa</h2><p>采用价值网络来近似$Q_π(s,a)$，记为$q(s,a;w)$。动作价值函数Qπ和价值网络q都与策略π有关，策略π的好坏会影响这两个函数</p><p>神经网络$q(s,a;w)$被称为价值网络，它的输入是一个状态s，输出是状态s下对应动作的价值。如果有n个动作，那么价值网络q就会输出一个n维向量，向量元素对应在状态s下，各动作a的价值</p><p>TD target就变成了$y_t&#x3D;r_t+\gamma q(s_{t+1},a_{t+1};w)$，TD error变成$δ_t&#x3D;q(s_t,a_t;w)-y_t$</p><p>然后计算Loss，求梯度，更新网络参数w即可</p><h2 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h2><p>Sarsa算法主要是用于学习动作价值函数$Q_π(s,a)$</p><ul><li>表格形式（直接学习Qπ）</li></ul><p>适合有限个状态和动作且数量不大的情况。通过一张表格记录每种状态和动作对应的$Q_π(s,a)$的值，通过采用TD算法更新表中Q值。</p><ul><li>神经网络形式（采用函数近似）</li></ul><p>这种形式并不是直接获取Q，而是通过价值网络q(s,a;w)来近似动作价值函数Q</p><h1 id="Q-Learning算法"><a href="#Q-Learning算法" class="headerlink" title="Q-Learning算法"></a>Q-Learning算法</h1><p>Q-Learning是用于学习最优动作价值函数$Q^*(s,a)$</p><p>TD target的更新函数为$y_t&#x3D;r_t+\gamma maxQ^*(s_{t+1},a)$，对Q*求最大化</p><p>其他部分与Sarsa相同，只是Sarsa算法是学习动作价值函数Qπ；而Q-Learning是学习最优动作价值函数Q*</p><h1 id="Experience-Replay"><a href="#Experience-Replay" class="headerlink" title="Experience Replay"></a>Experience Replay</h1><p>我们通常使用TD算法来训练DQN，回顾一下TD算法的步骤：</p><ul><li>Agent在t时刻观测到一个状态st并做出动作at</li><li>Agent与环境交互获得了一个新的状态st+1并且获得了一个奖励rt</li><li>得到TD Target yt</li><li>计算TD error δt</li></ul><p>我们的目标是让qt接近yt，即TD error δt尽量的小，因此TD算法就是寻找一个神经网络参数w使损失函数（Loss Function）L(w)尽可能的小</p><ul><li>采用梯度下降来使$w_t$不断逼近w*</li></ul><p>$（s_t,a_t,r_t,s_{t+1}）$是一个transition，可以认为是一条训练数据，传统算法在使用完该训练数据后就把它丢掉，不再使用，这种对于DQN的训练效果并不好</p><h2 id="Replay-Buffer"><a href="#Replay-Buffer" class="headerlink" title="Replay Buffer"></a>Replay Buffer</h2><p>我们在使用完一个transition时，会把一个transition放入一个队列里，这个队列被称之为回放缓冲区（Replay Buffer），它的容量是一个超参数n，Replay Buffer可以存储n条transitions。如果Replay Buffer满了，那么新来的transition会替代老的transition</p><h3 id="TD算法中经验回放"><a href="#TD算法中经验回放" class="headerlink" title="TD算法中经验回放"></a>TD算法中经验回放</h3><p>我们通过找到神经网络参数w来最小化损失函数Loss Function。</p><p>使用随机梯度下降（Stochastic Gradient Descent SGD）来最小化Loss Function：从buffer中随机抽取一批transition，计算TD error δi，再算出随机梯度gi，调整神经网络参数w</p><h1 id="小技巧"><a href="#小技巧" class="headerlink" title="小技巧"></a>小技巧</h1><h2 id="Prioritized-Experience-Replay"><a href="#Prioritized-Experience-Replay" class="headerlink" title="Prioritized Experience Replay"></a>Prioritized Experience Replay</h2><p>对于Agent而言，并不是所有的transition都同等重要，有些state较难遇见，统一抽样的话难以抽到，训练出的DQN对于数量较少的场景不熟悉，所以预测就会偏离TD Target，因此产生的TD error就比较大，即|δt|就大。正因为DQN不熟悉数量较少的场景，所以要让DQN给这些场景更高的优先级，让它更好的应对这样的场景。</p><p>优先经验回放的核心在于：使用非均匀抽样代替均匀抽样</p><p>这里有两种抽样方式：</p><ul><li>抽样概率pt正比于|δt|+ε：即TD error越大，被抽到的概率就越大</li><li>对|δt|排序：|δt|越大越靠前，容易被抽到</li></ul><h3 id="学习率调整"><a href="#学习率调整" class="headerlink" title="学习率调整"></a>学习率调整</h3><p>TD算法采用SGD来更新神经网络参数w，α是学习率，如果做均匀抽样，所有的transitions的学习率α都相同；如果做非均匀抽样，那么就要根据每个transition的重要性来调整学习率。</p><p>如果一个transition有一个较大的抽样概率pt，那么对这个transition的学习率就应该调小，可以采用如下圈出的方法来自适应调整学习率的因子：如果pt很大，那么学习率就会变小，学习率减小可以减小训练过程中的波动，提高收敛速度和精度，设置学习率为$\alpha(np_t)^{-\beta}$</p><h3 id="更新TD-Error"><a href="#更新TD-Error" class="headerlink" title="更新TD Error"></a>更新TD Error</h3><p>为了做优先经验回放，我们要对每一个transition标记上TD error δt。δt决定了这条transition的重要性，决定了它被抽样的概率。</p><p>如果一个transition刚刚被收集到，我们并不知道它的δt，那么我们就直接把它的δt设置为最大值，让它有最高的优先级。</p><p>每次从buffer中抽取一个transition，都要对它的δt进行一次更新</p><h2 id="缓解高估问题"><a href="#缓解高估问题" class="headerlink" title="缓解高估问题"></a>缓解高估问题</h2><p>由于DQN选择的是$maxQ^*(s_{t+1},a)$，而且原始的DQN使用Bootstrapping方式更新自己的参数，会导致严重的非均匀高估问题。</p><p>这里有两种方法用于缓解高估问题：</p><ul><li><p>使用目标网络（Target Network）</p></li><li><p>Double DQN</p></li></ul><h3 id="Target-Network"><a href="#Target-Network" class="headerlink" title="Target Network"></a>Target Network</h3><p>相比于DQN，Target Network是独立于DQN的。它与DQN有着相同的神经网络结构，但有不同的参数，记作w-。</p><p>我们使用Q(s,a;w)来控制Agent并且收集transition</p><p>使用Q(s,q;w-)来选择动作和计算TD target。</p><p>以前使用DQN来计算yt，用yt来更新DQN的参数。这会产生自举。现在我们使用Target Network，用Target Network来计算yt。这样就可以缓解高估问题。</p><h3 id="Double-DQN"><a href="#Double-DQN" class="headerlink" title="Double DQN"></a>Double DQN</h3><p>DDQN在选择动作时使用的是DQN原始的网络，在计算TD target时使用的是target network，这样相比原始DQN和target network可以更加缓解高估问题</p><h1 id="离散动作空间与连续动作空间"><a href="#离散动作空间与连续动作空间" class="headerlink" title="离散动作空间与连续动作空间"></a>离散动作空间与连续动作空间</h1><p>离散动作空间是一个集合，其中包含有限个动作。比如在玩2D横版过关游戏时，agent的动作空间只能是上下左右四种动作。对于动作空间是离散的agent可以使用传统的SARSA、DQN算法进行训练</p><p>连续动作空间就像是gym的摆杆子环境，或者机械臂控制，由于可以[0,360]度旋转，所以有无穷多种动作空间。对于连续的动作空间，可以使用离散化将连续动作空间变成离散的，但是对于自由度很高的agent，离散化的点就会很多，这会导致维度灾难，离散化只适合较小的连续动作空间</p><h2 id="Deterministic-Policy-Gradient"><a href="#Deterministic-Policy-Gradient" class="headerlink" title="Deterministic Policy Gradient"></a>Deterministic Policy Gradient</h2><p>DPG是策略梯度的变体，专门用于处理确定性策略。与传统的基于随机策略的策略梯度方法不同，DPG直接优化确定性策略，可以处理连续动作空间的问题。</p><p>在DPG中，策略$\mu(s; \theta)$是确定性的，即给定一个状态s，策略直接输出一个具体的动作a：$a &#x3D; \mu(s; \theta))$</p><p>DPG是属于AC方法，有一个策略网络，有一个价值网络</p><p>策略网络是一个确定性的网络，它的输入是状态s，它的输出不是概率分布，而是一个确定性的动作。只要给定状态s，那么对应的动作a就是一个具体的动作</p><p>价值网络有两个输入，一个是状态s一个是动作a，基于状态s，Critic对a进行打分。</p><h3 id="DPG的更新策略"><a href="#DPG的更新策略" class="headerlink" title="DPG的更新策略"></a>DPG的更新策略</h3><p>训练策略网络需要价值网络的帮忙，改善策略网络的参数θ以至于动作a可以依靠策略网络变得更好，而更新参数θ是要根据价值网络q(s,a;w)&#x3D;q(s,π(s;θ);w)的增加而决定的。</p><p>因而我们训练策略网络的目标是让价值网络q的输出变的更大。价值网络的输入是状态s,a，对于确定的状态，确定性策略网络会输出对应的动作a。</p><p>如果输入的状态s是固定的，Critic也是固定的，那么唯一会影响q（Critic的打分）的因素，就是Actor的参数θ。我们想要更新θ，让q变大，那么就可以求取q对θ的梯度，然后使用梯度上升来更新参数θ，这样就可以让价值q变大。这个梯度就叫做确定策略梯度DPG，它是价值q关于Actor参数θ的梯度</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/ReinforceLearning/">ReinforceLearning</category>
      
      
      <category domain="http://example.com/tags/ReinforceLearning/">ReinforceLearning</category>
      
      
      <comments>http://example.com/inori/64d6f53b.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>pytorch基础入门</title>
      <link>http://example.com/inori/8f832495.html</link>
      <guid>http://example.com/inori/8f832495.html</guid>
      <pubDate>Thu, 23 May 2024 08:30:34 GMT</pubDate>
      
      <description>pytorch入门</description>
      
      
      
      <content:encoded><![CDATA[<h1 id="两个实用函数"><a href="#两个实用函数" class="headerlink" title="两个实用函数"></a>两个实用函数</h1><p>假如pytorch是一个大型工具箱，我们想查看工具箱中有什么工具，这时就可以使用<code>dir()</code>函数查看，如果我们想知道某一个工具是如何使用的，就可以使用<code>help()</code>函数查看</p><ul><li><code>dir()</code>：可以查看指定对象包含的全部内容，包括变量、方法、函数和类等。不仅包含可供我们调用的模块成员，还包含所有名称以双下划线“__”开头和结尾的“特殊”命名的私有成员，这些成员是在本模块中使用的，不能在类的外部调用。</li><li><code>help()</code>：查看指定对象（类型、模块、变量、方法等）的详细使用说明</li></ul><h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><p>在学习和使用pytorch时，要经常使用<a href="https://pytorch.org/docs/stable/index.html">官方文档</a>，里面有详细的使用说明</p><h2 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h2><h3 id="加载数据方法及label形式"><a href="#加载数据方法及label形式" class="headerlink" title="加载数据方法及label形式"></a>加载数据方法及label形式</h3><p>Pytorch中加载数据需要Dataset、Dataloader</p><ul><li>Dataset提供一种方式去获取每个数据及其对应的label和编号，以及总共有多少个数据</li><li>Dataloader为后面的网络提供不同的数据形式，可以将数据进行打包</li></ul><p>label形式</p><ul><li>文件夹名即为label。文件夹中存放若干条数据</li><li>一个文件夹存放数据，数据有编号，另一个文件夹存放数据对应编号的说明文本（txt），文本中有label</li><li>直接把label写在数据的名称上</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br></pre></td></tr></table></figure><h3 id="通过路径加载数据"><a href="#通过路径加载数据" class="headerlink" title="通过路径加载数据"></a>通过路径加载数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img_path = <span class="string">&quot;数据路径/train/ants/0013035.jpg&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure><h3 id="python特殊方法补充"><a href="#python特殊方法补充" class="headerlink" title="python特殊方法补充"></a>python特殊方法补充</h3><p>Python中有很多特殊方法，这些特殊方法的命名都以双下划线 <code>__</code>开头和结尾，它们是Python中常用的特殊方法。通过定义这些方法，我们可以自定义对象的行为和操作，使得对象能够更好地适应我们的需求</p><ul><li><code>__init__(self[, args...])</code>: 构造函数，用于在创建对象时进行初始化。即Java中的构造器</li><li><code>__repr__(self)</code>: 用于定义对象的字符串表示形式，通常用于调试和记录日志</li><li><code>__str__(self)</code>: 用于定义对象的字符串表示形式，通常用于显示给终端用户。即Java中的toString</li><li><code>__len__(self)</code>: 用于返回对象的长度，通常在对像被视为序列或集合时使用</li><li><code>__getitem__(self, key)</code>: 用于实现索引操作，可以通过索引或切片访问对象中的元素</li><li><code>__setitem__(self, key, value)</code>: 用于实现索引赋值操作，可以通过索引或切片为对象中的元素赋值</li><li><code>__delitem__(self, key)</code>: 用于实现删除某个元素的操作，可以通过索引或切片删除对象中的元素</li><li><code>__contains__(self, item)</code>: 用于检查对象是否包含某个元素，可以通过 in 关键字使用</li><li><code>__enter__(self)</code>: 用于实现上下文管理器的进入操作，通常与 with 语句一起使用</li><li><code>__exit__(self, exc_type, exc_value, traceback)</code>: 用于实现上下文管理器的退出操作，通常与 with 语句一起使用</li><li><code>__call__(self[, args...])</code>: 用于使对象能够像函数一样被调用，通常在创建可调用的类时使用</li><li><code>__eq__(self, other)</code>: 用于定义对象相等的比较操作，可以通过 &#x3D;&#x3D; 运算符使用</li><li><code>__lt__(self, other)</code>: 用于定义对象小于的比较操作，可以通过 &lt; 运算符使用</li><li><code>__gt__(self, other)</code>: 用于定义对象大于的比较操作，可以通过 &gt; 运算符使用</li><li><code>__add__(self, other)</code>: 用于实现对象加法操作，可以通过 + 运算符使用</li><li><code>__sub__(self, other)</code>: 用于实现对象减法操作，可以通过 - 运算符使用</li><li><code>__mul__(self, other)</code>: 用于实现对象乘法操作，可以通过 * 运算符使用</li><li><code>__truediv__(self, other)</code>: 用于实现对象除法操作，可以通过 &#x2F; 运算符使用</li></ul><h3 id="Dataset加载数据示例"><a href="#Dataset加载数据示例" class="headerlink" title="Dataset加载数据示例"></a>Dataset加载数据示例</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset  </span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image  </span><br><span class="line"><span class="keyword">import</span> os </span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, label_dir</span>):</span><br><span class="line">        self.root_dir = root_dir  <span class="comment"># 记录数据集根目录的路径</span></span><br><span class="line">        self.label_dir = label_dir  <span class="comment"># 记录数据集标签目录的名称</span></span><br><span class="line">        self.path = os.path.join(self.root_dir, self.label_dir)  <span class="comment"># os.path.join可将两个字符串拼接成一个完整路径，以获取数据集标签目录的完整路径</span></span><br><span class="line">        self.img_path = os.listdir(self.path)  <span class="comment"># os.listdir() 函数用于获取指定目录下的所有文件和文件夹的名称列表，以获取数据集标签目录下所有图像文件的路径</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):  <span class="comment"># 获取数据集中指定索引位置的数据项</span></span><br><span class="line">        img_name = self.img_path[idx]  <span class="comment"># 获取图像文件名</span></span><br><span class="line">        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)  <span class="comment"># 获取该图像文件的完整路径</span></span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_item_path)  <span class="comment"># 打开图像文件</span></span><br><span class="line">        label = self.label_dir  <span class="comment"># 获取该图像文件所属的标签</span></span><br><span class="line">        <span class="keyword">return</span> img, label  <span class="comment"># 返回图像和标签</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root_dir = <span class="string">&quot;data/train&quot;</span>   <span class="comment"># 数据集根目录的路径</span></span><br><span class="line">ants_label_dir = <span class="string">&quot;ants&quot;</span>     <span class="comment"># 蚂蚁标签目录的名称</span></span><br><span class="line">bees_label_dir = <span class="string">&quot;bees&quot;</span>     <span class="comment"># 蜜蜂标签目录的名称</span></span><br><span class="line">ants_dataset = MyData(root_dir, ants_label_dir)  <span class="comment"># 创建蚂蚁数据集对象</span></span><br><span class="line">bees_dataset = MyData(root_dir, bees_label_dir)   <span class="comment"># 创建蜜蜂数据集对象</span></span><br><span class="line">train_dataset = ants_dataset + bees_dataset  <span class="comment"># 合并蚂蚁和蜜蜂数据集，得到训练集</span></span><br><span class="line">img, label = train_dataset[<span class="number">200</span>]  <span class="comment"># 自动调用__getitem__() 方法，获取训练集中第 200 个数据项的图像和标签 </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;label：&quot;</span>, label)   <span class="comment"># 查看该数据项的标签</span></span><br><span class="line">img.show()   </span><br></pre></td></tr></table></figure><h2 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h2><p>Tensorboad可以用来查看loss是否按照预想的变化，或者查看训练到某一步输出的图像是什么样</p><h3 id="Tensorboard使用示例"><a href="#Tensorboard使用示例" class="headerlink" title="Tensorboard使用示例"></a>Tensorboard使用示例</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)<span class="comment"># 创建一个 SummaryWriter 对象，指定日志存储目录为 &quot;logs&quot;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># 将 y=x 的函数值添加到 TensorBoard 中</span></span><br><span class="line">    writer.add_scalar(<span class="string">&quot;y=x&quot;</span>, i, i)</span><br><span class="line">writer.close()<span class="comment"># 关闭 SummaryWriter 对象</span></span><br></pre></td></tr></table></figure><p>运行完后会在当前目录下创建一个logs文件夹</p><p>在终端运行</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tensorboard --logdir=logs --port=6007</span><br></pre></td></tr></table></figure><h2 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h2><p>Transforms当成工具箱的话，里面的class就是不同的工具。例如像totensor、resize这些工具。Transforms拿一些特定格式的图片，经过Transforms里面的工具，获得我们想要的结果</p><h3 id="transforms-Totensor"><a href="#transforms-Totensor" class="headerlink" title="transforms.Totensor"></a>transforms.Totensor</h3><p>Tensor包装了神经网络需要的一些属性，比如反向传播、梯度等属性</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;/bees/10870992_eebeeb3a12.jpg&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor()  <span class="comment"># 创建 transforms.ToTensor类 的实例化对象</span></span><br><span class="line">tensor_img = tensor_trans(img)  <span class="comment"># 调用 transforms.ToTensor类的__call__方法   </span></span><br><span class="line"></span><br><span class="line">writer.add_image(<span class="string">&quot;Temsor_img&quot;</span>,tensor_img)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h3 id="transforms-Resize"><a href="#transforms-Resize" class="headerlink" title="transforms.Resize()"></a>transforms.Resize()</h3><p>调整图像的大小到指定的尺寸</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;data/Images/1.jpg&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将图片转为totensor类型</span></span><br><span class="line">trans_totensor = transforms.ToTensor() </span><br><span class="line">img_tensor = trans_totensor(img)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># resize图片，PIL数据类型的 img -&gt; resize -&gt; PIL数据类型的 img_resize</span></span><br><span class="line">trans_resize = transforms.Resize((<span class="number">512</span>,<span class="number">512</span>))  <span class="comment"># 调整尺寸为512*512</span></span><br><span class="line">img_resize = trans_resize(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># PIL 数据类型的 PIL -&gt; totensor -&gt; img_resize tensor</span></span><br><span class="line">img_resize = trans_totensor(img_resize)</span><br><span class="line"><span class="built_in">print</span>(img_resize.size()) </span><br></pre></td></tr></table></figure><h4 id="transforms-Compose"><a href="#transforms-Compose" class="headerlink" title="transforms.Compose"></a>transforms.Compose</h4><p>transforms.Compose 的作用是将多个数据预处理操作组合在一起，方便地对数据进行一次性处理</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;data/Images/1.jpg&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor() </span><br><span class="line">img_tensor = tensor_trans(img)  </span><br><span class="line"></span><br><span class="line">trans_resize_2 = transforms.Resize(<span class="number">512</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># PIL —— resize -&gt; PIL ——  totensor -&gt; tensor</span></span><br><span class="line">trans_compose = transforms.Compose([trans_resize_2, trans_totensor]) <span class="comment"># Compose函数中前面一个参数的输出为后面一个参数的输入，即trans_resize_2输出了pil，作为trans_totensor的输入</span></span><br><span class="line">img_resize_2 = trans_compose(img)</span><br><span class="line"><span class="built_in">print</span>(img_resize_2.size()) </span><br></pre></td></tr></table></figure><h2 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h2><p>DataLoader可以将数据集进行批量处理</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备的测试数据集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor())    </span><br><span class="line"></span><br><span class="line"><span class="comment"># batch_size=4 使得 img0, target0 = dataset[0]、img1, target1 = dataset[1]、img2, target2 = dataset[2]、img3, target3 = dataset[3]，然后这四个数据作为Dataloader的一个返回      </span></span><br><span class="line">test_loader = DataLoader(dataset=test_data,batch_size=<span class="number">4</span>,shuffle=<span class="literal">True</span>,num_workers=<span class="number">0</span>,drop_last=<span class="literal">True</span>)      </span><br><span class="line"><span class="comment"># 用for循环取出DataLoader打包好的四个数据</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">        imgs, targets = data <span class="comment"># 每个data都是由4张图片组成，imgs.size 为 [4,3,32,32]，四张32×32图片三通道，targets由四个标签组成             </span></span><br><span class="line">        writer.add_images(<span class="string">&quot;Epoch：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch), imgs, step) <span class="comment"># 注意是images</span></span><br><span class="line">        step = step + <span class="number">1</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="nn-Module"><a href="#nn-Module" class="headerlink" title="nn.Module"></a>nn.Module</h3><p>orch.nn.Module是所有神经网络基本骨架，需要重写<code>__init__</code>和forward（前向传播）函数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Module, self).__init__()  <span class="comment"># 继承</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):           <span class="comment"># 前向传播</span></span><br><span class="line">        output = <span class="built_in">input</span> + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line">m = Module()</span><br><span class="line">x = torch.tensor(<span class="number">1.0</span>)  <span class="comment"># 创建一个值为 1.0 的tensor</span></span><br><span class="line">output = m(x)</span><br><span class="line"><span class="built_in">print</span>(output) <span class="comment"># 2</span></span><br></pre></td></tr></table></figure><h3 id="卷积函数"><a href="#卷积函数" class="headerlink" title="卷积函数"></a>卷积函数</h3><p>convolution卷积，conv2d表示二维</p><p>weight卷积核的大小，bias偏置，stride步长，padding填充，group是分组卷积，对不同的通道使用不同的卷积核，言外之意一般是对每个通道使用相同卷积核</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Module, self).__init__()</span><br><span class="line">        self.conv1 = Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">6</span>,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span>) <span class="comment"># 彩色图像输入为3层，我们想让它的输出为6层，选3 * 3 的卷积                </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">m = Module()</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = m(imgs)</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)   <span class="comment"># 输入为3通道32×32的64张图片</span></span><br><span class="line">    <span class="built_in">print</span>(output.shape) <span class="comment"># 输出为6通道30×30的64张图片</span></span><br></pre></td></tr></table></figure><h3 id="最大池化"><a href="#最大池化" class="headerlink" title="最大池化"></a>最大池化</h3><p>nn.MaxPool2d最大池化（下采样，最常用），nn.MaxUnpool2d（上采样），nn.AdaptiveMaxPool2d（自适应最大池化），ceil_mode&#x3D;True表示进位，默认为False，写论文会用到公式可以在官网查阅</p><p>最大池化的作用：保留输入的特征，同时把数据量减少，比如视频变720P</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MaxPool2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Module, self).__init__()</span><br><span class="line">        self.maxpool = MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.maxpool(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">m = Module()  <span class="comment"># 即调用forward()</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>, imgs, step)</span><br><span class="line">    output = m(imgs)</span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>, output, step)</span><br><span class="line">    step = step + <span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="非线性激活"><a href="#非线性激活" class="headerlink" title="非线性激活"></a>非线性激活</h3><p>作用：神经网络中引入非线性的特质，才能训练出符合各种特征的模型</p><p>nn.ReLU()小于0进行截断，大于0不变，nn.Sigmoid非线性缩放到[0,1]，1&#x2F;1+exp-x，inplace&#x3D;True表示把原来的值也改变（本来是通过返回值获取值）</p><h3 id="线性层及其它层"><a href="#线性层及其它层" class="headerlink" title="线性层及其它层"></a>线性层及其它层</h3><ul><li>Normalization Layers正则化层：加快神经网络的训练速度</li><li>Recurrent Layers用于文字识别，特定的网络结构，用的不多</li><li>Linear Layers：全连接层，用的较多</li><li>Dropout Layers随机将一些数设为0，防止过拟合</li><li>Distance Functions计算两个值之间的误差，常用余弦相似度</li><li>Loss Functions损失函数，常用值nn.MSELoss，nn.CrossEntropyLoss，nn.BCELoss，分布nn.NLLLoss，nn.KLDivLoss</li><li>torch.flatten()把输入展成一行，与reshape不同</li></ul><h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><ul><li>Loss损失函数一方面计算实际输出和目标之间的差距</li><li>Loss损失函数另一方面为我们更新输出提供一定的依据(反向传播)</li></ul><p><strong>L1loss</strong>:差值的绝对值之和，再求平均值</p><p><strong>MSE</strong>:平方差</p><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>梯度下降是一种优化算法，用于最小化损失函数（Loss Function）。神经网络的训练目标是通过不断调整网络的参数（即权重和偏置）来最小化损失函数的值。梯度下降算法通过计算损失函数相对于网络参数的梯度来指导参数更新的方向和步幅。</p><h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>反向传播是一种高效计算梯度的方法，适用于多层神经网络。反向传播算法通过链式法则（Chain Rule）逐层计算损失函数对每个参数的梯度。</p><p><strong>反向传播和梯度下降的关系</strong></p><ol><li><strong>目标一致</strong>：两者的目标都是通过调整神经网络的参数来最小化损失函数</li><li><strong>互补</strong>：反向传播计算损失函数对参数的梯度，而梯度下降利用这些梯度更新参数</li><li><strong>训练过程</strong>：在神经网络的训练过程中，反向传播和梯度下降是交替进行的。首先进行前向传播，计算损失并通过反向传播计算梯度，然后使用梯度下降更新参数</li></ol><h3 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h3><p>使用优化器时，首先需要optim.zero_grad()，把上一步的梯度清零，否则会累加，然后进行反向传播，再optimizer.step()，对weight参数进行更新</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Module, self).__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()  <span class="comment"># 交叉熵</span></span><br><span class="line">m = Module()</span><br><span class="line">optim = torch.optim.SGD(tudui.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 随机梯度下降优化器 lr为学习率,学习率太大不稳定,太小收敛慢</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化20轮</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = m(imgs)</span><br><span class="line">        result_loss = loss(outputs, targets) <span class="comment"># 计算实际输出与目标输出的差距</span></span><br><span class="line">        optim.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">        result_loss.backward() <span class="comment"># 反向传播，计算损失函数的梯度</span></span><br><span class="line">        optim.step()   <span class="comment"># 根据梯度，对网络的参数进行调优</span></span><br><span class="line">        running_loss = running_loss + result_loss</span><br><span class="line">    <span class="built_in">print</span>(running_loss) <span class="comment"># 每轮误差的总和</span></span><br></pre></td></tr></table></figure><h3 id="train-和tudui-eval-方法"><a href="#train-和tudui-eval-方法" class="headerlink" title="train()和tudui.eval()方法"></a>train()和tudui.eval()方法</h3><p>分别用于训练步骤和测试步骤，对特定层起作用，最好可以在训练和评估之前加上这个方法</p><h2 id="网络模型的保存和读取"><a href="#网络模型的保存和读取" class="headerlink" title="网络模型的保存和读取"></a>网络模型的保存和读取</h2><p>使用<code>save</code>方法保存网络模型的结构和参数，<code>load</code>加载时候要把模型定义给复制过来</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleModel, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fc(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建实例</span></span><br><span class="line">model = SimpleModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存权重</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model_weights.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载权重</span></span><br><span class="line">loaded_model = SimpleModel()</span><br><span class="line">loaded_model.load_state_dict(torch.load(<span class="string">&#x27;model_weights.pth&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确保模型和加载的权重一致</span></span><br><span class="line">loaded_model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以继续使用加载的模型进行预测等操作</span></span><br></pre></td></tr></table></figure><h2 id="利用GPU训练"><a href="#利用GPU训练" class="headerlink" title="利用GPU训练"></a>利用GPU训练</h2><p>在程序之前定义:</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure><p>然后找到</p><ul><li>网络模型</li><li>数据（输入、标注）</li><li>损失函数</li></ul><p>后面加上<code>to(device)</code>即可转到GPU训练</p><h2 id="完整版实战"><a href="#完整版实战" class="headerlink" title="完整版实战"></a>完整版实战</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练的设备</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Module, self).__init__()        </span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  <span class="comment"># 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),  <span class="comment"># 展平后变成 64*4*4 了</span></span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line"></span><br><span class="line"><span class="comment"># length 长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用 Dataloader 来加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=<span class="number">64</span>)        </span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line">m = Module() </span><br><span class="line">m = m.to(device) <span class="comment"># 也可以不赋值，直接m.to(device) </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss() <span class="comment"># 交叉熵</span></span><br><span class="line">loss_fn = loss_fn.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">learning = <span class="number">0.01</span></span><br><span class="line">optimizer = torch.optim.SGD(tudui.parameters(),learning)   <span class="comment"># 随机梯度下降优化器  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置网络的一些参数</span></span><br><span class="line"><span class="comment"># 记录训练的次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 记录测试的次数</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练的轮次</span></span><br><span class="line">epoch = <span class="number">3000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加 tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">start_time = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-----第 &#123;&#125; 轮训练开始-----&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练步骤开始</span></span><br><span class="line">    tudui.train() <span class="comment"># 当网络中有dropout层、batchnorm层时，这些层能起作用</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs, targets = data            </span><br><span class="line">        imgs = imgs.to(device)</span><br><span class="line">        targets = targets.to(device)</span><br><span class="line">        outputs = tudui(imgs)</span><br><span class="line">        loss = loss_fn(outputs, targets) <span class="comment"># 计算实际输出与目标输出的差距</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 优化器对模型调优</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">        loss.backward() <span class="comment"># 反向传播，计算损失函数的梯度</span></span><br><span class="line">        optimizer.step()   <span class="comment"># 根据梯度，对网络的参数进行调优</span></span><br><span class="line">        </span><br><span class="line">        total_train_step = total_train_step + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            end_time = time.time()</span><br><span class="line">            <span class="built_in">print</span>(end_time - start_time) <span class="comment"># 运行训练一百次后的时间间隔</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数：&#123;&#125;，Loss：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step,loss.item()))  <span class="comment"># 方式二：获得loss值</span></span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>,loss.item(),total_train_step)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）</span></span><br><span class="line">    tudui.<span class="built_in">eval</span>()  <span class="comment"># 当网络中有dropout层、batchnorm层时，这些层不能起作用</span></span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 没有梯度了</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader: <span class="comment"># 测试数据集提取数据</span></span><br><span class="line">            imgs, targets = data <span class="comment"># 数据放到cuda上</span></span><br><span class="line">            imgs = imgs.to(device) <span class="comment"># 也可以不赋值，直接 imgs.to(device)</span></span><br><span class="line">            targets = targets.to(device) <span class="comment"># 也可以不赋值，直接 targets.to(device)</span></span><br><span class="line">            outputs = tudui(imgs)</span><br><span class="line">            loss = loss_fn(outputs, targets) <span class="comment"># 仅data数据在网络模型上的损失</span></span><br><span class="line">            total_test_loss = total_test_loss + loss.item() <span class="comment"># 所有loss</span></span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy = total_accuracy + accuracy</span><br><span class="line">            </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>,total_test_loss,total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>,total_accuracy/test_data_size,total_test_step)  </span><br><span class="line">    total_test_step = total_test_step + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    torch.save(m, <span class="string">&quot;./model/m_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i)) <span class="comment"># 保存每一轮训练后的结果</span></span><br><span class="line">    <span class="comment">#torch.save(m.state_dict(),&quot;m_&#123;&#125;.path&quot;.format(i)) # 保存方式二         </span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型已保存&quot;</span>)</span><br><span class="line">    </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p>**with torch.no_grad()**表示在训练数据集的同时进行验证，可以让之后的代码不影响目前的梯度</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/pytorch/">pytorch</category>
      
      
      <category domain="http://example.com/tags/pytorch/">pytorch</category>
      
      
      <comments>http://example.com/inori/8f832495.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>numpy常用api</title>
      <link>http://example.com/inori/e0d16fd2.html</link>
      <guid>http://example.com/inori/e0d16fd2.html</guid>
      <pubDate>Tue, 21 May 2024 11:18:34 GMT</pubDate>
      
      <description>numpy入门</description>
      
      
      
      <content:encoded><![CDATA[<p>NumPy，全称Numerical Python，是一个开源的Python库，它为Python提供了强大的多维数组对象和用于处理这些数组的函数。NumPy的核心是ndarray，它是一个高效的多维数组容器，用于存储和处理大规模的数据。NumPy还提供了许多数学函数，用于数组之间的操作，以及用于线性代数、傅立叶变换和随机数生成等功能。</p><h1 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h1><h2 id="创建数组"><a href="#创建数组" class="headerlink" title="创建数组"></a>创建数组</h2><h3 id="ndarray"><a href="#ndarray" class="headerlink" title="ndarray"></a>ndarray</h3><p><code>numpy.array(object, dtype = None, copy = True, order = None, subok = False, ndmin = 0)</code></p><p>上面的构造器接受以下参数：</p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left">object</td><td align="left">返回一个数组或任何（嵌套）序列</td></tr><tr><td align="left">dtype</td><td align="left">数组的所需数据类型，可选</td></tr><tr><td align="left">copy</td><td align="left">可选，默认为<code>true</code>，对象是否被复制</td></tr><tr><td align="left">order</td><td align="left"><code>C</code>（按行）、<code>F</code>（按列）或<code>A</code>（任意，默认）</td></tr><tr><td align="left">subok</td><td align="left">默认情况下，返回的数组被强制为基类数组。 如果为<code>true</code>，则返回子类</td></tr><tr><td align="left">ndmin</td><td align="left">指定返回数组的最小维数</td></tr></tbody></table><h4 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line">a = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])  </span><br><span class="line">b = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]) </span><br><span class="line">c = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>], ndmin =  <span class="number">2</span>) </span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">a = [1, 2, 3]</span></span><br><span class="line"><span class="string">b = [[1, 2] ,[3, 4]]</span></span><br><span class="line"><span class="string">c = [[1,2,3,4,5]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="empty"><a href="#empty" class="headerlink" title="empty"></a>empty</h3><p>创建一个指定形状（shape）、数据类型（dtype）且未初始化的数组</p><p><code>numpy.empty(shape, dtype = float, order = &#39;C&#39;)</code></p><h3 id="zeros"><a href="#zeros" class="headerlink" title="zeros"></a>zeros</h3><p>创建指定大小的数组，数组元素以 0 来填充：</p><p><code>numpy.zeros(shape, dtype = float, order = &#39;C&#39;)</code></p><h3 id="ones"><a href="#ones" class="headerlink" title="ones"></a>ones</h3><p>创建指定形状的数组，数组元素以 1 来填充：</p><p><code>numpy.ones(shape, dtype = None, order = &#39;C&#39;)</code></p><h3 id="zeros-like"><a href="#zeros-like" class="headerlink" title="zeros_like"></a>zeros_like</h3><p>创建一个与给定数组具有相同形状的数组，数组元素以 0 来填充</p><p><code>numpy.zeros_like(a, dtype=None, order=&#39;K&#39;, subok=True, shape=None)</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建一个 3x3 的二维数组</span></span><br><span class="line">arr = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建一个与 arr 形状相同的，所有元素都为 0 的数组</span></span><br><span class="line">zeros_arr = np.zeros_like(arr)</span><br><span class="line"><span class="built_in">print</span>(zeros_arr)</span><br></pre></td></tr></table></figure><h3 id="asarray"><a href="#asarray" class="headerlink" title="asarray"></a>asarray</h3><p>类似numpy.array，但参数只有三个</p><p><code>numpy.asarray(a, dtype = None, order = None)</code></p><h4 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"> </span><br><span class="line">x =  [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>] </span><br><span class="line">a = np.asarray(x)  <span class="comment"># [1,2,3] </span></span><br><span class="line"></span><br><span class="line">x =  (<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>) </span><br><span class="line">a = np.asarray(x) <span class="comment"># [1,2,3]</span></span><br><span class="line"></span><br><span class="line">x =  [(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>),(<span class="number">4</span>,<span class="number">5</span>)] </span><br><span class="line">a = np.asarray(x) <span class="comment"># [1,2,3] </span></span><br></pre></td></tr></table></figure><h3 id="arange"><a href="#arange" class="headerlink" title="arange"></a>arange</h3><p>创建数值范围并返回ndarray对象</p><p><code>numpy.arange(start, stop, step, dtype)</code></p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>start</code></td><td align="left">起始值，默认为<code>0</code></td></tr><tr><td align="left"><code>stop</code></td><td align="left">终止值（不包含）</td></tr><tr><td align="left"><code>step</code></td><td align="left">步长，默认为<code>1</code></td></tr><tr><td align="left"><code>dtype</code></td><td align="left">返回<code>ndarray</code>的数据类型，如果没有提供，则会使用输入数据的类型。</td></tr></tbody></table><h4 id="示例-2"><a href="#示例-2" class="headerlink" title="示例"></a>示例</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.arange(<span class="number">10</span>,<span class="number">20</span>,<span class="number">2</span>)  </span><br><span class="line"><span class="built_in">print</span>(x)<span class="comment"># [10  12  14  16  18]</span></span><br></pre></td></tr></table></figure><h3 id="linspace"><a href="#linspace" class="headerlink" title="linspace"></a>linspace</h3><p>用于创建一个一维数组，数组是一个等差数列构成的</p><p><code>np.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)</code></p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>start</code></td><td align="left">序列的起始值</td></tr><tr><td align="left"><code>stop</code></td><td align="left">序列的终止值，如果<code>endpoint</code>为<code>true</code>，该值包含于数列中</td></tr><tr><td align="left"><code>num</code></td><td align="left">要生成的等步长的样本数量，默认为<code>50</code></td></tr><tr><td align="left"><code>endpoint</code></td><td align="left">该值为<code>true</code>时，数列中包含<code>stop</code>值，反之不包含，默认是True</td></tr><tr><td align="left"><code>retstep</code></td><td align="left">如果为<code>true</code>时，生成的数组中会显示间距，反之不显示</td></tr><tr><td align="left"><code>dtype</code></td><td align="left">数据类型</td></tr></tbody></table><h4 id="示例-3"><a href="#示例-3" class="headerlink" title="示例"></a>示例</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 设置起始点为 1 ，终止点为 10，数列个数为 10</span></span><br><span class="line">a = np.linspace(<span class="number">1</span>,<span class="number">10</span>,<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(a)<span class="comment"># [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]</span></span><br><span class="line"></span><br><span class="line">a = np.linspace(<span class="number">10</span>, <span class="number">20</span>,  <span class="number">5</span>, endpoint =  <span class="literal">False</span>)  </span><br><span class="line"><span class="built_in">print</span>(a)<span class="comment"># [10. 12. 14. 16. 18.]</span></span><br></pre></td></tr></table></figure><h3 id="logspace"><a href="#logspace" class="headerlink" title="logspace"></a>logspace</h3><p>用于创建一个等比数列</p><p><code>np.logspace(start, stop, num=50, endpoint=True, base=10.0, dtype=None)</code></p><table><thead><tr><th align="left">参数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>start</code></td><td align="left">序列的起始值为：$base^{start}$</td></tr><tr><td align="left"><code>stop</code></td><td align="left">序列的终止值为：$base^{stop}$。如果<code>endpoint</code>为<code>true</code>，该值包含于数列中</td></tr><tr><td align="left"><code>num</code></td><td align="left">要生成的等步长的样本数量，默认为<code>50</code></td></tr><tr><td align="left"><code>endpoint</code></td><td align="left">该值为 <code>true</code> 时，数列中中包含<code>stop</code>值，反之不包含，默认是<code>true</code></td></tr><tr><td align="left"><code>base</code></td><td align="left">对数 log 的底数，默认底数是 10</td></tr><tr><td align="left"><code>dtype</code></td><td align="left">数据类型</td></tr></tbody></table><h4 id="示例-4"><a href="#示例-4" class="headerlink" title="示例"></a>示例</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 默认底数是 10</span></span><br><span class="line">a = np.logspace(<span class="number">1.0</span>,  <span class="number">2.0</span>, num =  <span class="number">10</span>)  </span><br><span class="line"><span class="built_in">print</span> (a)<span class="comment"># [ 10. 12.91549665 16.68100537 21.5443469  27.82559402 35.93813664  46.41588834  59.94842503  77.42636827  100.]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对数的底数设置为 2 </span></span><br><span class="line">a = np.logspace(<span class="number">0</span>,<span class="number">9</span>,<span class="number">10</span>,base=<span class="number">2</span>)</span><br><span class="line"><span class="built_in">print</span> (a)<span class="comment"># [  1.   2.   4.   8.  16.  32.  64. 128. 256. 512.]</span></span><br></pre></td></tr></table></figure><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><p>NumPy支持比Python更多种类的数值类型</p><table><thead><tr><th align="left">数据类型</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>bool_</code></td><td align="left">存储为一个字节的布尔值（真或假）</td></tr><tr><td align="left"><code>int_</code></td><td align="left">默认整数，相当于 C 的<code>long</code>，通常为<code>int32</code>或<code>int64</code></td></tr><tr><td align="left"><code>intc</code></td><td align="left">相当于 C 的<code>int</code>，通常为<code>int32</code>或<code>int64</code></td></tr><tr><td align="left"><code>intp</code></td><td align="left">用于索引的整数，相当于 C 的<code>size_t</code>，通常为<code>int32</code>或<code>int64</code></td></tr><tr><td align="left"><code>int8</code></td><td align="left">字节（-128 ~ 127）</td></tr><tr><td align="left"><code>int16</code></td><td align="left">16 位整数（-32768 ~ 32767）</td></tr><tr><td align="left"><code>int32</code></td><td align="left">32 位整数（-2147483648 ~ 2147483647）</td></tr><tr><td align="left"><code>int64</code></td><td align="left">64 位整数（-9223372036854775808 ~ 9223372036854775807）</td></tr><tr><td align="left"><code>uint8</code></td><td align="left">8 位无符号整数（0 ~ 255）</td></tr><tr><td align="left"><code>uint16</code></td><td align="left">16 位无符号整数（0 ~ 65535）</td></tr><tr><td align="left"><code>uint32</code></td><td align="left">32 位无符号整数（0 ~ 4294967295）</td></tr><tr><td align="left"><code>uint64</code></td><td align="left">64 位无符号整数（0 ~ 18446744073709551615）</td></tr><tr><td align="left"><code>float_</code></td><td align="left"><code>float64</code>的简写</td></tr><tr><td align="left"><code>float16</code></td><td align="left">半精度浮点：符号位，5 位指数，10 位尾数</td></tr><tr><td align="left"><code>float32</code></td><td align="left">单精度浮点：符号位，8 位指数，23 位尾数</td></tr><tr><td align="left"><code>float64</code></td><td align="left">双精度浮点：符号位，11 位指数，52 位尾数</td></tr><tr><td align="left"><code>complex_</code></td><td align="left"><code>complex128</code>的简写</td></tr><tr><td align="left"><code>complex64</code></td><td align="left">复数，由两个 32 位浮点表示（实部和虚部）</td></tr><tr><td align="left"><code>complex128</code></td><td align="left">复数，由两个 64 位浮点表示（实部和虚部）</td></tr></tbody></table><p>NumPy数字类型是<code>dtype</code>（数据类型）对象的实例，每个对象具有唯一的特征。这些类型可以是<code>np.bool_</code>，<code>np.float32</code>等。</p><p>数据类型对象描述了对应于数组的固定内存块的解释，取决于以下方面：</p><ul><li>数据类型（整数、浮点或者Python对象）</li><li>数据大小</li><li>字节序（小端或大端）</li><li>在结构化类型的情况下，字段的名称，每个字段的数据类型，和每个字段占用的内存块部分</li><li>如果数据类型是子序列，它的形状和数据类型</li></ul><p>字节顺序取决于数据类型的前缀<code>&lt;</code>或<code>&gt;</code>。 <code>&lt;</code>意味着编码是小端（最小有效字节存储在最小地址中）。 <code>&gt;</code>意味着编码是大端（最大有效字节存储在最小地址中）。</p><p><code>dtype</code>可由以下语法构造：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">numpy.dtype(<span class="built_in">object</span>, align, copy)</span><br></pre></td></tr></table></figure><ul><li><code>Object</code>：被转换为数据类型的对象</li><li><code>align</code>：向字段添加间隔，填充字段使其类似结构体</li><li><code>copy</code>：生成<code>dtype</code>对象的新副本，如果为<code>false</code>，结果是内建数据类型对象的引用</li></ul><h3 id="示例-5"><a href="#示例-5" class="headerlink" title="示例"></a>示例</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 使用标量类型</span></span><br><span class="line">dt = np.dtype(np.int32)</span><br><span class="line"><span class="comment"># int8, int16, int32, int64 四种数据类型可以使用字符串 &#x27;i1&#x27;, &#x27;i2&#x27;,&#x27;i4&#x27;,&#x27;i8&#x27; 代替</span></span><br><span class="line">dt = np.dtype(<span class="string">&#x27;i4&#x27;</span>)</span><br><span class="line"><span class="comment"># 将数据类型应用于 ndarray 对象</span></span><br><span class="line">dt = np.dtype([(<span class="string">&#x27;age&#x27;</span>,np.int8)]) </span><br><span class="line">a = np.array([(<span class="number">10</span>,),(<span class="number">20</span>,),(<span class="number">30</span>,)], dtype = dt) </span><br><span class="line"></span><br><span class="line">student = np.dtype([(<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;S20&#x27;</span>), (<span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;i1&#x27;</span>), (<span class="string">&#x27;marks&#x27;</span>, <span class="string">&#x27;f4&#x27;</span>)]) </span><br><span class="line">a = np.array([(<span class="string">&#x27;abc&#x27;</span>, <span class="number">21</span>, <span class="number">50</span>),(<span class="string">&#x27;xyz&#x27;</span>, <span class="number">18</span>, <span class="number">75</span>)], dtype = student) </span><br><span class="line"><span class="comment"># [(&#x27;abc&#x27;, 21, 50.0), (&#x27;xyz&#x27;, 18, 75.0)]</span></span><br></pre></td></tr></table></figure><h2 id="数组属性"><a href="#数组属性" class="headerlink" title="数组属性"></a>数组属性</h2><p>数组的维数称为秩（rank），秩就是轴的数量，一维数组的秩为1，二维数组的秩为2</p><p>在NumPy中，每一个线性的数组称为是一个轴（axis），也就是维度（dimensions）。比如说，二维数组相当于是两个一维数组，其中第一个一维数组中每个元素又是一个一维数组。所以一维数组就是NumPy中的轴，第一个轴相当于是底层数组，第二个轴是底层数组里的数组。而轴的数量秩，就是数组的维数。</p><p>很多时候可以声明axis。axis&#x3D;0，表示沿着第0轴进行操作，即对每一列进行操作；axis&#x3D;1，表示沿着第1轴进行操作，即对每一行进行操作</p><p>NumPy的数组中比较重要ndarray对象属性有：</p><table><thead><tr><th align="left">属性</th><th align="left">说明</th></tr></thead><tbody><tr><td align="left">ndarray.ndim</td><td align="left">秩，即轴的数量或维度的数量</td></tr><tr><td align="left">ndarray.shape</td><td align="left">数组的维度，对于矩阵，n行m列</td></tr><tr><td align="left">ndarray.size</td><td align="left">数组元素的总个数，相当于<code>.shape</code>中n*m的值</td></tr><tr><td align="left">ndarray.dtype</td><td align="left">元素类型</td></tr><tr><td align="left">ndarray.itemsize</td><td align="left">每个元素的大小，以字节为单位</td></tr><tr><td align="left">ndarray.flags</td><td align="left">内存信息</td></tr><tr><td align="left">ndarray.real</td><td align="left">实部</td></tr><tr><td align="left">ndarray.imag</td><td align="left">虚部</td></tr></tbody></table><h3 id="示例-6"><a href="#示例-6" class="headerlink" title="示例"></a>示例</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"> </span><br><span class="line">a = np.arange(<span class="number">24</span>)  </span><br><span class="line"><span class="built_in">print</span> (a.ndim)<span class="comment"># 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调整大小</span></span><br><span class="line">b = a.reshape(<span class="number">2</span>,<span class="number">4</span>,<span class="number">3</span>)</span><br><span class="line"><span class="built_in">print</span> (b.ndim)  <span class="comment"># 2</span></span><br><span class="line"></span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])  </span><br><span class="line"><span class="built_in">print</span> (a.shape)<span class="comment"># (2, 3)</span></span><br><span class="line"><span class="comment"># 调整数组大小</span></span><br><span class="line">a.shape =  (<span class="number">3</span>,<span class="number">2</span>)  </span><br><span class="line"><span class="built_in">print</span> (a)<span class="comment"># [[1 2],[3 4],[5 6]]</span></span><br></pre></td></tr></table></figure><h2 id="切片和索引"><a href="#切片和索引" class="headerlink" title="切片和索引"></a>切片和索引</h2><p>ndarray对象的内容可以通过索引或切片来访问和修改，与Python中list的切片操作一样。</p><p>ndarray数组可以基于0-n的下标进行索引，切片对象可以通过内置的slice函数，并设置start, stop及step参数进行，从原数组中切割出一个新数组</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line">a = np.arange(<span class="number">10</span>)</span><br><span class="line">s = <span class="built_in">slice</span>(<span class="number">2</span>,<span class="number">7</span>,<span class="number">2</span>)   <span class="comment"># 从索引 2 开始到索引 7 停止，间隔为2</span></span><br><span class="line"><span class="built_in">print</span> (a[s])<span class="comment"># [2  4  6]</span></span><br><span class="line"></span><br><span class="line">b = a[<span class="number">2</span>:<span class="number">7</span>:<span class="number">2</span>]   <span class="comment"># 从索引 2 开始到索引 7 停止，间隔为 2</span></span><br><span class="line"><span class="built_in">print</span>(b)<span class="comment"># [2  4  6]</span></span><br></pre></td></tr></table></figure><p>多维数组同样适用上述索引提取方法：</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># 从某个索引处开始切割</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;从数组索引 a[1:] 处开始切割&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(a[<span class="number">1</span>:])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    [[1 2 3]</span></span><br><span class="line"><span class="string">     [3 4 5]</span></span><br><span class="line"><span class="string">     [4 5 6]]</span></span><br><span class="line"><span class="string">    从数组索引 a[1:] 处开始切割</span></span><br><span class="line"><span class="string">    [[3 4 5]</span></span><br><span class="line"><span class="string">     [4 5 6]]</span></span><br><span class="line"><span class="string"> &#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>切片还可以包括省略号<code>**…**</code>，如果在行位置使用省略号，它将返回包含行中元素的 ndarray。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])  </span><br><span class="line"><span class="built_in">print</span> (a[...,<span class="number">1</span>])   <span class="comment"># 第2列元素</span></span><br><span class="line"><span class="built_in">print</span> (a[<span class="number">1</span>,...])   <span class="comment"># 第2行元素</span></span><br><span class="line"><span class="built_in">print</span> (a[...,<span class="number">1</span>:])  <span class="comment"># 第2列及剩下的所有元素</span></span><br></pre></td></tr></table></figure><ul><li><code>a[..., 1]</code> 的意思是：选择数组 <code>a</code> 的所有行，并选取每行的第2个元素（索引1处的元素）</li><li><code>a[1, ...]</code> 的意思是：选取数组 <code>a</code> 的第2行（索引1处的整行），并保留该行的所有列</li><li><code>...</code> 表示取所有剩余维度</li></ul><h3 id="高级索引"><a href="#高级索引" class="headerlink" title="高级索引"></a>高级索引</h3><h4 id="整数数组索引"><a href="#整数数组索引" class="headerlink" title="整数数组索引"></a>整数数组索引</h4><p>整数数组索引是指使用一个数组来访问另一个数组的元素。这个数组中的每个元素都是目标数组中某个维度上的索引值。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"> </span><br><span class="line">x = np.array([[<span class="number">1</span>,  <span class="number">2</span>],  [<span class="number">3</span>,  <span class="number">4</span>],  [<span class="number">5</span>,  <span class="number">6</span>]]) </span><br><span class="line"><span class="comment"># 获取数组中 (0,0)、(1,1)和(2,0)位置处的元素</span></span><br><span class="line">y = x[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],  [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>]]  </span><br><span class="line"><span class="built_in">print</span> (y)</span><br></pre></td></tr></table></figure><p><code>x[[0, 1, 2], [0, 1, 0]]</code>的意思是：</p><ul><li>选取<code>x</code>中的元素，其行索引由第一个数组 <code>[0, 1, 2]</code> 指定</li><li>列索引由第二个数组 <code>[0, 1, 0]</code> 指定</li></ul><p>具体选取的元素为：</p><ul><li><code>x[0, 0]</code> -&gt; 第0行第0列的元素，值为1</li><li><code>x[1, 1]</code> -&gt; 第1行第1列的元素，值为4</li><li><code>x[2, 0]</code> -&gt; 第2行第0列的元素，值为5</li></ul><h4 id="布尔索引"><a href="#布尔索引" class="headerlink" title="布尔索引"></a>布尔索引</h4><p>可以通过一个布尔数组来索引目标数组，布尔索引通过布尔运算（如：比较运算符）来获取符合指定条件的元素的数组</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"> </span><br><span class="line">x = np.array([[  <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>],[  <span class="number">3</span>,  <span class="number">4</span>,  <span class="number">5</span>],[  <span class="number">6</span>,  <span class="number">7</span>,  <span class="number">8</span>],[  <span class="number">9</span>,  <span class="number">10</span>,  <span class="number">11</span>]])  </span><br><span class="line"><span class="built_in">print</span> (x)</span><br><span class="line"><span class="comment"># 打印出大于5的元素  </span></span><br><span class="line"><span class="built_in">print</span>  (<span class="string">&#x27;大于 5 的元素是：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> (x[x &gt;  <span class="number">5</span>])</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[ 0  1  2]</span></span><br><span class="line"><span class="string"> [ 3  4  5]</span></span><br><span class="line"><span class="string"> [ 6  7  8]</span></span><br><span class="line"><span class="string"> [ 9 10 11]]</span></span><br><span class="line"><span class="string">大于 5 的元素是：</span></span><br><span class="line"><span class="string">[ 6  7  8  9 10 11]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="广播"><a href="#广播" class="headerlink" title="广播"></a>广播</h2><p>广播(Broadcast)是numpy对不同形状(shape)的数组（a.shape !&#x3D; b.shape）进行数值计算的方式，对数组的算术运算通常在相应的元素上进行。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"> </span><br><span class="line">a = np.array([[ <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">           [<span class="number">10</span>,<span class="number">10</span>,<span class="number">10</span>],</span><br><span class="line">           [<span class="number">20</span>,<span class="number">20</span>,<span class="number">20</span>],</span><br><span class="line">           [<span class="number">30</span>,<span class="number">30</span>,<span class="number">30</span>]])</span><br><span class="line">b = np.array([<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>])</span><br><span class="line"><span class="built_in">print</span>(a + b)</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">[[ 0  1  2]</span></span><br><span class="line"><span class="string"> [10 11 12]</span></span><br><span class="line"><span class="string"> [20 21 22]</span></span><br><span class="line"><span class="string"> [30 31 32]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><p>3x3的二维数组与长为3的一维数组相加，等效于把一维数组在二维上重复3次再运算</p><h3 id="广播规则"><a href="#广播规则" class="headerlink" title="广播规则"></a>广播规则</h3><ul><li>让所有输入数组都向其中形状最长的数组看齐，形状中不足的部分都通过在前面加1补齐</li><li>输出数组的形状是输入数组形状的各个维度上的最大值</li><li>如果输入数组的某个维度和输出数组的对应维度的长度相同或者其长度为1时，这个数组能够用来计算，否则出错</li><li>当输入数组的某个维度的长度为1时，沿着此维度运算时都用此维度上的第一组值</li></ul><p><strong>简单理解：</strong></p><p>对两个数组，分别比较他们的每一个维度（若其中一个数组没有当前维度则忽略），满足：</p><ul><li>数组拥有相同形状</li><li>当前维度的值相等</li><li>当前维度的值有一个是 1</li></ul><p>若条件不满足，抛出 <strong>“ValueError: frames are not aligned”</strong> 异常</p><h2 id="统计函数"><a href="#统计函数" class="headerlink" title="统计函数"></a>统计函数</h2><p>NumPy提供了很多统计函数，用于从数组中查找最小元素，最大元素，百分位标准差和方差等</p><h3 id="numpy-amin-和-numpy-amax"><a href="#numpy-amin-和-numpy-amax" class="headerlink" title="numpy.amin() 和 numpy.amax()"></a>numpy.amin() 和 numpy.amax()</h3><p>用于计算数组中的元素沿指定轴的最小值</p><p><code>numpy.(amin|amax)(a, axis=None, out=None, keepdims=&lt;no value&gt;, initial=&lt;no value&gt;, where=&lt;no value&gt;)</code></p><ul><li><code>a</code>: 输入的数组，可以是一个NumPy数组或类似数组的对象</li><li><code>axis</code>: 可选参数，用于指定在哪个轴上计算最值。如果不提供此参数，则返回整个数组的最小值。可以是一个整数表示轴的索引，也可以是一个元组表示多个轴</li><li><code>out</code>: 可选参数，用于指定结果的存储位置</li><li><code>keepdims</code>: 可选参数，如果为True，将保持结果数组的维度数目与输入数组相同。如果为False（默认值），则会去除计算后维度为1的轴</li><li><code>initial</code>: 可选参数，用于指定一个初始值，然后在数组的元素上计算最值</li><li><code>where</code>: 可选参数，一个布尔数组，用于指定仅考虑满足条件的元素</li></ul><h3 id="numpy-ptp"><a href="#numpy-ptp" class="headerlink" title="numpy.ptp()"></a>numpy.ptp()</h3><p>计算数组中元素最大值与最小值的差（最大值 - 最小值）。</p><p><code>numpy.ptp(a, axis=None, out=None, keepdims=&lt;no value&gt;, initial=&lt;no value&gt;, where=&lt;no value&gt;)</code></p><ul><li><code>axis</code>: 可选参数，用于指定在哪个轴上计算峰值。如果不提供此参数，则返回整个数组的峰值。可以是一个整数表示轴的索引，也可以是一个元组表示多个轴</li><li><code>initial</code>: 可选参数，用于指定一个初始值，然后在数组的元素上计算峰值</li></ul><h3 id="numpy-percentile"><a href="#numpy-percentile" class="headerlink" title="numpy.percentile()"></a>numpy.percentile()</h3><p>百分位数是统计中使用的度量，表示小于这个值的观察值的百分比</p><p><code>numpy.percentile(a, q, axis)</code></p><ul><li>a: 输入数组</li><li>q: 要计算的百分位数，在0~100之间</li><li>axis: 沿着它计算百分位数的轴</li></ul><p><strong>百分位数：</strong></p><p>第p个百分位数是这样一个值，它使得至少有p%的数据项小于或等于这个值，且至少有(100-p)%的数据项大于或等于这个值</p><h3 id="numpy-median"><a href="#numpy-median" class="headerlink" title="numpy.median()"></a>numpy.median()</h3><p>用于计算数组a中元素的中位数（中值）</p><p><code>numpy.median(a, axis=None, out=None, overwrite_input=False, keepdims=&lt;no value&gt;)</code></p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"> </span><br><span class="line">a = np.arange(<span class="number">6</span>).reshape(<span class="number">3</span>,<span class="number">2</span>)  </span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;我们的数组是：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> (a)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;修改后的数组：&#x27;</span>)</span><br><span class="line">wt = np.array([<span class="number">3</span>,<span class="number">5</span>])  </span><br><span class="line"><span class="built_in">print</span> (np.average(a, axis =  <span class="number">1</span>, weights = wt))</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;修改后的数组：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> (np.average(a, axis =  <span class="number">1</span>, weights = wt, returned =  <span class="literal">True</span>))</span><br></pre></td></tr></table></figure><h3 id="numpy-mean"><a href="#numpy-mean" class="headerlink" title="numpy.mean()"></a>numpy.mean()</h3><p>返回数组中元素的平均值，如果提供了轴，则沿其计算，算术平均值是沿轴的元素的总和除以元素的数量</p><p><code>numpy.mean(a, axis=None, dtype=None, out=None, keepdims=&lt;no value&gt;)</code></p><h3 id="numpy-average"><a href="#numpy-average" class="headerlink" title="numpy.average()"></a>numpy.average()</h3><p>根据在另一个数组中给出的各自的权重计算数组中元素的加权平均值，该函数可以接受一个轴参数。如果没有指定轴，则数组会被展开</p><p><code>numpy.average(a, axis=None, weights=None, returned=False)</code></p><p>加权平均值即将各数值乘以相应的权数，然后加总求和得到总体值，再除以总的单位数</p><p>考虑数组[1,2,3,4]和相应的权重[4,3,2,1]，通过将相应元素的乘积相加，并将和除以权重的和，来计算加权平均值</p><p><code>加权平均值 = (1*4+2*3+3*2+4*1)/(4+3+2+1)</code></p><h3 id="方差"><a href="#方差" class="headerlink" title="方差"></a>方差</h3><p>统计中的方差（样本方差）是每个样本值与全体样本值的平均数之差的平方的平均数</p><p><code>numpy.var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=&lt;no value&gt;, *, where=&lt;no value&gt;, mean=&lt;no value&gt;)</code></p><h3 id="标准差"><a href="#标准差" class="headerlink" title="标准差"></a>标准差</h3><p>标准差是一组数据平均值分散程度的一种度量。标准差是方差的算术平方根。</p><p><code>numpy.std(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False)</code></p><h2 id="数学函数"><a href="#数学函数" class="headerlink" title="数学函数"></a>数学函数</h2><h3 id="三角函数"><a href="#三角函数" class="headerlink" title="三角函数"></a>三角函数</h3><p>NumPy 提供了标准的三角函数：sin()、cos()、tan()</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line">a = np.array([<span class="number">0</span>,<span class="number">30</span>,<span class="number">45</span>,<span class="number">60</span>,<span class="number">90</span>])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;含有正弦值的数组：&#x27;</span>)</span><br><span class="line">sin = np.sin(a*np.pi/<span class="number">180</span>)</span><br><span class="line"><span class="built_in">print</span>(sin)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;计算角度的反正弦，返回值以弧度为单位：&#x27;</span>)</span><br><span class="line">inv = np.arcsin(sin)</span><br><span class="line"><span class="built_in">print</span>(inv)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;通过转化为角度制来检查结果：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(np.degrees(inv))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;arccos 和 arctan 函数行为类似：&#x27;</span>)</span><br><span class="line">cos = np.cos(a*np.pi/<span class="number">180</span>)  </span><br><span class="line"><span class="built_in">print</span>(cos)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;反余弦：&#x27;</span>)</span><br><span class="line">inv = np.arccos(cos)  </span><br><span class="line"><span class="built_in">print</span>(inv)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;角度制单位：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(np.degrees(inv))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;tan 函数：&#x27;</span>)</span><br><span class="line">tan = np.tan(a*np.pi/<span class="number">180</span>)  </span><br><span class="line"><span class="built_in">print</span>(tan)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;反正切：&#x27;</span>)</span><br><span class="line">inv = np.arctan(tan)  </span><br><span class="line"><span class="built_in">print</span>(inv)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;角度制单位：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(np.degrees(inv))</span><br></pre></td></tr></table></figure><h3 id="舍入函数"><a href="#舍入函数" class="headerlink" title="舍入函数"></a>舍入函数</h3><p>返回指定数字的四舍五入值</p><p><code>numpy.around(a,decimals)</code></p><ul><li>a: 数组</li><li>decimals: 舍入的小数位数。 默认值为0。 如果为负，整数将四舍五入到小数点左侧的位置</li></ul><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line">a = np.array([<span class="number">1.0</span>,<span class="number">5.55</span>,  <span class="number">123</span>,  <span class="number">0.567</span>,  <span class="number">25.532</span>])  </span><br><span class="line"><span class="built_in">print</span>  (<span class="string">&#x27;原数组：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> (a)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;舍入后：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> (np.around(a))</span><br><span class="line"><span class="built_in">print</span> (np.around(a, decimals =  <span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span> (np.around(a, decimals =  -<span class="number">1</span>))</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">原数组：</span></span><br><span class="line"><span class="string">[  1.      5.55  123.      0.567  25.532]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">舍入后：</span></span><br><span class="line"><span class="string">[  1.   6. 123.   1.  26.]</span></span><br><span class="line"><span class="string">[  1.    5.6 123.    0.6  25.5]</span></span><br><span class="line"><span class="string">[  0.  10. 120.   0.  30.]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h2 id="矩阵库"><a href="#矩阵库" class="headerlink" title="矩阵库"></a>矩阵库</h2><p>NumPy 中包含了一个矩阵库 numpy.matlib，该模块中的函数返回的是一个矩阵，而不是 ndarray 对象</p><p>一个<code>m*n</code>的矩阵是一个由m行n列元素排列成的矩形阵列,矩阵里的元素可以是数字、符号或数学式。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"> </span><br><span class="line">a = np.arange(<span class="number">12</span>).reshape(<span class="number">3</span>,<span class="number">4</span>)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;原数组：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> (a)</span><br><span class="line"><span class="built_in">print</span> (<span class="string">&#x27;转置数组：&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span> (a.T)</span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">原数组：</span></span><br><span class="line"><span class="string">[[ 0  1  2  3]</span></span><br><span class="line"><span class="string"> [ 4  5  6  7]</span></span><br><span class="line"><span class="string"> [ 8  9 10 11]]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">转置数组：</span></span><br><span class="line"><span class="string">[[ 0  4  8]</span></span><br><span class="line"><span class="string"> [ 1  5  9]</span></span><br><span class="line"><span class="string"> [ 2  6 10]</span></span><br><span class="line"><span class="string"> [ 3  7 11]]</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure><h3 id="matlib-empty"><a href="#matlib-empty" class="headerlink" title="matlib.empty()"></a>matlib.empty()</h3><p>返回一个新的矩阵，矩阵内的元素为未初始化的值:</p><p><code>numpy.matlib.empty(shape, dtype, order)</code></p><ul><li><strong>shape</strong>: 定义新矩阵形状的整数或整数元组</li><li><strong>Dtype</strong>: 可选，数据类型</li><li><strong>order</strong>: C（行序优先） 或者 F（列序优先）</li></ul><h3 id="matlib-zeros"><a href="#matlib-zeros" class="headerlink" title="matlib.zeros()"></a>matlib.zeros()</h3><p>创建一个以 0 填充的矩阵</p><h3 id="matlib-ones"><a href="#matlib-ones" class="headerlink" title="matlib.ones()"></a>matlib.ones()</h3><p>创建一个以 1 填充的矩阵</p><h3 id="matlib-eye"><a href="#matlib-eye" class="headerlink" title="matlib.eye()"></a>matlib.eye()</h3><p>返回一个矩阵，对角线元素为1，其他位置为零</p><p><code>numpy.matlib.eye(n, M,k, dtype)</code></p><ul><li><strong>n</strong>: 返回矩阵的行数</li><li><strong>M</strong>: 返回矩阵的列数，默认为 n</li><li><strong>k</strong>: 对角线的索引</li><li><strong>dtype</strong>: 数据类型</li></ul><h3 id="matlib-identity"><a href="#matlib-identity" class="headerlink" title="matlib.identity()"></a>matlib.identity()</h3><p>返回给定大小的单位矩阵</p><h3 id="matlib-rand"><a href="#matlib-rand" class="headerlink" title="matlib.rand()"></a>matlib.rand()</h3><p>创建一个给定大小的矩阵，数据是随机填充的</p><h2 id="numpy线性代数"><a href="#numpy线性代数" class="headerlink" title="numpy线性代数"></a>numpy线性代数</h2><p>NumPy 提供了线性代数函数库 <strong>linalg</strong>，该库包含了线性代数所需的所有功能</p><table><thead><tr><th align="left">函数</th><th align="left">描述</th></tr></thead><tbody><tr><td align="left"><code>dot</code></td><td align="left">两个数组的点积，即元素对应相乘。</td></tr><tr><td align="left"><code>vdot</code></td><td align="left">两个向量的点积</td></tr><tr><td align="left"><code>inner</code></td><td align="left">两个数组的内积</td></tr><tr><td align="left"><code>matmul</code></td><td align="left">两个数组的矩阵积</td></tr><tr><td align="left"><code>determinant</code></td><td align="left">数组的行列式</td></tr><tr><td align="left"><code>solve</code></td><td align="left">求解线性矩阵方程</td></tr><tr><td align="left"><code>inv</code></td><td align="left">计算矩阵的乘法逆矩阵</td></tr></tbody></table><h3 id="numpy-dot"><a href="#numpy-dot" class="headerlink" title="numpy.dot()"></a>numpy.dot()</h3><p>对于两个一维的数组，计算的是这两个数组对应下标元素的乘积和(数学上称之为向量点积)；</p><p>对于二维数组，计算的是两个数组的矩阵乘积；</p><p>对于多维数组，它的通用计算公式如下，即结果数组中的每个元素都是数组a的最后一维上的所有元素与数组b的倒数第二维上的所有元素的乘积和：<code>dot(a, b)[i,j,k,m] = sum(a[i,j,:] \* b[k,:,m])</code></p><p><code>numpy.dot(a, b, out=None) </code></p><ul><li><strong>a</strong>:ndarray 数组</li><li><strong>b</strong>:ndarray 数组</li><li><strong>out</strong>:ndarray, 可选，用来保存dot()的计算结果</li></ul><h3 id="numpy-vdot"><a href="#numpy-vdot" class="headerlink" title="numpy.vdot()"></a>numpy.vdot()</h3><p>计算两个向量的点积。 如果第一个参数是复数，那么它的共轭复数会用于计算。 如果参数是多维数组，它会被展开。</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"> </span><br><span class="line">a = np.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]) </span><br><span class="line">b = np.array([[<span class="number">11</span>,<span class="number">12</span>],[<span class="number">13</span>,<span class="number">14</span>]]) </span><br><span class="line"> </span><br><span class="line"><span class="comment"># vdot 将数组展开计算内积</span></span><br><span class="line"><span class="built_in">print</span> (np.vdot(a,b))</span><br><span class="line"><span class="comment"># 1*11 + 2*12 + 3*13 + 4*14 = 130</span></span><br></pre></td></tr></table></figure><h3 id="numpy-inner"><a href="#numpy-inner" class="headerlink" title="numpy.inner()"></a>numpy.inner()</h3><p>numpy.inner() 函数返回一维数组的向量内积。对于更高的维度，它返回最后一个轴上的和的乘积。</p><h3 id="numpy-matmul"><a href="#numpy-matmul" class="headerlink" title="numpy.matmul"></a>numpy.matmul</h3><p>返回两个数组的矩阵乘积。 </p><p>虽然它返回二维数组的正常乘积，但如果任一参数的维数大于2，则将其视为存在于最后两个索引的矩阵的栈，并进行相应广播。</p><p>另一方面，如果任一参数是一维数组，则通过在其维度上附加 1 来将其提升为矩阵，并在乘法之后被去除。</p><p>对于二维数组，它就是矩阵乘法</p><h3 id="numpy-linalg-det"><a href="#numpy-linalg-det" class="headerlink" title="numpy.linalg.det()"></a>numpy.linalg.det()</h3><p>计算输入矩阵的行列式</p><h3 id="numpy-linalg-solve"><a href="#numpy-linalg-solve" class="headerlink" title="numpy.linalg.solve()"></a>numpy.linalg.solve()</h3><p>矩阵形式的线性方程的解。</p><p>考虑以下线性方程：</p><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">x + y + z = 6</span><br><span class="line"></span><br><span class="line">2y + 5z = -4</span><br><span class="line"></span><br><span class="line">2x + 5y - z = 27</span><br></pre></td></tr></table></figure><p>可以使用矩阵表示为：</p><figure class="highlight txt"><table><tr><td class="code"><pre><span class="line">1  1  1   x      6</span><br><span class="line">0  2  5   y  =  -4</span><br><span class="line">2  5 -1   z   27</span><br></pre></td></tr></table></figure><p>如果矩阵成为A、X和B，方程变为：<code>AX = B</code>或<code>X = A^(-1)B</code></p><h3 id="numpy-linalg-inv"><a href="#numpy-linalg-inv" class="headerlink" title="numpy.linalg.inv()"></a>numpy.linalg.inv()</h3><p>计算矩阵的逆矩阵。</p><p><strong>逆矩阵（inverse matrix）</strong>：设A是数域上的一个n阶矩阵，若在相同数域上存在另一个n阶矩阵B，使得：AB&#x3D;BA&#x3D;E，则我们称B是A的逆矩阵，而A则被称为可逆矩阵</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/numpy/">numpy</category>
      
      
      <category domain="http://example.com/tags/numpy/">numpy</category>
      
      
      <comments>http://example.com/inori/e0d16fd2.html#disqus_thread</comments>
      
    </item>
    
    <item>
      <title>机器学习基础知识</title>
      <link>http://example.com/inori/54da7039.html</link>
      <guid>http://example.com/inori/54da7039.html</guid>
      <pubDate>Mon, 20 May 2024 06:27:45 GMT</pubDate>
      
      <description>MachineLearning基础入门</description>
      
      
      
      <content:encoded><![CDATA[<p>机器学习是人工智能的重要技术基础，涉及的内容十分广泛。由于我是以发强化学习方向论文为导向，所以机器学习部分知识点不做深入学习，了解为主，本文主要涵盖机器学习入门的基础知识。</p><p>机器学习的核心元素：<strong>数据、模型、算法</strong></p><p>机器学习工作流程一般为数据预处理（Processing）、模型学习（Learning）、模型评估（Evaluation）、新样本预测（Prediction）几个步骤</p><p><strong>数据预处理：</strong>输入（未处理的数据 + 标签）-&gt; 处理过程 -&gt;（特征处理+幅度缩放、特征选择、维度约减、采样）-&gt; 输出（测试集 + 训练集）</p><p><strong>模型学习：</strong>模型选择、交叉验证、结果评估、超参选择。</p><p><strong>模型评估：</strong>了解模型对于数据集测试的得分。</p><p><strong>新样本预测：</strong>预测测试集</p><h1 id="基本名词"><a href="#基本名词" class="headerlink" title="基本名词"></a>基本名词</h1><p><strong>示例&#x2F;样本：</strong>一条数据集中的一条数据</p><p><strong>属性&#x2F;特征：</strong>判断样本好坏的根据，如「色泽」「根蒂」</p><p><strong>属性空间&#x2F;样本空间&#x2F;输入空间X：</strong>由全部属性张成的空间</p><p><strong>特征向量：</strong>空间中每个点对应的一个坐标向量</p><p><strong>标记：</strong>关于示例结果的信息，如（（色泽&#x3D;青绿，根蒂&#x3D;蜷缩，敲声&#x3D;浊响），好瓜），其中「好瓜」称为标记</p><p><strong>分类：</strong>若要预测的是离散值，如「好瓜」，「坏瓜」，此类学习任务称为分类</p><p><strong>假设：</strong>学得模型对应了关于数据的某种潜在规律</p><p><strong>真相：</strong>潜在规律自身</p><p><strong>学习过程：</strong>是为了找出或逼近真相</p><p><strong>泛化能力：</strong>学得模型适用于新样本的能力。一般来说，训练样本越大，越有可能通过学习来获得具有强泛化能力的模型</p><p><strong>训练集</strong>：帮助训练模型，简单的说就是通过训练集的数据让确定拟合曲线的参数</p><p><strong>测试集</strong>：为了测试已经训练好的模型的精确度</p><p><strong>过拟合</strong>：模型在训练集上表现的很好，但是在交叉验证集合测试集上表现一般，也就是说模型对未知样本的预测表现一般，泛化能力较差</p><p><strong>经验误差</strong>：模型在训练集上的误差称为「经验误差」（Empirical Error），经验误差并不是越小越好，因为我们希望在新的没有见过的数据上，也能有好的预估结果</p><p><strong>偏差</strong>：模型拟合的偏差程度。给定无数套训练集而期望拟合出来的模型就是平均模型，偏差就是真实模型和平均模型的差异</p><p><strong>二分类</strong>：类别中只有两个类，是 or 否，且只有一个类别，即一个label为0或者1</p><h1 id="算法分类"><a href="#算法分类" class="headerlink" title="算法分类"></a>算法分类</h1><p>机器学习算法有：监督学习，无监督学习，半监督学习，强化学习</p><h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>监督学习是从标记的训练数据中学习并建立模型，然后基于该模型预测未知的样本。其中，模型的输入是某个样本数据的特征，而函数的输出是与该样本相对应的标签。</p><p>常见的监督学习算法：回归分析、分类预测</p><ul><li>回归分析：线性回归，决策树，随机森林等</li><li>分类预测：逻辑回归，KNN，支持向量机，朴素贝叶斯等</li></ul><h3 id="KNN算法"><a href="#KNN算法" class="headerlink" title="KNN算法"></a>KNN算法</h3><p>KNN算法即K最近邻近算法。所谓K最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。KNN的输入基于实例的学习（instance-based learning），没有显式的学习过程，也就是说没有训练阶段，数据集事先已有了分类和特征值，待收到新样本后直接进行处理，<strong>KNN通过测量不同特征值之间的距离进行分类</strong></p><p>思路：如果一个样本在特征空间中的k个最邻近的样本中的大多数属于某一个类别，则该样本也划分为这个类别。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别</p><h4 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h4><ol><li>计算测试数据与各个训练数据之间的距离</li><li>按照距离的递增关系进行排序</li><li>选取距离最小的K个点</li><li>确定前K个点所在类别的出现频率</li><li>返回前K个点中出现频率最高的类别作为测试数据的预测分类</li></ol><h4 id="K的取值"><a href="#K的取值" class="headerlink" title="K的取值"></a>K的取值</h4><p>K：临近数，即在预测目标点时取几个临近的点来预测。</p><p>K值得选取非常重要：</p><ul><li>如果K的取值过小，一旦有噪声存在将会对预测产生比较大影响，例如取K值为1时，一旦最近的一个点是噪声，那么就会出现偏差</li><li>如果K的取值过大，就相当于用较大邻域中的训练实例进行预测，学习的近似误差会增大。这时与输入目标点较远实例也会对预测起作用，使预测发生错误</li></ul><p>K的取值尽量要取<strong>奇数</strong>，以保证在计算结果最后会产生一个较多的类别，如果取偶数可能会产生相等的情况，不利于预测。</p><p>常用的方法是从k&#x3D;1开始，使用检验集估计分类器的误差率。重复该过程，每次K增值1，允许增加一个近邻。选取产生最小误差率的K。一般k的取值不超过20，上限是n的开方，随着数据集的增大，K的值也要增大</p><h3 id="朴素贝叶斯算法"><a href="#朴素贝叶斯算法" class="headerlink" title="朴素贝叶斯算法"></a>朴素贝叶斯算法</h3><p>朴素贝叶斯算法是一种基于概率论和统计学的算法。它的核心思想是概率，通过计算条件概率来预测或分类数据</p><h4 id="贝叶斯定理"><a href="#贝叶斯定理" class="headerlink" title="贝叶斯定理"></a>贝叶斯定理</h4><p>贝叶斯定理是朴素贝叶斯算法的核心，它是一个概率公式，用于计算一个事件的后验概率。根据贝叶斯定理，事件 A 的后验概率等于先验概率 P(A)，与另一个事件 B 发生的联合概率 P(B|A) 乘以一个正则因子，即：<br>$$<br>P(Y|X) &#x3D; \frac{P(X|Y)P(Y)}{P(X)}<br>（X：特征向量， Y：类别）<br>$$<br>**先验概率P(X)**：先验概率是指根据以往经验和分析得到的概率</p><p>**后验概率P(Y|X)**：事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，后验分布P(Y|X)表示事件X已经发生的前提下，事件Y发生的概率，叫做事件X发生下事件Y的条件概率</p><p>**后验概率P(X|Y)**：在已知Y发生后X的条件概率，也由于知道Y的取值而被称为X的后验概率</p><h4 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h4><p>朴素贝叶斯算法是假设各个特征之间相互独立，也是朴素这词的意思，那么贝叶斯公式中的P(X|Y)可写成：<br>$$<br>P(X|Y)&#x3D;P(x_1|Y)P(x_2|Y)…P(x_n|Y)<br>$$<br>即朴素贝叶斯公式：<br>$$<br>P(Y|X) &#x3D; \frac{P(x_1|Y)P(x_2|Y)…P(x_n|Y)P(Y)}{P(X)}<br>$$</p><h4 id="贝叶斯算法"><a href="#贝叶斯算法" class="headerlink" title="贝叶斯算法"></a>贝叶斯算法</h4><p>贝叶斯方法源于决一个“逆概”问题：</p><p><strong>正向概率</strong>：假设袋子里面有N个白球，M个黑球，伸手进去摸一把，摸出黑球的概率是多大</p><p><strong>逆向概率</strong>：如果事先不知道袋子里面黑白球的比例，而是闭着眼睛摸出一个（或者几个）球，观察这些取出来的球的颜色之后，那么可以就此对袋子里面的黑白球的比例做出什么样的推测</p><p>什么是贝叶斯？</p><ul><li>现实世界本身就是不确定的，人类的观察能力是有局限性的</li><li>我们日常观察到的只是事物表明上的结果，因此我们需要提供一个猜测</li></ul><p>朴素贝叶斯算法的朴素：特征条件独立；贝叶斯：基于贝叶斯定理。属于监督学习的生成模型，实现监督，没有迭代，并有坚实的数学理论（即贝叶斯定理）作为支撑。在大量样本下会有较好的表现，不适用于输入向量的特征条件有关联的场景</p><p>朴素贝叶斯会单独考量每一维独立特征被分类的条件概率，进而综合这些概率并对其所在的特征向量做出分类预测。因此，朴素贝叶斯的基本数据假设是：各个维度上的特征被分类的条件概率之间是相互独立的。它经常被用于文本分类中，包括互联网新闻的分类，垃圾邮件的筛选</p><p>朴素贝叶斯的思想：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，即认为此待分类项属于哪个类别</p><h3 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h3><p>决策树算法是一种逼近离散函数值的方法。它是一种典型的分类方法，首先对数据进行处理，利用归纳算法生成可读的规则和决策树，然后使用决策对新数据进行分析。本质上决策树是通过一系列规则对数据进行分类的过程，适用于类别和连续输入（特征）和输出（预测）变量。基于树的方法把特征空间划分成一系列矩形，然后给每一个矩形安置一个简单的模型（像一个常数）</p><p>决策树构造可以分两步进行：</p><ol><li>生成：由训练样本集生成决策树的过程。一般情况下，训练样本数据集是根据实际需要有历史的、有一定综合程度的，用于数据分析处理的数据集</li><li>剪枝：对上一阶段生成的决策树进行检验和校正，主要是用新的样本数据集（测试数据集）中的数据校验决策树生成过程中产生的初步规则，将那些影响预衡准确性的分枝剪除</li></ol><p>决策树学习的算法通常是一个递归地决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。包含特征选择、决策树的生成和决策树的剪枝过程。选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。包含特征选择、决策树的生成和决策树的剪枝过程。</p><p><strong>剪枝：</strong>将树变得更简单，从而使它具有更好的泛化能力。</p><p>步骤：去掉过于细分的叶结点，使其回退到父结点，甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。</p><p>决策树的生成对应模型的局部选择，决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，决策树的剪枝则考虑全局最优。</p><p><strong>特征选择：</strong>如果特征数量很多，在决策树学习开始时对特征进行选择，只留下对训练数据有足够分类能力的特征</p><h4 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h4><ol><li>树以代表训练样本的单个结点开始</li><li>如果样本都在同一类．则该结点成为树叶，并用该类标记</li><li>否则，算法选择最有分类能力的属性作为决策树的当前结点</li><li>根据当前决策结点属性取值的不同，将训练样本数据集分为若干子集，每个取值形成一个分枝，有几个取值形成几个分枝。针对上一步得到的一个子集，重复进行先前步骤，递归形成每个划分样本上的决策树。一旦一个属性出现在一个结点上，就不必在该结点的任何后代考虑它</li><li>递归划分步骤仅当下列条件之一成立时停止：<ul><li>给定结点的所有样本属于同一类</li><li>没有剩余属性可以用来进一步划分样本．在这种情况下．使用多数表决，将给定的结点转换成树叶，并以样本中元组个数最多的类别作为类别标记，同时也可以存放该结点样本的类别分布</li><li>如果某一分枝tc，没有满足该分支中已有分类的样本，则以样本的多数类创建一个树叶</li></ul></li></ol><h3 id="SVM算法"><a href="#SVM算法" class="headerlink" title="SVM算法"></a>SVM算法</h3><p>SVM即支持向量机（support vector machine，SVM），支持向量机是一种二分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，其学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解。支持向量机的学习算法是求解凸二次规划的最优化算法。基础的SVM算法是一个二分类算法，至于多分类任务，可以通过多次使用SVM进行解决</p><h4 id="线性可分"><a href="#线性可分" class="headerlink" title="线性可分"></a>线性可分</h4><p>对于一个数据集合可以画一条直线将两组数据点分开，这样的数据成为线性可分（linearly separable）</p><p><img src="D:\Blog\source\img\smv_linearly_separable.png" alt="smv_linearly_separable"></p><ul><li>分割超平面：将上述数据集分隔开来的直线成为分隔超平面。对于二维平面来说，分隔超平面就是一条直线。对于三维及三维以上的数据来说，分隔数据的是个平面，称为超平面，也就是分类的决策边界</li><li>间隔：点到分割面的距离，称为点相对于分割面的间隔。数据集所有点到分隔面的最小间隔的2倍，称为分类器或数据集的间隔。论文中提到的间隔多指这个间隔。SVM分类器就是要找最大的数据集间隔</li><li>支持向量：离分隔超平面最近的那些点</li></ul><p>SVM所做的工作就是找这样的超平面，能够将两个不同类别的样本划分开来，但是这种平面是不唯一的，即可能存在无数个超平面都可以将两种样本分开，那么我们如何才能确定一个分类效果最好的超平面呢？<br>对每一种可能的超平面，我们将它进行平移，直到它与空间中的样本向量相交。我们称这两个向量为支持向量，之后我们计算支持向量到该超平面的距离d，分类效果最好的超平面应该使d最大</p><p>支持向量机的核心思想：最大间隔化，最不受到噪声的干扰</p><h4 id="优缺点及应用场景"><a href="#优缺点及应用场景" class="headerlink" title="优缺点及应用场景"></a>优缺点及应用场景</h4><ol><li>SVM的优点：</li></ol><ul><li>高效的处理高维特征空间：SVM通过将数据映射到高维空间中，可以处理高维特征，并在低维空间中进行计算，从而有效地处理高维数据</li><li>适用于小样本数据集：SVM是一种基于边界的算法，它依赖于少数支持向量，因此对于小样本数据集具有较好的泛化能力</li><li>可以处理非线性问题：SVM使用核函数将输入数据映射到高维空间，从而可以解决非线性问题。常用的核函数包括线性核、多项式核和径向基函数（RBF）核</li><li>避免局部最优解：SVM的优化目标是最大化间隔，而不是仅仅最小化误分类点。这使得SVM在解决复杂问题时能够避免陷入局部最优解</li><li>对于噪声数据的鲁棒性：SVM通过使用支持向量来定义决策边界，这使得它对于噪声数据具有一定的鲁棒性</li></ul><ol start="2"><li>SVM的缺点：</li></ol><ul><li>对大规模数据集的计算开销较大：SVM的计算复杂度随着样本数量的增加而增加，特别是在大规模数据集上的训练时间较长</li><li>对于非线性问题选择合适的核函数和参数较为困难：在处理非线性问题时，选择适当的核函数和相应的参数需要一定的经验和领域知识</li><li>对缺失数据敏感：SVM在处理含有缺失数据的情况下表现不佳，因为它依赖于支持向量的定义</li><li>难以解释模型结果：SVM生成的模型通常是黑盒模型，难以直观地解释模型的决策过程和结果</li></ul><ol start="3"><li>SVM主要应用场景：</li></ol><ul><li>文本分类：如垃圾邮件分类、情感分析和文档分类等</li><li>图像识别：可用于图像分类、目标识别和人脸识别等任务。它可以通过提取图像的特征向量，并将其作为输入来训练SVM模型</li><li>金融领域：可用于信用评分、风险评估和股票市场预测等金融任务</li><li>医学诊断：可以应用于医学图像分析，如疾病检测、癌症诊断和医学影像分类等</li><li>视频分类：可以用于视频分类、行为识别和运动检测等任务，通过提取视频帧的特征并将其输入SVM模型进行分类</li><li>推荐系统：可以用于个性化推荐和用户分类等推荐系统任务，通过分析用户行为和特征来预测用户的兴趣和偏好</li></ul><h3 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h3><p>逻辑回归虽然名字中带有回归，其实是分类模型，主要用于二分类问题，通过给定的n组数据（训练集）来训练模型，并在训练结束后对给定的一组或多组数据（测试集）进行分类。其中每一组数据都是由p个指标构成。</p><p>由于二分类问题分成两类，可以让其中一类标签为0，另一类为1。我们需要一个函数，对于输入的每一组数据，都能映射成0~1之间的数。并且如果函数值大于0.5，就判定属于1，否则属于0。而且函数中需要待定参数，通过利用样本训练，使得这个参数能够对训练集中的数据有很准确的预测。</p><h4 id="形式"><a href="#形式" class="headerlink" title="形式"></a>形式</h4><p>逻辑回归的模型包含两个模块：线性部分和激活函数</p><p>线性部分的数学模型写为<code>h(x)=wx+b</code>，x表示样本的输入特征向量，w是权重，b是偏置</p><p>激活函数是sigmoid函数：<br>$$<br>\sigma(h(x))&#x3D; \frac{1}{1+e^{−h(x)}}<br>&#x3D; \frac{1}{1+e^{−(wx+b)}}<br>$$</p><p>这个函数的曲线是一条值域为(0,1)的曲线，当输入值趋近于无穷大时，输出结果会趋近于1，输入值趋近于无穷小时，输出结果会趋近于-1</p><p>逻辑回归不仅是一个二分类模型，也是一个线性分类模型，h(x)作为决策函数，通过判断样本落在决策函数的哪一边，区分出不同的类别</p><h2 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h2><p>该类算法的输入样本不需要标记，而是自动地从样本中学习这种特征以实现预测。</p><p>常见的非监督学习算法：聚类和降维</p><ul><li>聚类：Kmeans、Apriori、DBSCAN等</li><li>降维：主成分分析(PCA)等</li></ul><h3 id="Kmeans算法"><a href="#Kmeans算法" class="headerlink" title="Kmeans算法"></a>Kmeans算法</h3><p>k均值聚类算法（k-means）是一种迭代求解的聚类分析算法，其步骤是，预将数据分为K组，则随机选取K个对象作为初始的聚类中心，然后计算每个对象与各个种子聚类中心之间的距离，把每个对象分配给距离它最近的聚类中心。聚类中心以及分配给它们的对象就代表一个聚类。每分配一个样本，聚类的聚类中心会根据聚类中现有的对象被重新计算。这个过程将不断重复直到满足某个终止条件。终止条件可以是没有（或最小数目）对象被重新分配给不同的聚类，没有（或最小数目）聚类中心再发生变化，误差平方和局部最小。</p><h4 id="算法步骤"><a href="#算法步骤" class="headerlink" title="算法步骤"></a>算法步骤</h4><ol><li>从N个数据文档（样本）随机选取K个数据文档作为质心（聚类中心）。</li><li>对每个数据文档测量其到每个质心的距离，并把它归到最近的质心的类。</li><li>重新计算已经得到的各个类的质心。</li><li>迭代2~3步直至新的质心与原质心相等或小于指定阈值，算法结束。 本文采用所有样本所属的质心都不再变化时，算法收敛。</li></ol><h1 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h1><p>每一个样本经过模型后会得到一个预测值，然后得到的预测值和真实值的差值就成为损失（损失值越小证明模型越成功），有许多不同种类的损失函数，这些函数<strong>本质上就是计算预测值和真实值的差距的一类型函数</strong>，经过库的封装形成了有具体名字的函数</p><p>输入的feature（或称为x）需要通过模型（model）预测出y，此过程称为向前传播（forward pass），而要将预测与真实值的差值减小需要更新模型中的参数，这个过程称为向后传播（backward pass），其中损失函数（lossfunction）就基于这两种传播之间，起到一种有点像<strong>承上启下</strong>的作用，<strong>承上指：接収模型的预测值，启下指：计算预测值和真实值的差值，为下面反向传播提供输入数据</strong></p><p>常用的损失函数有：均方误差(MSE)、平均绝对误差(MAE)、均方根误差(RMSE)、平均偏差误差(MBE)、Huber损失、最大似然损失(Likelihood Loss&#x2F;LHL)、二元交叉熵（BCE）、交叉熵（CE）、Kullback-Leibler 散度 (KLD)等</p>]]></content:encoded>
      
      
      <category domain="http://example.com/categories/MachineLearning/">MachineLearning</category>
      
      
      <category domain="http://example.com/tags/MachineLearning/">MachineLearning</category>
      
      
      <comments>http://example.com/inori/54da7039.html#disqus_thread</comments>
      
    </item>
    
  </channel>
</rss>
