<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>pytorch基础入门 | 楪祈のBlog</title><meta name="author" content="楪舞飞祈"><meta name="copyright" content="楪舞飞祈"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="pytorch入门">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch基础入门">
<meta property="og:url" content="http://ltynote.cn/inori/8f832495.html">
<meta property="og:site_name" content="楪祈のBlog">
<meta property="og:description" content="pytorch入门">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://imgapi.xl0408.top/index.php">
<meta property="article:published_time" content="2024-05-23T08:30:34.000Z">
<meta property="article:modified_time" content="2024-06-03T01:50:06.433Z">
<meta property="article:author" content="楪舞飞祈">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://imgapi.xl0408.top/index.php"><link rel="shortcut icon" href="/img/favicon_32.png"><link rel="canonical" href="http://ltynote.cn/inori/8f832495.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css?v=4.12.0"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlight.js","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: false
  },
  runtime: '天',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid@4.11.0/dist/infinitegrid.min.js',
    buttonText: '加载更多'
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'pytorch基础入门',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-06-03 09:50:06'
}</script><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><link rel="stylesheet" href="/css/universe.css"><link rel="stylesheet" href="/css/custom.css"><meta name="generator" content="Hexo 7.0.0"><link rel="alternate" href="/rss2.xml" title="楪祈のBlog" type="application/rss+xml">
</head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/loading.gif" data-original="/img/avatar.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">76</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">29</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://imgapi.xl0408.top/index.php')"><nav id="nav"><span id="blog-info"><a href="/" title="楪祈のBlog"><span class="site-name">楪祈のBlog</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">pytorch基础入门</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-05-23T08:30:34.000Z" title="发表于 2024-05-23 16:30:34">2024-05-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-06-03T01:50:06.433Z" title="更新于 2024-06-03 09:50:06">2024-06-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/pytorch/">pytorch</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">4.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>17分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="pytorch基础入门"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="两个实用函数"><a href="#两个实用函数" class="headerlink" title="两个实用函数"></a>两个实用函数</h1><p>假如pytorch是一个大型工具箱，我们想查看工具箱中有什么工具，这时就可以使用<code>dir()</code>函数查看，如果我们想知道某一个工具是如何使用的，就可以使用<code>help()</code>函数查看</p>
<ul>
<li><code>dir()</code>：可以查看指定对象包含的全部内容，包括变量、方法、函数和类等。不仅包含可供我们调用的模块成员，还包含所有名称以双下划线“__”开头和结尾的“特殊”命名的私有成员，这些成员是在本模块中使用的，不能在类的外部调用。</li>
<li><code>help()</code>：查看指定对象（类型、模块、变量、方法等）的详细使用说明</li>
</ul>
<h1 id="Pytorch"><a href="#Pytorch" class="headerlink" title="Pytorch"></a>Pytorch</h1><p>在学习和使用pytorch时，要经常使用<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/index.html">官方文档</a>，里面有详细的使用说明</p>
<h2 id="加载数据集"><a href="#加载数据集" class="headerlink" title="加载数据集"></a>加载数据集</h2><h3 id="加载数据方法及label形式"><a href="#加载数据方法及label形式" class="headerlink" title="加载数据方法及label形式"></a>加载数据方法及label形式</h3><p>Pytorch中加载数据需要Dataset、Dataloader</p>
<ul>
<li>Dataset提供一种方式去获取每个数据及其对应的label和编号，以及总共有多少个数据</li>
<li>Dataloader为后面的网络提供不同的数据形式，可以将数据进行打包</li>
</ul>
<p>label形式</p>
<ul>
<li>文件夹名即为label。文件夹中存放若干条数据</li>
<li>一个文件夹存放数据，数据有编号，另一个文件夹存放数据对应编号的说明文本（txt），文本中有label</li>
<li>直接把label写在数据的名称上</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br></pre></td></tr></table></figure>

<h3 id="通过路径加载数据"><a href="#通过路径加载数据" class="headerlink" title="通过路径加载数据"></a>通过路径加载数据</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">img_path = <span class="string">&quot;数据路径/train/ants/0013035.jpg&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure>

<h3 id="python特殊方法补充"><a href="#python特殊方法补充" class="headerlink" title="python特殊方法补充"></a>python特殊方法补充</h3><p>Python中有很多特殊方法，这些特殊方法的命名都以双下划线 <code>__</code>开头和结尾，它们是Python中常用的特殊方法。通过定义这些方法，我们可以自定义对象的行为和操作，使得对象能够更好地适应我们的需求</p>
<ul>
<li><code>__init__(self[, args...])</code>: 构造函数，用于在创建对象时进行初始化。即Java中的构造器</li>
<li><code>__repr__(self)</code>: 用于定义对象的字符串表示形式，通常用于调试和记录日志</li>
<li><code>__str__(self)</code>: 用于定义对象的字符串表示形式，通常用于显示给终端用户。即Java中的toString</li>
<li><code>__len__(self)</code>: 用于返回对象的长度，通常在对像被视为序列或集合时使用</li>
<li><code>__getitem__(self, key)</code>: 用于实现索引操作，可以通过索引或切片访问对象中的元素</li>
<li><code>__setitem__(self, key, value)</code>: 用于实现索引赋值操作，可以通过索引或切片为对象中的元素赋值</li>
<li><code>__delitem__(self, key)</code>: 用于实现删除某个元素的操作，可以通过索引或切片删除对象中的元素</li>
<li><code>__contains__(self, item)</code>: 用于检查对象是否包含某个元素，可以通过 in 关键字使用</li>
<li><code>__enter__(self)</code>: 用于实现上下文管理器的进入操作，通常与 with 语句一起使用</li>
<li><code>__exit__(self, exc_type, exc_value, traceback)</code>: 用于实现上下文管理器的退出操作，通常与 with 语句一起使用</li>
<li><code>__call__(self[, args...])</code>: 用于使对象能够像函数一样被调用，通常在创建可调用的类时使用</li>
<li><code>__eq__(self, other)</code>: 用于定义对象相等的比较操作，可以通过 &#x3D;&#x3D; 运算符使用</li>
<li><code>__lt__(self, other)</code>: 用于定义对象小于的比较操作，可以通过 &lt; 运算符使用</li>
<li><code>__gt__(self, other)</code>: 用于定义对象大于的比较操作，可以通过 &gt; 运算符使用</li>
<li><code>__add__(self, other)</code>: 用于实现对象加法操作，可以通过 + 运算符使用</li>
<li><code>__sub__(self, other)</code>: 用于实现对象减法操作，可以通过 - 运算符使用</li>
<li><code>__mul__(self, other)</code>: 用于实现对象乘法操作，可以通过 * 运算符使用</li>
<li><code>__truediv__(self, other)</code>: 用于实现对象除法操作，可以通过 &#x2F; 运算符使用</li>
</ul>
<h3 id="Dataset加载数据示例"><a href="#Dataset加载数据示例" class="headerlink" title="Dataset加载数据示例"></a>Dataset加载数据示例</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset  </span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image  </span><br><span class="line"><span class="keyword">import</span> os </span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyData</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, label_dir</span>):</span><br><span class="line">        self.root_dir = root_dir  								<span class="comment"># 记录数据集根目录的路径</span></span><br><span class="line">        self.label_dir = label_dir  							<span class="comment"># 记录数据集标签目录的名称</span></span><br><span class="line">        self.path = os.path.join(self.root_dir, self.label_dir)  <span class="comment"># os.path.join可将两个字符串拼接成一个完整路径，以获取数据集标签目录的完整路径</span></span><br><span class="line">        self.img_path = os.listdir(self.path)  					<span class="comment"># os.listdir() 函数用于获取指定目录下的所有文件和文件夹的名称列表，以获取数据集标签目录下所有图像文件的路径</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):  			<span class="comment"># 获取数据集中指定索引位置的数据项</span></span><br><span class="line">        img_name = self.img_path[idx]  		<span class="comment"># 获取图像文件名</span></span><br><span class="line">        img_item_path = os.path.join(self.root_dir, self.label_dir, img_name)  <span class="comment"># 获取该图像文件的完整路径</span></span><br><span class="line">        img = Image.<span class="built_in">open</span>(img_item_path)  	<span class="comment"># 打开图像文件</span></span><br><span class="line">        label = self.label_dir  			<span class="comment"># 获取该图像文件所属的标签</span></span><br><span class="line">        <span class="keyword">return</span> img, label  					<span class="comment"># 返回图像和标签</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_path)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">root_dir = <span class="string">&quot;data/train&quot;</span>   <span class="comment"># 数据集根目录的路径</span></span><br><span class="line">ants_label_dir = <span class="string">&quot;ants&quot;</span>   									  <span class="comment"># 蚂蚁标签目录的名称</span></span><br><span class="line">bees_label_dir = <span class="string">&quot;bees&quot;</span>   									  <span class="comment"># 蜜蜂标签目录的名称</span></span><br><span class="line">ants_dataset = MyData(root_dir, ants_label_dir) 			 <span class="comment"># 创建蚂蚁数据集对象</span></span><br><span class="line">bees_dataset = MyData(root_dir, bees_label_dir)  			 <span class="comment"># 创建蜜蜂数据集对象</span></span><br><span class="line">train_dataset = ants_dataset + bees_dataset  				<span class="comment"># 合并蚂蚁和蜜蜂数据集，得到训练集</span></span><br><span class="line">img, label = train_dataset[<span class="number">200</span>]  							<span class="comment"># 自动调用__getitem__() 方法，获取训练集中第 200 个数据项的图像和标签 </span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;label：&quot;</span>, label)   									<span class="comment"># 查看该数据项的标签</span></span><br><span class="line">img.show()   						</span><br></pre></td></tr></table></figure>

<h2 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h2><p>Tensorboad可以用来查看loss是否按照预想的变化，或者查看训练到某一步输出的图像是什么样</p>
<h3 id="Tensorboard使用示例"><a href="#Tensorboard使用示例" class="headerlink" title="Tensorboard使用示例"></a>Tensorboard使用示例</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)	<span class="comment"># 创建一个 SummaryWriter 对象，指定日志存储目录为 &quot;logs&quot;</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># 将 y=x 的函数值添加到 TensorBoard 中</span></span><br><span class="line">    writer.add_scalar(<span class="string">&quot;y=x&quot;</span>, i, i)</span><br><span class="line">writer.close()	<span class="comment"># 关闭 SummaryWriter 对象</span></span><br></pre></td></tr></table></figure>

<p>运行完后会在当前目录下创建一个logs文件夹</p>
<p>在终端运行</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">tensorboard --logdir=logs --port=6007</span><br></pre></td></tr></table></figure>

<h2 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h2><p>Transforms当成工具箱的话，里面的class就是不同的工具。例如像totensor、resize这些工具。Transforms拿一些特定格式的图片，经过Transforms里面的工具，获得我们想要的结果</p>
<h3 id="transforms-Totensor"><a href="#transforms-Totensor" class="headerlink" title="transforms.Totensor"></a>transforms.Totensor</h3><p>Tensor包装了神经网络需要的一些属性，比如反向传播、梯度等属性</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;/bees/10870992_eebeeb3a12.jpg&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor()  <span class="comment"># 创建 transforms.ToTensor类 的实例化对象</span></span><br><span class="line">tensor_img = tensor_trans(img)  <span class="comment"># 调用 transforms.ToTensor类的__call__方法   </span></span><br><span class="line"></span><br><span class="line">writer.add_image(<span class="string">&quot;Temsor_img&quot;</span>,tensor_img)</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<h3 id="transforms-Resize"><a href="#transforms-Resize" class="headerlink" title="transforms.Resize()"></a>transforms.Resize()</h3><p>调整图像的大小到指定的尺寸</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;data/Images/1.jpg&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将图片转为totensor类型</span></span><br><span class="line">trans_totensor = transforms.ToTensor() </span><br><span class="line">img_tensor = trans_totensor(img)  </span><br><span class="line"></span><br><span class="line"><span class="comment"># resize图片，PIL数据类型的 img -&gt; resize -&gt; PIL数据类型的 img_resize</span></span><br><span class="line">trans_resize = transforms.Resize((<span class="number">512</span>,<span class="number">512</span>))  <span class="comment"># 调整尺寸为512*512</span></span><br><span class="line">img_resize = trans_resize(img)</span><br><span class="line"></span><br><span class="line"><span class="comment"># PIL 数据类型的 PIL -&gt; totensor -&gt; img_resize tensor</span></span><br><span class="line">img_resize = trans_totensor(img_resize)</span><br><span class="line"><span class="built_in">print</span>(img_resize.size()) 				</span><br></pre></td></tr></table></figure>

<h4 id="transforms-Compose"><a href="#transforms-Compose" class="headerlink" title="transforms.Compose"></a>transforms.Compose</h4><p>transforms.Compose 的作用是将多个数据预处理操作组合在一起，方便地对数据进行一次性处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;data/Images/1.jpg&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor() </span><br><span class="line">img_tensor = tensor_trans(img)  </span><br><span class="line"></span><br><span class="line">trans_resize_2 = transforms.Resize(<span class="number">512</span>) </span><br><span class="line"></span><br><span class="line"><span class="comment"># PIL —— resize -&gt; PIL ——  totensor -&gt; tensor</span></span><br><span class="line">trans_compose = transforms.Compose([trans_resize_2, trans_totensor]) <span class="comment"># Compose函数中前面一个参数的输出为后面一个参数的输入，即trans_resize_2输出了pil，作为trans_totensor的输入</span></span><br><span class="line">img_resize_2 = trans_compose(img)</span><br><span class="line"><span class="built_in">print</span>(img_resize_2.size()) </span><br></pre></td></tr></table></figure>

<h2 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h2><p>DataLoader可以将数据集进行批量处理</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备的测试数据集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor())    </span><br><span class="line"></span><br><span class="line"><span class="comment"># batch_size=4 使得 img0, target0 = dataset[0]、img1, target1 = dataset[1]、img2, target2 = dataset[2]、img3, target3 = dataset[3]，然后这四个数据作为Dataloader的一个返回      </span></span><br><span class="line">test_loader = DataLoader(dataset=test_data,batch_size=<span class="number">4</span>,shuffle=<span class="literal">True</span>,num_workers=<span class="number">0</span>,drop_last=<span class="literal">True</span>)      </span><br><span class="line"><span class="comment"># 用for循环取出DataLoader打包好的四个数据</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>):</span><br><span class="line">    step = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">        imgs, targets = data <span class="comment"># 每个data都是由4张图片组成，imgs.size 为 [4,3,32,32]，四张32×32图片三通道，targets由四个标签组成             </span></span><br><span class="line">        writer.add_images(<span class="string">&quot;Epoch：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(epoch), imgs, step) <span class="comment"># 注意是images</span></span><br><span class="line">        step = step + <span class="number">1</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="nn-Module"><a href="#nn-Module" class="headerlink" title="nn.Module"></a>nn.Module</h3><p>orch.nn.Module是所有神经网络基本骨架，需要重写<code>__init__</code>和forward（前向传播）函数</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Module, self).__init__()  <span class="comment"># 继承</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):           <span class="comment"># 前向传播</span></span><br><span class="line">        output = <span class="built_in">input</span> + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line">    </span><br><span class="line">m = Module()</span><br><span class="line">x = torch.tensor(<span class="number">1.0</span>)  <span class="comment"># 创建一个值为 1.0 的tensor</span></span><br><span class="line">output = m(x)</span><br><span class="line"><span class="built_in">print</span>(output) 	<span class="comment"># 2</span></span><br></pre></td></tr></table></figure>

<h3 id="卷积函数"><a href="#卷积函数" class="headerlink" title="卷积函数"></a>卷积函数</h3><p>convolution卷积，conv2d表示二维</p>
<p>weight卷积核的大小，bias偏置，stride步长，padding填充，group是分组卷积，对不同的通道使用不同的卷积核，言外之意一般是对每个通道使用相同卷积核</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Module, self).__init__()</span><br><span class="line">        self.conv1 = Conv2d(in_channels=<span class="number">3</span>,out_channels=<span class="number">6</span>,kernel_size=<span class="number">3</span>,stride=<span class="number">1</span>,padding=<span class="number">0</span>) <span class="comment"># 彩色图像输入为3层，我们想让它的输出为6层，选3 * 3 的卷积                </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">m = Module()</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    output = m(imgs)</span><br><span class="line">    <span class="built_in">print</span>(imgs.shape)   <span class="comment"># 输入为3通道32×32的64张图片</span></span><br><span class="line">    <span class="built_in">print</span>(output.shape) <span class="comment"># 输出为6通道30×30的64张图片</span></span><br></pre></td></tr></table></figure>

<h3 id="最大池化"><a href="#最大池化" class="headerlink" title="最大池化"></a>最大池化</h3><p>nn.MaxPool2d最大池化（下采样，最常用），nn.MaxUnpool2d（上采样），nn.AdaptiveMaxPool2d（自适应最大池化），ceil_mode&#x3D;True表示进位，默认为False，写论文会用到公式可以在官网查阅</p>
<p>最大池化的作用：保留输入的特征，同时把数据量减少，比如视频变720P</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> MaxPool2d</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Module, self).__init__()</span><br><span class="line">        self.maxpool = MaxPool2d(kernel_size=<span class="number">3</span>, ceil_mode=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.maxpool(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">m = Module()  <span class="comment"># 即调用forward()</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    writer.add_images(<span class="string">&quot;input&quot;</span>, imgs, step)</span><br><span class="line">    output = m(imgs)</span><br><span class="line">    writer.add_images(<span class="string">&quot;output&quot;</span>, output, step)</span><br><span class="line">    step = step + <span class="number">1</span></span><br></pre></td></tr></table></figure>

<h3 id="非线性激活"><a href="#非线性激活" class="headerlink" title="非线性激活"></a>非线性激活</h3><p>作用：神经网络中引入非线性的特质，才能训练出符合各种特征的模型</p>
<p>nn.ReLU()小于0进行截断，大于0不变，nn.Sigmoid非线性缩放到[0,1]，1&#x2F;1+exp-x，inplace&#x3D;True表示把原来的值也改变（本来是通过返回值获取值）</p>
<h3 id="线性层及其它层"><a href="#线性层及其它层" class="headerlink" title="线性层及其它层"></a>线性层及其它层</h3><ul>
<li>Normalization Layers正则化层：加快神经网络的训练速度</li>
<li>Recurrent Layers用于文字识别，特定的网络结构，用的不多</li>
<li>Linear Layers：全连接层，用的较多</li>
<li>Dropout Layers随机将一些数设为0，防止过拟合</li>
<li>Distance Functions计算两个值之间的误差，常用余弦相似度</li>
<li>Loss Functions损失函数，常用值nn.MSELoss，nn.CrossEntropyLoss，nn.BCELoss，分布nn.NLLLoss，nn.KLDivLoss</li>
<li>torch.flatten()把输入展成一行，与reshape不同</li>
</ul>
<h3 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h3><ul>
<li>Loss损失函数一方面计算实际输出和目标之间的差距</li>
<li>Loss损失函数另一方面为我们更新输出提供一定的依据(反向传播)</li>
</ul>
<p><strong>L1loss</strong>:差值的绝对值之和，再求平均值</p>
<p><strong>MSE</strong>:平方差</p>
<h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>梯度下降是一种优化算法，用于最小化损失函数（Loss Function）。神经网络的训练目标是通过不断调整网络的参数（即权重和偏置）来最小化损失函数的值。梯度下降算法通过计算损失函数相对于网络参数的梯度来指导参数更新的方向和步幅。</p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>反向传播是一种高效计算梯度的方法，适用于多层神经网络。反向传播算法通过链式法则（Chain Rule）逐层计算损失函数对每个参数的梯度。</p>
<p><strong>反向传播和梯度下降的关系</strong></p>
<ol>
<li><strong>目标一致</strong>：两者的目标都是通过调整神经网络的参数来最小化损失函数</li>
<li><strong>互补</strong>：反向传播计算损失函数对参数的梯度，而梯度下降利用这些梯度更新参数</li>
<li><strong>训练过程</strong>：在神经网络的训练过程中，反向传播和梯度下降是交替进行的。首先进行前向传播，计算损失并通过反向传播计算梯度，然后使用梯度下降更新参数</li>
</ol>
<h3 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h3><p>使用优化器时，首先需要optim.zero_grad()，把上一步的梯度清零，否则会累加，然后进行反向传播，再optimizer.step()，对weight参数进行更新</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>, drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Module, self).__init__()</span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">3</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">32</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>, <span class="number">64</span>, <span class="number">5</span>, padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>, <span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>, <span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">loss = nn.CrossEntropyLoss()  <span class="comment"># 交叉熵</span></span><br><span class="line">m = Module()</span><br><span class="line">optim = torch.optim.SGD(tudui.parameters(), lr=<span class="number">0.01</span>)  <span class="comment"># 随机梯度下降优化器 lr为学习率,学习率太大不稳定,太小收敛慢</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化20轮</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = m(imgs)</span><br><span class="line">        result_loss = loss(outputs, targets) <span class="comment"># 计算实际输出与目标输出的差距</span></span><br><span class="line">        optim.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">        result_loss.backward() <span class="comment"># 反向传播，计算损失函数的梯度</span></span><br><span class="line">        optim.step()   <span class="comment"># 根据梯度，对网络的参数进行调优</span></span><br><span class="line">        running_loss = running_loss + result_loss</span><br><span class="line">    <span class="built_in">print</span>(running_loss) <span class="comment"># 每轮误差的总和</span></span><br></pre></td></tr></table></figure>

<h3 id="train-和tudui-eval-方法"><a href="#train-和tudui-eval-方法" class="headerlink" title="train()和tudui.eval()方法"></a>train()和tudui.eval()方法</h3><p>分别用于训练步骤和测试步骤，对特定层起作用，最好可以在训练和评估之前加上这个方法</p>
<h2 id="网络模型的保存和读取"><a href="#网络模型的保存和读取" class="headerlink" title="网络模型的保存和读取"></a>网络模型的保存和读取</h2><p>使用<code>save</code>方法保存网络模型的结构和参数，<code>load</code>加载时候要把模型定义给复制过来</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义模型</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(SimpleModel, self).__init__()</span><br><span class="line">        self.fc = nn.Linear(<span class="number">10</span>, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fc(x)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建实例</span></span><br><span class="line">model = SimpleModel()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存权重</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model_weights.pth&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载权重</span></span><br><span class="line">loaded_model = SimpleModel()</span><br><span class="line">loaded_model.load_state_dict(torch.load(<span class="string">&#x27;model_weights.pth&#x27;</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确保模型和加载的权重一致</span></span><br><span class="line">loaded_model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以继续使用加载的模型进行预测等操作</span></span><br></pre></td></tr></table></figure>

<h2 id="利用GPU训练"><a href="#利用GPU训练" class="headerlink" title="利用GPU训练"></a>利用GPU训练</h2><p>在程序之前定义:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>然后找到</p>
<ul>
<li>网络模型</li>
<li>数据（输入、标注）</li>
<li>损失函数</li>
</ul>
<p>后面加上<code>to(device)</code>即可转到GPU训练</p>
<h2 id="完整版实战"><a href="#完整版实战" class="headerlink" title="完整版实战"></a>完整版实战</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义训练的设备</span></span><br><span class="line">device = torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Module</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Module, self).__init__()        </span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  <span class="comment"># 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),  <span class="comment"># 展平后变成 64*4*4 了</span></span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line"></span><br><span class="line"><span class="comment"># length 长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用 Dataloader 来加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=<span class="number">64</span>)        </span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line">m = Module() </span><br><span class="line">m = m.to(device) <span class="comment"># 也可以不赋值，直接m.to(device) </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss() <span class="comment"># 交叉熵</span></span><br><span class="line">loss_fn = loss_fn.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">learning = <span class="number">0.01</span></span><br><span class="line">optimizer = torch.optim.SGD(tudui.parameters(),learning)   <span class="comment"># 随机梯度下降优化器  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置网络的一些参数</span></span><br><span class="line"><span class="comment"># 记录训练的次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 记录测试的次数</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练的轮次</span></span><br><span class="line">epoch = <span class="number">3000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加 tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">start_time = time.time()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-----第 &#123;&#125; 轮训练开始-----&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 训练步骤开始</span></span><br><span class="line">    tudui.train() <span class="comment"># 当网络中有dropout层、batchnorm层时，这些层能起作用</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs, targets = data            </span><br><span class="line">        imgs = imgs.to(device)</span><br><span class="line">        targets = targets.to(device)</span><br><span class="line">        outputs = tudui(imgs)</span><br><span class="line">        loss = loss_fn(outputs, targets) <span class="comment"># 计算实际输出与目标输出的差距</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 优化器对模型调优</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">        loss.backward() <span class="comment"># 反向传播，计算损失函数的梯度</span></span><br><span class="line">        optimizer.step()   <span class="comment"># 根据梯度，对网络的参数进行调优</span></span><br><span class="line">        </span><br><span class="line">        total_train_step = total_train_step + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            end_time = time.time()</span><br><span class="line">            <span class="built_in">print</span>(end_time - start_time) <span class="comment"># 运行训练一百次后的时间间隔</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数：&#123;&#125;，Loss：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step,loss.item()))  <span class="comment"># 方式二：获得loss值</span></span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>,loss.item(),total_train_step)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）</span></span><br><span class="line">    tudui.<span class="built_in">eval</span>()  <span class="comment"># 当网络中有dropout层、batchnorm层时，这些层不能起作用</span></span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 没有梯度了</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader: <span class="comment"># 测试数据集提取数据</span></span><br><span class="line">            imgs, targets = data <span class="comment"># 数据放到cuda上</span></span><br><span class="line">            imgs = imgs.to(device) <span class="comment"># 也可以不赋值，直接 imgs.to(device)</span></span><br><span class="line">            targets = targets.to(device) <span class="comment"># 也可以不赋值，直接 targets.to(device)</span></span><br><span class="line">            outputs = tudui(imgs)</span><br><span class="line">            loss = loss_fn(outputs, targets) <span class="comment"># 仅data数据在网络模型上的损失</span></span><br><span class="line">            total_test_loss = total_test_loss + loss.item() <span class="comment"># 所有loss</span></span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy = total_accuracy + accuracy</span><br><span class="line">            </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>,total_test_loss,total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>,total_accuracy/test_data_size,total_test_step)  </span><br><span class="line">    total_test_step = total_test_step + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    torch.save(m, <span class="string">&quot;./model/m_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i)) <span class="comment"># 保存每一轮训练后的结果</span></span><br><span class="line">    <span class="comment">#torch.save(m.state_dict(),&quot;m_&#123;&#125;.path&quot;.format(i)) # 保存方式二         </span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型已保存&quot;</span>)</span><br><span class="line">    </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>

<p>**with torch.no_grad()**表示在训练数据集的同时进行验证，可以让之后的代码不影响目前的梯度</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta"><i class="fas fa-circle-user fa-fw"></i>文章作者: </span><span class="post-copyright-info"><a href="http://ltynote.cn">楪舞飞祈</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta"><i class="fas fa-square-arrow-up-right fa-fw"></i>文章链接: </span><span class="post-copyright-info"><a href="http://ltynote.cn/inori/8f832495.html">http://ltynote.cn/inori/8f832495.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta"><i class="fas fa-circle-exclamation fa-fw"></i>版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://ltynote.cn" target="_blank">楪祈のBlog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post_share"><div class="social-share" data-image="https://imgapi.xl0408.top/index.php" data-sites="qq,wechat,twitter,weibo"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/inori/64d6f53b.html" title="强化学习基础知识"><img class="cover" src="/img/loading.gif" data-original="https://api.suyanw.cn/api/comic3.php" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">强化学习基础知识</div></div></a></div><div class="next-post pull-right"><a href="/inori/e0d16fd2.html" title="numpy常用api"><img class="cover" src="/img/loading.gif" data-original="https://api.suyanw.cn/api/comic3.php" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">numpy常用api</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/loading.gif" data-original="/img/avatar.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">楪舞飞祈</div><div class="author-info__description">星座になれたら。</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">76</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">29</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/inori-320"><i class="fab fa-github"></i><span>My GitHub</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="mailto:719471785@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content"><p align="center" color="brown">多读书，多看报，少吃零食，多睡觉。</p></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%B8%A4%E4%B8%AA%E5%AE%9E%E7%94%A8%E5%87%BD%E6%95%B0"><span class="toc-number">1.</span> <span class="toc-text">两个实用函数</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Pytorch"><span class="toc-number">2.</span> <span class="toc-text">Pytorch</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.1.</span> <span class="toc-text">加载数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E6%96%B9%E6%B3%95%E5%8F%8Alabel%E5%BD%A2%E5%BC%8F"><span class="toc-number">2.1.1.</span> <span class="toc-text">加载数据方法及label形式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E8%B7%AF%E5%BE%84%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">2.1.2.</span> <span class="toc-text">通过路径加载数据</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#python%E7%89%B9%E6%AE%8A%E6%96%B9%E6%B3%95%E8%A1%A5%E5%85%85"><span class="toc-number">2.1.3.</span> <span class="toc-text">python特殊方法补充</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dataset%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E7%A4%BA%E4%BE%8B"><span class="toc-number">2.1.4.</span> <span class="toc-text">Dataset加载数据示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TensorBoard"><span class="toc-number">2.2.</span> <span class="toc-text">TensorBoard</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Tensorboard%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="toc-number">2.2.1.</span> <span class="toc-text">Tensorboard使用示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transforms"><span class="toc-number">2.3.</span> <span class="toc-text">Transforms</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#transforms-Totensor"><span class="toc-number">2.3.1.</span> <span class="toc-text">transforms.Totensor</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#transforms-Resize"><span class="toc-number">2.3.2.</span> <span class="toc-text">transforms.Resize()</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#transforms-Compose"><span class="toc-number">2.3.2.1.</span> <span class="toc-text">transforms.Compose</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DataLoader"><span class="toc-number">2.4.</span> <span class="toc-text">DataLoader</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">2.5.</span> <span class="toc-text">神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#nn-Module"><span class="toc-number">2.5.1.</span> <span class="toc-text">nn.Module</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%B7%E7%A7%AF%E5%87%BD%E6%95%B0"><span class="toc-number">2.5.2.</span> <span class="toc-text">卷积函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96"><span class="toc-number">2.5.3.</span> <span class="toc-text">最大池化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%9E%E7%BA%BF%E6%80%A7%E6%BF%80%E6%B4%BB"><span class="toc-number">2.5.4.</span> <span class="toc-text">非线性激活</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E5%B1%82%E5%8F%8A%E5%85%B6%E5%AE%83%E5%B1%82"><span class="toc-number">2.5.5.</span> <span class="toc-text">线性层及其它层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.5.6.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.5.7.</span> <span class="toc-text">梯度下降</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">2.5.8.</span> <span class="toc-text">反向传播</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">2.5.9.</span> <span class="toc-text">优化器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#train-%E5%92%8Ctudui-eval-%E6%96%B9%E6%B3%95"><span class="toc-number">2.5.10.</span> <span class="toc-text">train()和tudui.eval()方法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BF%9D%E5%AD%98%E5%92%8C%E8%AF%BB%E5%8F%96"><span class="toc-number">2.6.</span> <span class="toc-text">网络模型的保存和读取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%A9%E7%94%A8GPU%E8%AE%AD%E7%BB%83"><span class="toc-number">2.7.</span> <span class="toc-text">利用GPU训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E7%89%88%E5%AE%9E%E6%88%98"><span class="toc-number">2.8.</span> <span class="toc-text">完整版实战</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/inori/17befa9.html" title="手撕—带过期时间的LRU"><img src="/img/loading.gif" data-original="https://api.suyanw.cn/api/comic3.php" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="手撕—带过期时间的LRU"/></a><div class="content"><a class="title" href="/inori/17befa9.html" title="手撕—带过期时间的LRU">手撕—带过期时间的LRU</a><time datetime="2025-08-13T06:38:59.000Z" title="发表于 2025-08-13 14:38:59">2025-08-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/inori/c29b5d38.html" title="手撕—单例模式"><img src="/img/loading.gif" data-original="https://api.suyanw.cn/api/comic3.php" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="手撕—单例模式"/></a><div class="content"><a class="title" href="/inori/c29b5d38.html" title="手撕—单例模式">手撕—单例模式</a><time datetime="2025-08-13T06:16:41.000Z" title="发表于 2025-08-13 14:16:41">2025-08-13</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/inori/a774566a.html" title="SpringSecurity基础入门"><img src="/img/loading.gif" data-original="https://api.suyanw.cn/api/comic3.php" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="SpringSecurity基础入门"/></a><div class="content"><a class="title" href="/inori/a774566a.html" title="SpringSecurity基础入门">SpringSecurity基础入门</a><time datetime="2025-07-22T06:08:57.000Z" title="发表于 2025-07-22 14:08:57">2025-07-22</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/inori/a2118cb.html" title="JUnit与Mockito测试框架入门"><img src="/img/loading.gif" data-original="https://imgapi.xl0408.top/index.php" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="JUnit与Mockito测试框架入门"/></a><div class="content"><a class="title" href="/inori/a2118cb.html" title="JUnit与Mockito测试框架入门">JUnit与Mockito测试框架入门</a><time datetime="2025-07-21T02:02:43.000Z" title="发表于 2025-07-21 10:02:43">2025-07-21</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/inori/6b394c8.html" title="OSS与优化基础入门"><img src="/img/loading.gif" data-original="https://imgapi.xl0408.top/index.php" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="OSS与优化基础入门"/></a><div class="content"><a class="title" href="/inori/6b394c8.html" title="OSS与优化基础入门">OSS与优化基础入门</a><time datetime="2025-07-20T02:30:13.000Z" title="发表于 2025-07-20 10:30:13">2025-07-20</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://imgapi.xl0408.top/index.php')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By 楪舞飞祈</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js?v=4.12.0"></script><script src="/js/main.js?v=4.12.0"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui@5.0.32/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><canvas id="universe"></canvas><script defer src="/js/universe.js"></script><canvas id="snow"></canvas><script async src="/js/snow.js"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1.1.3/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div>
        <style>
            [bg-lazy] {
                background-image: none !important;
                background-color: #eee !important;
            }
        </style>
        <script>
            window.imageLazyLoadSetting = {
                isSPA: false,
                preloadRatio: 1,
                processImages: null,
            };
        </script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})});</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a,i=c[o];e=function(){c=c.filter(function(t){return i!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(i)},(t=i).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),e&&e()):(n=new Image,a=t.getAttribute("data-original"),n.onload=function(){t.src=a,t.removeAttribute("data-original"),e&&e()},t.src!==a&&(n.src=a))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this);</script></body></html>